[
{
	"uri": "https://omar2cloud.github.io/aws/",
	"title": "AWS Tutorials",
	"tags": [],
	"description": "",
	"content": "AWS Tutorials Multiple AWS Tutorials "
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/domain/domain/",
	"title": "Create a Free Domain Name",
	"tags": [],
	"description": "",
	"content": "Tutorial Scenario:  Register a free domain at Freenom Signup for a free account at Cloudflare Add a site to Cloudflare  Step 1: Register a free domain at Freenom:  Navigate to Freenom and create a free account. Select Register a New Domain as show below.  Check the availability of a domain name of your choice. In my case, it\u0026rsquo;s mytunnel. Then, select the free domain TDL  Choose the Period for your selected domain. It ranges from 1 to 12 months. Then, click Continue.  Congratulations. You have successfully obtained your first free domain.\nStep 2: Signup for a free account at Cloudflare:   Navigate to Cloudflare and signup for a free account.\n  Click on Add a Site and type in the name of your new domain.   Select Get started for free and hit Continue.  We will add records later (CNAME). Click Continue.   We will copy our Cloudflare\u0026rsquo;s nameservers and paste them in our nameservers at Freenom as shown below.   Navigate back to Freenom to replace the nameservers. Click on Change Nameservers.\n  Paste the copied two Cloudflare\u0026rsquo;s nameservers and delete any other nameservers. Click Change Nameservers.  Navigate back to Cloudflare site and click on Finish later.  To check the status of nameservers change, click on Check nameservers.  Please wait until you receive an email stating Status Active. It will take several hours to complete the nameservers change.  Conclusion: By the end of this tutorial, we have successfully created registered a free domain, created a Cloudflare site and replaced the Freenom\u0026rsquo;s nameservers by the new Cloudflare\u0026rsquo;s nameservers.\n"
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/domain/",
	"title": "Domain Registration",
	"tags": [],
	"description": "",
	"content": "Free Domain Registration at Freenom Freenom is a free and paid domain provider. It offers free domains for 1 to 12 months with an option for unlimited renewal. It\u0026rsquo;s a very appealing service which doe snot require any payment method to obtain a domain with a Top Level Domain (TLD) of the following: .TK / .ML / .GA / .CF / .GQ.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/myrasp/",
	"title": "My First Raspberry Pi",
	"tags": [],
	"description": "",
	"content": "What is Raspberry Pi (RPI)? The Raspberry Pi is a small single-board computer capable of doing almost everything that an ordinary Linux-based computer can do. Once it connects to an HDMI display, a keyboard, and a mouse, we have have a fully functional computer for a pice of $35 (or even way less for the Raspberry Pi Zero). The Raspberry Pi is widely known and used for home lab environments, where anyone can experiment safely. To learn more about Raspberry Pi.\nNote that we will need to purchase a microSD card with a capacity of at least 8GB. We will also need a power supply to power the Raspberry Pi via USB-C for Raspberry Pi 4.\nRaspberry Pi 4 There are several models of RPI, and for most people Raspberry Pi 4 Model B is the one to choose. Raspberry Pi 4 Model B is the newest, fastest, and easiest to use.\nRaspberry Pi 4 comes with 2GB, 4GB, or 8GB of RAM. For most educational purposes and hobbyist projects, and for use as a desktop computer, 2GB is enough.\nRaspberry Zero Raspberry Pi Zero, Raspberry Pi Zero W, and Raspberry Pi Zero WH are smaller and require less power, so they’re useful for portable projects such as robots. It’s generally easier to start a project with Raspberry Pi 4, and to move to Raspberry Pi Zero when you have a working prototype that a smaller Raspberry Pi would be useful for. I\u0026rsquo;m using one of these amazing gems as a Twitter Bot. I will definitely put a tutorial together about how to create and host a Twitter Bot on a RPI Zero soon.\nUnpacking and Assembling My First Raspberry Pi 4 I purchased the latest Raspberry Pi 4 from Amazon which rocks a 1.5GHz 64-bit quad-core (ARMv8) CPU (4GB of RAM)—though you can step up to 8GB of RAM for a bit more money. Moreover, 2.4 Ghz and 5.0 GHz IEEE 802.11ac wireless, Bluetooth 5.0, BLE. It is very powerful for pretty little thing.\nI have had the starter kit with my RPI 4 which includes a premium case with integrated fan mount, 3.5A USB-C power supply with on/off power switch, set of heat sinks, micro HDMI to HDMI cable and 32 GB microSD card.\nThis is my RPI 4 after being fully assembled.\nWe will need a keyboard, a monitor and a mouse for the initial setup. However, we might not need them down the road. It depends on the project and how we are going to access the RPI 4.\nInstall an Operating System on the microSD Card The RPI operating system (previously called Raspbian) is the Foundation\u0026rsquo;s official OS for the Pi based on Debian Linux. The Pi Foundation also offers a Raspberry Pi Imager for a quick and easy way to install Raspberry PI OS and other operating systems to our microSD card. It\u0026rsquo;s available for Windows, MacOS and Ubuntu. Let\u0026rsquo;s download the Raspberry Pi Imager.\nFor our upcoming projects, I would suggest to install 64-bit Ubuntu Desktop 20.10 on the RPI instead of the 32-bit Raspbian operating system.\nAfter downloading the Raspberry Pi Imager, we will click on \u0026ldquo;CHOOSE OS\u0026rdquo; to select the Ubuntu Desktop 20.10 on our microSD card.\nIf the downloading process to the microSD is completed successfully, we shall see the below message.\nConclusion: Now, we have successfully created an image of Ubuntu Desktop 20.10. Let\u0026rsquo;s insert this MicroSD card into our RPI 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/spin-ubuntu-instance/",
	"title": "Part 1 - Ubuntu Instance",
	"tags": [],
	"description": "",
	"content": "Spinning up Ubuntu 20.04 LTS EC2 The following is a step by step tutorial on how to spin a Ubuntu 20.04 EC2 instance and how to SSH into our Ubuntu instance.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/portainer/",
	"title": "Part 2 - Portainer on Ubuntu",
	"tags": [],
	"description": "",
	"content": "Install Portainer on Ubuntu 20.04 LTS Instance This is a step by step tutorial on how to install Portainer Docker management tool on Ubuntu 20.04 on EC2 instance. Portainer is a great way to learn Docker and Containers for beginners.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/spin-ubuntu-instance/ec2-ubuntu/",
	"title": "Spin up Ubuntu Instance",
	"tags": [],
	"description": "",
	"content": "What is Ubuntu 20.04 LTS? Ubuntu is a complete Linux operating system. It is available with both community and professional support. The Ubuntu community should be available free of charge and people should have the freedom to customize it. To learn more about Ubuntu.\nPlease note that some parts of the images in this tutorial will be pixilated to hide all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rIn order to complete this tutorial, you will access to an AWS account. Therefore, if you don\u0026rsquo;t have an AWS account, you should sign up and create an account. AWS offers a 12-month free tier account. With this free-trier account, AWS gives you 750 hrs/monthly of EC2 computing service in a single Availability Zone and t2.micro instance. If we have one instance running 24hrs X 31 days = 744 hrs, which is still below the AWS free-tier account limit of 750 hrs/monthly. Basically, you would have one EC2 instance running for an entire year for free.\nOnce you\u0026rsquo;re logged in and on the home page, as shown below, make sure you select the region to where you want to spin your instance. Us East-1 (N. Virginia) is selected in my case. Then, click on EC2 under the Compute section.\nClick on Launch Instance On EC2 Dashboard\nChoose AMI (Amazon Machine Image) Ubuntu 20.04 LTS. You might need to scroll down to find Ubuntu 20.04 LTS.\nChoose Instance Type as General purpose t2.micro. This is the instance type identified by AWS for the Free-Tier account eligibility. Now click on Configure Instance Details\nOn the Configure Instance Details, we don\u0026rsquo;t need to make any changes. However, ensure that all selections are similar to what is on the below image.\nOn Add Storage screen is where we can increase the storage size of the instance. AWS permits up to 30GB for free-tier accounts. Let\u0026rsquo;s take advantage of it. I increased mine to 16 GB, you could use more if you\u0026rsquo;d like.\nIt is always a good practice and good a habit to have when it comes to adding tags. Since I have multiple Ubuntu instances, I would like to dedicate this to Portainer. You could tag your instance under any name you would like.\nConfigure Security Group is where we create security groups and specify Ports to open for the EC2 instance. Note that we will only ports that will allow us to install and run Portainer for now. We will open more ports down the road based on the ports required by the web application we will run.\nBe very careful when you open ports to the world.\nOn Create a new security group, name your security group and give it a description.\nThese are the two ports we need to open at this stage.\n   Type Port     SSH 22   Custom TCP 9000    The final stage of this process is Review Instance Launch. Once you have reviewed the instance configurations, click on Launch.\nWe will create a key pair to connect (SSH) to our EC2 instance from our local machine. Let us download the Key Pair and keep it in a secure location. Without this key, we will not be abe to connect to the EC2 instance.\nA key pair, consisting of a private key and a public key, is a set of security credentials that you use to prove your identity when connecting to an instance. Amazon EC2 stores the public key, and you store the private key. You use the private key, instead of a password, to securely access your instances. Anyone who possesses your private keys can connect to your instances, so it\u0026rsquo;s important that you store your private keys in a secure place. To read more about a key pair from AWS.\nClick on Launch Instance\nLet\u0026rsquo;s wait until the Instance State is updated to \u0026ldquo;running\u0026rdquo; status. The running status \u0026ldquo;Green\u0026rdquo; is indication that our EC2 instance is fully provisioned and ready to be connected to.\nConclusion: This concludes the first section of part one of the tutorial. We have successfully spun a Ubuntu 20.04 LTS EC2 instance. In the next section, we will learn how to SSH into the instance.\n"
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/",
	"title": "Cloudflare Tutorials",
	"tags": [],
	"description": "",
	"content": "Cloudflare Cloudflare Tunnel Cloudflare is a global network designed to make everything you connect to the Internet secure, private, fast, and reliable. Cloudflare offers a suite of services and Zero Trust Services are the services we will utilize in the following tutorials. Zero Trust Services consist of Teams, Access, Gateway and Browser Isolation.\nOur main goal is to obtain a free domain from Freenom and connect our hosted applications on a Ubuntu 20.04 LTS Raspberry Pi 4 within our local home network via a Cloudflare Tunnel to the world wide web securely without any port-forwarding complications or altering firewall.\n"
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/cloudflared/",
	"title": "Cloudflare Tutorials",
	"tags": [],
	"description": "",
	"content": "Cloudflare Tunnel In April, 2021, Cloudflare Tunnel is announced as a free service for everyone. This service creates a secure, outbound-only connection between applications hosted locally and Cloudflare by deploying a lightweight connector (Cloudflared daemon).\nBasically, with Cloudflare Tunnel, anyone can create a private link/tunnel from any locally hosted application or server to Cloudflare without a public IP address, port-forwarding or punching through a firewall. This secured Tunnel is established by running Cloudflared daemon, on the origin, which allows for a secure, outbound-only connection. All traffic, to the origin, funnels through Cloudflare network service.\n"
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/cloudflared/cloudflare/",
	"title": "Create a Free Cloudflare Tunnel",
	"tags": [],
	"description": "",
	"content": "Tutorial Scenario:  Signup for a free Cloudflare for Teams. Install and authenticate cloudflared on a Raspberry Pi 4. Create a Cloudflare Tunnel. Configure the Tunnel details. Create DNS records to route traffic to the Tunnel. Run and manage the Tunnel. Add a Zero Trust policy. Run Tunnel as a service.  Step 1: Signup for a free Cloudflare for Teams: Navigate to Cloudflare for Teams and signup for a free account. Cloudflare has a well documented Get started site to walk you through the setup process. For this step, you don\u0026rsquo;t need to go beyond signing up.\nStep 2: Install and authenticate Cloudflared on a Raspberry Pi 4:  First of all, if you’d like to check your device’s architecture, run the following command:  uname -a\rNavigate to Install Cloudflared site to download the proper package for your architecture. In my case, I will install the Cloudflared daemon on my RPI-4, which is an arm64 architecture.  arm64 architecture (64-bit Raspberry Pi 4): sudo wget -O cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-arm64\rsudo mv cloudflared /usr/local/bin\rsudo chmod +x /usr/local/bin/cloudflared\rcloudflared -v\rAMD64 architecture (Debian/Ubuntu): sudo wget https://bin.equinox.io/c/VdrWdbjqyF/cloudflared-stable-linux-amd64.deb\rsudo apt-get install ./cloudflared-stable-linux-amd64.deb\rcloudflared -v\rarmhf architecture (32-bit Raspberry Pi): sudo wget https://bin.equinox.io/c/VdrWdbjqyF/cloudflared-stable-linux-arm.tgz\rtar -xvzf cloudflared-stable-linux-arm.tgz\rsudo cp ./cloudflared /usr/local/bin\rsudo chmod +x /usr/local/bin/cloudflared\rcloudflared -v\rOnce we have installed Cloudflared successfully, we will run the following command to authenticate the cloudflared to our Cloudflare account.  cloudflared login\rRunning the above command will launch the default browser window and prompt you to login to your Cloudflare account. Then, you will be prompted to select a hostname site, which we have create previously in Part 1: Step 2.\nAs soon as you have chosen your hostname, Cloudflare will download a certificate file to authenticate Cloudflared with Cloudflare\u0026rsquo;s network.\nThe cert.pem gives Cloudflared the capabilities to create tunnels and modify DNS records in the account. Once you have created a named Tunnel, you no longer need the cert.pem file to run that Tunnel and connect it to Cloudflare’s network. However, hte cert.pem file is still required to create additional Tunnels, list existing tunnels, manage DNS records, or delete Tunnels.\n\rOnce authorization is completed successfully, your cert.pem will be download to the default directory as shown below.\nIf you\u0026rsquo;re running a headless server (no monitor or keyboard), you could copy the authentication URL and paste it in a browser manually.\nThe credentials file contains a secret scoped to the specific Tunnel UUID which establishes a connection from cloudflared to Cloudflare’s network. cloudflared operates like a client and establishes a TLS connection from your infrastructure to Cloudflare’s edge.\n\rStep 3: Create a Cloudflare Tunnel:  Now, we are ready to create a Cloudflare Tunnel that will connect Cloudflared to Cloudflare\u0026rsquo;s edge. Utilizing the following command will create a Tunnel with tht name and generate an ID credentials file for it.  Prior to creating the Tunnel, you may need to exit the Command Line (CL). Next, let create the Tunnel.\nNote: replace \u0026lt;NAME\u0026gt; with any name of your choosing for the Tunnel.\ncloudflared tunnel create \u0026lt;NAME\u0026gt;\rOnce the Tunnel is created, a credential file is generated. It\u0026rsquo;s a JSON file that has the Universally Unique Identifier (UUID) assigned for the Tunnel.\nNote: although the Tunnel is created, the connection is not established yet.\nStep 4: Configure the Tunnel details: Although we can configure the Tunnel run in an add hoc mode, we will go over creating a configuring the Tunnel to automatically run it as a service.\nCloudflare utilizes a configuration file to determine how to route traffic. The configuration file contains keys and values, which is written in YAML syntax. You may need to modify the following keys and values to meet your configuration file requirements:\n   Keys Values     tunnel Tunnel name or Tunnel UUID   credentials-file location of credentials file (JSON)   hostname subdomain.hostname.xxx (example, test.example.com)   service url to local application - http://localhost:8000   service http_status:404   port of your app 80    By default, on Linux systems, Tunnel expects to find the configuration file in ~/.cloudflared, /etc/cloudflared and /usr/local/etc/cloudflared in that order. Let\u0026rsquo;s create our config file and save in the default expected directory for this tutorial.\nsudo nano ~/.cloudflared/config.yml\rOr,\nsudo nano home/\u0026lt;username\u0026gt;/.cloudflared/config.yml\rThen, we will paste our keys and values as shown below:\ntunnel: 1082b601-bce9-45e4-b6ae-f19020e7d071\rcredentials-file: /root/.cloudflared/1082b601-bce9-45e4-b6ae-f19020e7d071.json\ringress:\r- hostname: test.mytunnel.ml\rservice: http://localhost:80\r- service: http_status:404\rIf you don\u0026rsquo;t have any application ready to test the Tunnel, I\u0026rsquo;d suggest installing NGINX web server and port mapping it to port 80 as I\u0026rsquo;ve done in the configuration file.\n\r\r\rExpand me...\r\r\rHow to install NGINX web server on RPI-4:\nsudo apt install nginx Once the installation is completed, open a browser and type in: localhost:80. If the NGINX web server is installed properly, you shall see it running with its default index.html as shown below. \r Let\u0026rsquo;s make sure that we have all files in this directory:\nls -al\rNow, we have configured all required files to run the Tunnel in the default directory.\n\r\rExpand me...\r\r\rNote, if you\u0026rsquo;d like to save the config.yml file in a different location ( we will refrain from using this method for this tutorial), you will have to point to that directory during the run command by using the following: cloudflared tunnel \u0026ndash;config path/config.yml run UUID or Tunnel Name\nIt\u0026rsquo;s very import to specify \u0026ndash;config to change default directory for the config file. For more information about the configuration file.\n\r Step 5: Create DNS records to route traffic to the Tunnel: Cloudflare can route traffic to our Tunnel connection using a DNS record or a loud balancer. We will configure a DNS CNAME record to point to our Tunnel subdomain. There are two ways to acheive this mission:\nA. Manually: navigate to the DNS tab on Cloudflare Dashboard, create a new CNAME record and add your subdomain of your Tunnel as follows:\n Type: CNAME Name: any subdomain name of your choosing. Target: consists of two parts: \u0026lt;UUID\u0026gt; and \u0026lt;cfargotunnel.com\u0026gt; such as, \u0026lt;UUID.cfargotunnel.com\u0026gt;  B. Programmatically: run the following command from the command line. This command will generate a CNAME record that points to the subdomain of a specific Tunnel. The result is the same as creating a CNAME record from the dashboard as shown in step A.\ncloudflared tunnel route dns \u0026lt;UUID or NAME\u0026gt; test.example.com\rNote: unlike the previous Argo Tunnel architecture, this DNS record will not be deleted if the Tunnel disconnects.\nStep 6: Run and manage the Tunnel: The run command will connect cloudflared to Cloudflare\u0026rsquo;s edge network using the configuration created in step 4. We will not specify a configuration file location so Cloudflared retrieves it from the default location, which is ~/.cloudflared/config.yml\ncloudflared tunnel run \u0026lt;UUID\u0026gt; or \u0026lt;Tunnel Name\u0026gt;\rIf the config.yml file is not placed in the default directory, we need to pinpoint to its location to run the Tunnel:\ncloudflared tunnel --config path/config.yml run \u0026lt;NAME\u0026gt; or \u0026lt;UUID\u0026gt;\rWe can review the list of Tunnels we have created by running the following command:\nCloudflared Commands:    Functions Commands     Create a Tunnel cloudflared tunnel run \u0026lt;NAME\u0026gt;   List Tunnels cloudflared tunnel list   Stop Tunnel cloudflared tunnel stop \u0026lt;NAME\u0026gt;   Restart Tunnel cloudflared tunnel restart \u0026lt;NAME\u0026gt;   Delete Tunnel cloudflared tunnel delete \u0026lt;NAME\u0026gt;   Force Delete Tunnel cloudflared tunnel delete -f \u0026lt;NAME\u0026gt;   Show each Cloudflared info cloudflared tunnel info \u0026lt;NAME\u0026gt;    Stopping Cloudflared will not delete the Tunnel or the DNS record created. Although Tunnel deletes DNS records after 24-48 hours of a Tunnel being unregistered, it does not delete TLS certificates on your behalf once the Tunnel is shut down. If you want to clean up a Tunnel you’ve shut down, you can delete DNS records in the DNS editor and revoke TLS certificates in the Origin Certificates section of the SSL/TLS tab of the Cloudflare dashboard.\n\rTo update Cloudflared:\nsudo cloudflared update\rTo uninstall Cloudflared\nsudo cloudflared service uninstall Step 7: Add a Zero Trust policy: Now, we are ready to head back to Teams dashboard to configure our application and create a Zero Trust Policy.\n On Teams dashboard, navigate to the Application tab and click on Add an application.  Select Self-hosted.  Choose an application name, Session Duration, subdomain and Application domain. Then, click on Next.  Notice that the Tunnel duration ranges from 15 mins to 1 month.\nAdd a name to the rule and select Bypass as a Rule action. On Configure a rule, include Everyone. This rule allows everyone to view our NGINX site at test.mytunnel.ml  In the Advanced settings, enable automatic cloudflared authentication and browser rendering.  Finally, our application is now available in Cloudflare Access and is part of our Application list. We can navigate to a browser and type in our url test.MyTunnel.ml and if our Tunnel is established correctly, we shall see our NGINX web server running as shown below.\nStep 8: Run Tunnel as a service: By running the following command, the Tunnel can be installed as a system service which allows the Tunnel to run at boot automatically as launch daemon. By default, the Tunnel expects to find the configuration file in the default directory, ~/.cloudflared/config.yml but to run Tunnel as a service, we might need to move the config.yml file in ~/etc/cloudflared/.\nWe can employ the move mv command to do the job: mv \u0026lt;path/config.yml\u0026gt; to \u0026lt;/etc/cloudflared/\u0026gt;\nThe below command is in my case with my RPI-4 and how I moved the config file to /etc/cloudflared/\nsudo mv /home/p2/.cloudflared/config.yml /etc/cloudflared/\rNow, we are ready to run Tunnel as a service utilizing the command below:\nsudo cloudflared service install Conclusion: We have successfully established a secure Cloudflare Tunnel that links our locally hosted NGINX web server to Cloudflare\u0026rsquo;s network without requiring any public IP address, port-forwarding or punching through a firewall. We have also configured the Tunnel as a service to start at boot, and now we have our NGINX web server associated and accessible via our domain name, test.MyTunnel.ml\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/vino/",
	"title": "How to Remote into Ubuntu",
	"tags": [],
	"description": "",
	"content": "What is a Virtual Network Computing (VNC)? VNC is a graphical desktop sharing system that allows you to remotely control the desktop interface from another computer or mobile device (running VNC Viewer). VNC Viewer transmits the keyboard and the mouse events to VNC Server, and receives updates to the screen in return. Setting up a VNC Server on our RPI 4 is very beneficial to gain access to it from another device.\nVino VNC Server on Ubuntu Vino is the default VNC server in Ubuntu which allows us to share our graphical desktop form other devices. If you have multiple RPI 4, utilizing VNC server to remote into these Pis (nodes) from your main device is definitely a great way to gain access.\nHow to configure Vino VNC Server on Ubuntu 1- Click on settings and then navigate to \u0026ldquo;Sharing.\u0026rdquo;\n2- On Sharing, toggle sharing to on.\n3- To set Vino to request access each time, check \u0026ldquo;Allow connections to control the screen.\u0026rdquo;\n4- To add a layer of security, select \u0026ldquo;require a password,\u0026rdquo; and set a password hard-to-guess.\n5- Toggle the \u0026ldquo;Networks\u0026rdquo; which you will allow the sharing over. All local networks should show up under \u0026ldquo;Networks.\u0026rdquo;\n6- Now, we will need to locate our IP address. Let\u0026rsquo;s open Terminal and type the following command.\nip addr show\ror\nip a\ror\nhostname -I\rIf you prefer a graphical method to obtain the IP address, then we shall go back to settings, Wi-Fi and clock on the settings of the Wi-Fi network.\nIf you have you RPI 4 wire, then you could obtain the IP address from the settings of the wired network.\n7- The last step is to download a VNC viewer on your primary device to remote into our RPI 4. I would recommend to download RealVNC as VNC viewer. We will use the IP address of our RPI 4 and port 5900 or 5901 to access our RPI4 remotely.\nConclusion: This tutorial concludes the activation of Vino the default VNC server of Ubuntu on RPI 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/portainer/portainer/",
	"title": "Install Docker and Portainer on Ubuntu Instance",
	"tags": [],
	"description": "",
	"content": "What is Portainer? Portainer is the definitive open source container management tool for Kubernetes, Docker, Docker Swarm and Azure ACI. It allows anyone to deploy and manage containers without the need to write codes. To learn more about Portainer.\nUpdating the Operating System Now, we have a secured connection with our instance, let\u0026rsquo;s update and upgrade the operating system. Updating and upgrading the operating system of the instance is always a good habit to have. We will run the following update/upgrade commands. The -y in the end of the command line will automatically enters \u0026ldquo;yes\u0026rdquo; as a confirmation before installing the updates.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rCleaning up after an update If this is not the packges first time to run an update, there might have some unnecessary packages left for cleaning. Removing these packages will free space and prevent your system from cluttering. The following command shall do the job.\nsudo apt autoremove\rStep 1: Install Docker If always want to automatically get the latest version of Docker on Ubuntu, you must add its official repository to Ubuntu system. To do that, run the commands below to install prerequisite packages:\nsudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common\rNext, run the commands below to download and install Docker’s official GPG key. The key is used to validate packages installed from Docker’s repository making sure they’re trusted.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\rsudo apt-key fingerprint 0EBFCD88\rThe response would be like this:\nResponse:\rpub rsa4096 2017-02-22 [SCEA]\r9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88\ruid [ unknown] Docker Release (CE deb) \u0026lt;docker@docker.com\u0026gt;\rsub rsa4096 2017-02-22 [S]\rNow that the official GPG key is installed, run the commands below to add its stable repository to Ubuntu. To add the nightly or test repository, add the word nightly or test (or both) after the word stable in the commands below.\nsudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot;\rAfter this command, Docker’s official GPG and repository should be installed on Ubuntu. If you have older versions of Docker, run the commands below to remove them:\nsudo apt-get remove docker docker-engine docker.io containerd runc\rWhen you have removed all the previous versions of Docker, run the commands below to install the latest and current stable version of Docker: To install specific version of Docker, run the apt-cache command. Then select the version to install.\napt-cache madison docker-ce\rOutput:\rdocker-ce | 5:19.03.12~3-0~ubuntu-focal | https://download.docker.com/linux/ubuntu focal/stable amd64 Packages\rdocker-ce | 5:19.03.11~3-0~ubuntu-focal | https://download.docker.com/linux/ubuntu focal/stable amd64 Packages\rdocker-ce | 5:19.03.10~3-0~ubuntu-focal | https://download.docker.com/linux/ubuntu focal/stable amd64 Packages\rdocker-ce | 5:19.03.9~3-0~ubuntu-focal | https://download.docker.com/linux/ubuntu focal/stable amd64 Packages\r....\rNow to install a specific version, run the commands below with the version you wish to install:\nsudo apt-get install docker-ce=5:19.03.10~3-0~ubuntu-focal docker-ce-cli=5:19.03.10~3-0~ubuntu-focal containerd.io\rIf you just want to latest version without specifying above, run the commands below. The command below will always install the highest possible version:\nsudo apt-get install docker-ce docker-ce-cli containerd.io\rThis will install Docker software on Ubuntu. Add your account, for most cases it will be ubuntu, to Docker group and restart:\nsudo usermod -aG docker $USER\rReboot your instance:\nsudo reboot\rTo verify that Docker CE is installed correctly you can run the hello-world image:\nsudo docker run hello-world\rIf Docker is installed correctly you will see the following response:\nResponse:\rHello from Docker!\rThis message shows that your installation appears to be working correctly.\rTo generate this message, Docker took the following steps:\r1. The Docker client contacted the Docker daemon.\r2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub.\r(amd64)\r3. The Docker daemon created a new container from that image which runs the\rexecutable that produces the output you are currently reading.\r4. The Docker daemon streamed that output to the Docker client, which sent it\rto your terminal.\rStep 2: Install Docker Compose On Ubuntu Linux, you can download the Docker Compose binary from the Compose repository release page on GitHub.\nTo install it, run the commands below to download version 1.28.5 As of this writing, this was the current version.\nsudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose\rTo install a different version of Compose, substitute 1.28.5 with the version of Compose you want to use.\nAfter downloading it, run the commands below to apply executable permissions to the binary file and create a symbolic link to /usr/binary\nsudo chmod +x /usr/local/bin/docker-compose\rsudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\rNow, Docker Compose should work. To test it, we will run the command below:\ndocker-compose --version\rYou should see similar output as below:\nResponse:\rdocker-compose version 1.28.5, build 0aa59064\rStep 3: Setup Portainer Now that Docker and Docker Composer are installed, follow the steps below to get Portainer setup.\nYou can use Docker command to deploy the Portainer Server; note the agent is not needed on standalone hosts, however, it does provide additional functionality if used.\nTo get the server installed, run the commands below.\ncd ~/\rdocker volume create portainer_data\rdocker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:latest\rYou’ll just need to access the port 9000 of the Docker engine where Portainer is running using your browser.\nNote: the -v /var/run/docker.sock:/var/run/docker.sock option can be used in Linux environments only.\nAfter a successful pull, you should get a similar message as below:\nlatest: Pulling from portainer/portainer-ce\rd1e017099d17: Pull complete a7dca5b5a9e8: Pull complete Digest: sha256:4ae7f14330b56ffc8728e63d355bc4bc7381417fa45ba0597e5dd32682901080\rStatus: Downloaded newer image for portainer/portainer-ce:latest\r2fd5f4a0883a9d358ad424fd963699445be8839f3e6a2cf73d55778bcc268523\rAt this point, all you need to do is access Portainer portal to manage Docker. Open your web browser and browse to the server’s hostname or IP address followed by port #9000\nhttp://localhost:9000 or http://your_EC2_instance_public_ip_adress:9000\nYou should get Portainer login page to create an admin password.\nSubmit a new password.\nNow, you see some options to choose the environment you want to manage. Since we installed Docker on the same instance, select to connect and manage Docker locally.\nYou’ll be directed to Portainer dashboard where you can start managing Docker. If you see a notification for upgrade, click on it and proceed with the upgrade process.\nConclusion: By the end of this tutorial, we have successfully installed Portainer Docker management tool on our Ubuntu 20.04 EC2 instance. In part 3 of this tutorial, we will learn how to install and run a Wordpress web application.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/",
	"title": "Portainer Tutorials",
	"tags": [],
	"description": "",
	"content": "PORTAINER Portainer Docker UI Manager Tutorials Portainer is one of the best way to learn Docker and Containers for beginners. Portainer is an open source management UI for Docker including DOcker Swarm environment. It makes managing Docker containers simpler and allows you to manage containers, images, networks and volumes from the web based Portainer dashboard. This tutorial consists of three parts. In the first part, we will spin a Ubuntu 20.04 LTS EC2 instance, and then in the second part, we will install Portainer on the Ubuntu 20.04 LTS EC2 instance. Finally, in the last part, we will run Portainer and install Wordpress site.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/spin-ubuntu-instance/ssh-into-ubuntu-ec2/",
	"title": "SSH into Ubuntu Instance",
	"tags": [],
	"description": "",
	"content": "What is Secure Shell (SSH)? Secure Shell is a network protocol uses encryption to secure connection between a client and a server. All user authentication, commands, output, and file transfers are encrypted to protect against attacks in the network. There are many tools available to establish SSH connection such as, PuTTy and WinSCP for Windows, CyberDuck for MacOS, OpenSSH for Unix and Linux and many other great tools.\nI would suggest we utilize AWS Command Line Interface (AWS CLI) version 2. The AWS CLI is a open source powerful tool that enables you to interact with AWS services using commands in your local machine command line shell. In my case, I\u0026rsquo;m using AWS CLI version 2 from within my Windows Command Prompt. Please, follow the installation process as documented by AWS based on your machine type (local device).\nWith this brief introduction about SSH, we will proceed with tutorial from where have stopped. Next, let’s get to actually logging into our Ubuntu EC2 instance. To do so, you’ll have to open Terminal of the SSH tool, which you have chosen to use. As I have previously stated, I have downloaded AWS CLI v2 ;therefore, I will run Command Line (Command Prompt) on Windows.\nOn CL, the first to do is change the directory using the command cd into the directory/folder where we have saved our key pair (pem file). This is the file we have downloaded in Part 1 tutorial.\nExample:\ncd C:\\Users\\Omar\\XXXXX\\Documents\\XXX\\AWS\\XXXX\rcd C:\\your full URL where you have saved the pem file for the instance\u0026gt;\rBack to the AWS console and on the Instances section, let select our Ubuntu instance and then click on Actions \u0026gt; Connect button.\nThen, let\u0026rsquo;s click on the third tab, SSH Client, and copy the SSH command line by clicking on the copy icon as shown below. This command has combined our pem file name, ssh command and our instance public domain name.\nLet\u0026rsquo;s paste this command on our CL or your chosen SSH tool and click enter. During the authentication process, you will prompted, \u0026ldquo;Are you sure you want to continue connecting (yes/no)?\u0026rdquo;, type in yes.\ncd C:\\Users\\Omar\\XX\\XX\u0026gt; ssh -i \u0026quot;Portainer.pem\u0026quot; ubuntu@ec2-xxxxxxx.compute-1.amazonaws.com\rCongratulations, now we have successfully SSH into our Ubuntu 20.04 LTS instance.\non Now, time to connect to the instance using terminal or we can say securely SSH into the instance. Select the EC2 instance into which you want to SSH.\nPlease note that some parts of the images in this tutorial will be pixilated or hidden to hide all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rConclusion: This concludes the second section of this tutorial. We have successfully established a secure connection (SSH) with our Ubuntu 20.04 LTS EC2 instance. Let\u0026rsquo;s move on to Part 2 of the tutorial.\n"
},
{
	"uri": "https://omar2cloud.github.io/_template.en-copy/",
	"title": "Create a MySQL database using RDS",
	"tags": [],
	"description": "",
	"content": "RDS | EC2 database program The objective of this tutorial is to create a MySQL database using RDS and access it using a custom Python script from an EC2 instance using an appropriate role.\nLearning Outcomes:  Creating an EC2 instance using the 7 step workflow \u0026amp; SSH Creating a MySQL database using RDS (Use free-tier eligible options) Writing custom application and deploy onto EC2 Able to apply IAM roles to EC2 instance  What you do you need to complete the tutorial:  AWS Account Credentials EC2 Instances (Linux) Shell script environment (any text editor of your choice) Full access to - EC2, RDS, IAM  Step 1: Create Private MySQL Instance:  Ensure your region is set to \u0026ldquo;N Virginia\u0026rdquo; Create a private MySQL RDS: a)\tCreate a \u0026ldquo;Dev/Test\u0026rdquo; MySQL instance b)\tSelect \u0026ldquo;Non publicly\u0026rdquo; available DB c)\tDB instance = \u0026ldquo;bluesky\u0026rdquo;, UID/PWD = root/password. d)\tAvailability Zone = 1a, create a new security group. e)\tIt\u0026rsquo;s very important to ensue that the RDS instance security group has an \u0026ldquo;inbound\u0026rdquo; rule using the VPC CIDR block of the EC2 instance. Otherwise, we will not be able to connect from the EC2 instance.  Let\u0026rsquo;s SSH to the EC2 instance.\n$ ssh -i \u0026quot;My_EC2.pem\u0026quot; ubuntu@ec2-3-84-60-60.compute-1.amazonaws.com\rLet\u0026rsquo;s run the following commands to update the operating system and to download my-sql-client.\n$ sudo apt update\r$ sudo apt install mysql-client\rNow, let\u0026rsquo;s connect to the RDS instance and create the data.\n$ wget https://storage.googleapis.com/skl-training/aws-codelabs/rds/employees.sql\rPlease, replace the place holder [RDS end point] with your RDS end point.\n$ mysql -h [RDS end point] -u root -p\rYou should get the MySQL prompt after entering the password.\n create database employees; use employees; source employees.sql describe employees;  Note the column names in a text editor from the output of the above command.\nLet\u0026rsquo;s close the mysql terminal by typing exit.\nStep 3: Python program for connecting to the RDS instance from EC2 Firstly, let\u0026rsquo;s install Python.\n$ sudo apt install python2-minimal\rNow, we will install the package installer for Python, [pip](https://pypi.org/project/pip/)\n$ sudo apt install python3-pip\rThe following command will install mysql-connector for Python.\n$ sudo pip3 install mysql-connector-python\rWe will also install [PyMySQL](https://pypi.org/project/PyMySQL/) package with contains a pure Python MySQL library.\n$ sudo pip3 install pymysql\rWe will use the rds.py as the template to update the hostname to the endpoint of your RDS instance and run the code\n$ wget https://storage.googleapis.com/skl-training/aws-codelabs/rds/rds.py\rWe need to modify the python code in all the places marked with \u0026ldquo;TBD\u0026rdquo;. Let\u0026rsquo;s access the python code using [nano] code editor. (https://www.nano-editor.org/) To save the file after editing it, Ctrl+X and enter. Let\u0026rsquo;s make sure we have all perantasis in place as shown on the below image.\nNote: nano text editor commands are at the bottom of the nano screen (IDE).\n$ nano rds\rNow, let\u0026rsquo;s run our rds.py script.\n$ python3 rds.py\rConclusion: By the end of this tutorial, we have successfully created a MySQL database using RDS and accessed it using a Python script from an EC2 using an appropriate role.\n"
},
{
	"uri": "https://omar2cloud.github.io/_template/",
	"title": "Docker on Ubuntu Raspberry Pi 4",
	"tags": [],
	"description": "",
	"content": "Updating the Operating System Now, we have a secured connection with our instance, let\u0026rsquo;s update and upgrade the operating system. Updating and upgrading the operating system of the instance is always a good habit to have. We will run the following update/upgrade commands. The -y in the end of the command line will automatically enters \u0026ldquo;yes\u0026rdquo; as a confirmation before installing the updates.\nsudo apt autoremove\rA public folder will be generated, containing all static content and assets for your website. It can now be deployed on any web server. \u0026ndash;\u0026gt;\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/cloudwatch/",
	"title": "Install Unified CloudWatch Agent",
	"tags": [],
	"description": "",
	"content": "Install Unified CloudWatch Agent: The unified CloudWatch agent. It enables you to collect both logs and advanced metrics with one agent. It offers support across operating systems, including servers running Windows Server. This agent also provides better performance. The older CloudWatch Logs agent, which supports the collection of logs from only servers running Linux. AWS strongly recommends migrating to the unified CloudWatch agent.\nThe unified CloudWatch agent enables you to do the following: 1- Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in Metrics Collected by the CloudWatch Agent.\n2- Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.\n3- Retrieve custom metrics from your applications or services using the StatsD and collectd protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers.\n4- Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.\nCreate IAM Role to Use CloudWatch Agent: Access to AWS resources requires permissions. You create an IAM role, an IAM user, or both to grant permissions that the CloudWatch agent needs to write metrics to CloudWatch.\nIn the navigation pane on the left, choose Roles and then Create role.\nFor Choose a use case, choose EC2 under Common use cases.\nIn the list of policies, select the check box next to CloudWatchAgentServerPolicy.\nLet us confirm that CloudWatchAgentServerPolicy appears next to Policies, then choose Create role.\nNow, we will go back to our Ubuntu EC2 instance and attach this role to it.\nChoose our IAM role and save it.\nInstall the unified CloudWatch agent: The following dependencies need to be installed on the EC2 instance ( t2micro running Ubuntu 20.04 LTS) before the agent can be installed:\nsudo apt update \u0026amp;\u0026amp; sudo apt install collectd -y \u0026amp;\u0026amp; sudo apt install awscli -y\rThe following command will walk you through some configurations. The most important one is the region code to point to the region where the EC2 instance is configured. My region is N Virginia, so it should be us-east-1.\naws configure We will use a role for the CloudWatch Agent and will attach to the EC2 instance; therefore, we will leave the AWS Access and Secret keys empty. The result:\nAWS Access Key ID [None]:\rAWS Secret Access Key [None]:\rDefault region name [None]: us-east-1\rDefault output format [None]:\rInstall the agent using the following steps: sudo chown ubuntu:ubuntu -R /opt\rmkdir /opt/softwares\rwget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb\r\rIf you would like to download the agent for a different operating system, please refer to AWS instruction.\n\rLet\u0026rsquo;s install our Ubuntu version of the CloudWatch Agent.\nsudo dpkg -i -E ./amazon-cloudwatch-agent.deb\rTo start the installation wizard use the following command: Note: to enable the \u0026ldquo;usual metrics\u0026rdquo;, you might to select the defaults for all the questions during the wizard setup. Add the following path to collect logs from /var/log/syslog, and select Advanced for the following question: “Which default metrics config do you want?”.\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard\rAWS has a well detailed instruction about this installation wizard, for additional configuration of the CloudWatch Agent wizard.\nOnce the installation is completed, let us run the below commands to run and test the agent.\nTo start the agent:\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s\rTo restart the agent:\nsudo service amazon-cloudwatch-agent stop start status\rThe logs should be in the the below folder:\nls -al /opt/aws/amazon-cloudwatch-agent/logs/\rNotice, once you\u0026rsquo;re on CloudWatch \u0026ldquo;Metrics\u0026rdquo; \u0026ldquo;custom namespace\u0026rdquo;, scroll all the down until you find \u0026ldquo;CWAgent.\u0026quot;\nFrom the top select \u0026ldquo;Numbers\u0026rdquo; as the visualization. In the cloudwatch management console, select metrics under the \u0026ldquo;EC2\u0026rdquo; namespace as well to see the CPU and other numbers.\nA Stress Tool to Simulate a Stress for testing Sysbench is a command line app to run benchmarks on our system/instance. It is mainly intended for testing CPU, memory and file throughput as well. We can install this utility to simulate a stress on our EC2 to push CloudWatch Agent to present it on our metrics.\nTo install Sysbench in Ubuntu, run the command below:\nsudo apt install sysbench\rWe can increase or decrease the threads to simulate the stress.\nsysbench cpu --threads=5 run\rBelow, you will notice clearly the result of the stress on our EC2 instance by Sysbench.\nMany metrics are generated by our CloudWatch Agent:\nI would suggest you manuever CloudWatch dashboard and get familiar with how to make these metrics presentable in a fashion way by adjusting the x and y axises. Also, let\u0026rsquo;s try creating a dashboard and explore all other features that CloudWatch offers.\nConclusion: This concludes our tutorial of the successful installation of CloudWatch Agent.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/x11vnc/",
	"title": "Install x11vnc Server on Ubuntu 20.04 for Remote Access",
	"tags": [],
	"description": "",
	"content": "What is a X11VNC Server? The Virtual Network Computing (VNC) allows to view and interact with devices remotely with any VNC viewer. When working with multiple RPIs, having VNC servers give us access all our RPIs from one device. There are many VNC servers and X11VNC is one of them. On another tutorial, we will install RealVNC, which is another vnc server, and I will go over the differences between X11VNC and RealVNC.\nTo learn more about X11VNC.\nX11VNC is an alternative to Vino. If you\u0026rsquo;re happy with Vino, you don\u0026rsquo;t need to install x11vnc.\n\rLet\u0026rsquo;s Install X11VNC Server on our Ubuntu As a best practice, we shall update and upgrade our operating system.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rNow, we will need to change display manager to lightdm\nsudo apt-get install lightdm\rLet\u0026rsquo;s reboot the Raspberry Pi for these changes to take effect.\nsudo reboot\rThe following command to install x11vnc server\nsudo apt-get install x11vnc\rNow, we will create our service configuration file.\nsudo nano /lib/systemd/system/x11vnc.service\rLet\u0026rsquo;s copy and paste the following configuration into our newly create service file. These configurations will start the x11vnc server and make it run continuously. It will also force the server to restart automatically in case of any failure.\nNote: yourPassword is the password.\n[Unit]\rDescription=x11vnc service\rAfter=display-manager.service network.target syslog.target\r[Service]\rType=simple\rExecStart=/usr/bin/x11vnc -forever -display :0 -auth guess -passwd yourPassword\rExecStop=/usr/bin/killall x11vnc\rRestart=on-failure\r[Install]\rWantedBy=multi-user.target\rThe following commands to reload the systmd system and to enable and start the x11vnc service.\nsystemctl daemon-reload\rsystemctl enable x11vnc.service\rsystemctl start x11vnc.service\rsystemctl status x11vnc.service\rWe can check the status of our x11vnc server as shown.\nx11vnc status\rFinally, let\u0026rsquo;s reboot the RPI 4.\nsudo reboot\rNow, we are able to vnc into our RPI4 from any vnc viewer. We will need the Notice that x11vnc is warning us that we are running this server without a password. Well, let\u0026rsquo;s setup a password.\nIf you have not set your password in the previous step (yourPassword), you could use the following command to setup. You will be promoted to enter a password and to confirm it.\nx11vnc -storepasswd Make sure you have received a similar success message,and that you\u0026rsquo;re password has been written, if not try it again.\npassword written to: /home/\u0026lt;your-login-name\u0026gt;/.vnc/passwd\rConclusion: This concludes our x11vnc server installation on a Ubuntu 20.10 or 20.04 LTS installed on a RPI 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/",
	"title": "Raspberry Pi Tutorials",
	"tags": [],
	"description": "",
	"content": "Raspberry Pi Tutorials Multiple Raspberry Pi Tutorials and Projects\nIf you\u0026rsquo;re interested in Raspberry Pi project but are not sure where to start, let\u0026rsquo;s walk this journey together. In these tutorials, we will learn what a Raspberry Pi is, hardware and software, different models and more resources for projects and tutorials.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/mattermost/",
	"title": "install Mattermost and MySQL on EC2",
	"tags": [],
	"description": "",
	"content": "Mattermost Mattermost is an open-source, self-hostable online chat service with file sharing, search, and integrations. It is designed as an internal chat for organizations and companies, and mostly markets itself as an open-source alternative to Slack and Microsoft Teams. For information about Mattermost.\nAs part of this tutorial, we will install and configure MySQL on Ubuntu 20.04 LTS EC2 instance. The intension is implement two different subnets, Public and Private subnets. The public subnet will host the Mattermost web application, and the private subnet will host the MySQL database for the Mattermost web application. Since MySQL database will be isolated in a private subnet, we will need to associated it with a NAT instance, which will be hosted on the public subnet to allow the MySQL database in the private subnet to send traffic to the internet gateway. For more information about the functionality of NAT instance.\nThe below diagram is our design architecture for this tutorial.\nStep 1: VPC 1- Create a VPC.\n2- Name the VPC and choose the CIDR block for the VPC as 10.0.0.0/16 3- Select the VPC, edit DNS hostnames and enable DNS hostnames.\nWe have successfully created our VPC.\nStep: Create Public and Private Subnets 1- Navigate to Subnets and then click on \u0026ldquo;Create Subnet\u0026rdquo;.\n2- Make sure you select the newly created VNC and not the default VPC.\n3- Name this subnet as a Public_SN as in my case. It\u0026rsquo;s optional of you would like to select a specific Availability Zone. The CIDR block that does not conflict with any in the same VPC. Let choose 10.0.1.0/24 as our CIDR block for the public subnet and then click on create subnet.\n4- Since this is our designated Public Subnet, we will need to click on \u0026ldquo;Actions\u0026rdquo;, \u0026ldquo;Modify auto-assign IP setting\u0026rdquo; and enable it. If we don\u0026rsquo;t enable auto-assign public IPv4 address, our subnet will not be abe to allocate public IP addresses to instances within the subnet. To read more about IP Addressing in VPC.\n5- Now, we will create a Private_SN. It\u0026rsquo;s optional of you would like to select a specific Availability Zone. I will select a different AZ than the one I have my public subnet. Let choose a CIDR block that does not conflict or overlap with any other CIDR block in our VPC. I chose 10.0.2.0/24 as our CIDR block for the private subnet and then click on create subnet.\nNote: don’t enable auto-assign public IPv4 address for the private subnet\nStep 2: Internet Gateway In order for the VPC to communicate with the internet, we will need to create and attach an **Internet Gateway\u0026quot;\u0026quot; to our architecture.\n1- Let\u0026rsquo;s navigate to VPC, Internet Gateway, name it and then click on \u0026ldquo;Create Internet Gateway.\u0026rdquo;\n2- The Internet Gateway should be attached to our VPC by clicking on \u0026ldquo;Actions\u0026quot;and \u0026ldquo;Attach to VPC.\u0026rdquo;\n3- We will select our newly created VPC and click on \u0026ldquo;Attach internet Gateway.\u0026rdquo;\nStep 3: Route tables The Route Tables contain a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. Each subnet in our VPC must be associated with a route table, which controls the routing for the subnet. We can explicitly associate a subnet with a particular route table. Otherwise, the subnet is implicitly associated with the main route table. A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same subnet route table.\n1- Let\u0026rsquo;s create a public route table by navigating to \u0026ldquo;Route Tables\u0026rdquo; and then clicking on \u0026ldquo;Create route table.\u0026rdquo;\n2- After naming the public route table, make sure you are selecting our newly created VPC, and then click on \u0026ldquo;Create.\u0026rdquo;\n3- Now, we will add a route to out Internet Gateway. Let\u0026rsquo;s click on \u0026ldquo;Routes\u0026rdquo; and then \u0026ldquo;Edit routes.\u0026rdquo;\n4- Click on \u0026ldquo;add route\u0026rdquo;, choose Destination as 0.0.0.0/0, select the Target as our Internet Gateway and click on Save routes.\n5- Now, we will need to associate our public subnet with this public route table. Let\u0026rsquo;s click on Subnet Associations, Edit subnet associations, check our public subnet and click Save.\nAs a best practice, let\u0026rsquo;s inspect Routes and Subnet Associations to ensure they reflect the association of public route and public subnet as well as thr attachment of Internet Gateway to this public subnet.\nNote, we will also create a private route table but after we spin our NAT instance.\nStep 4: NAT instance Our MySQL database will be hosted in a private subnet;therefore, we will utilize a NAT instance to send traffic to the internet gateway. AWS recommends using NAT Gateway instead a NAT instance, but for this tutorial, we will use a community available NAT instance.\n1- Let\u0026rsquo;s navigate to EC2, select Instances and Launch Instances. From the lest menu, click on Community AMIs and then type in NAT in the search bar. My NAT AMI selection is shown on the image below.\n2- To keep the tutorial within the Free Tier eligible, I will select t2.micro for the Instance Type. On the Configuration Instance Details, select the VPC(MyVPC) and Public Subnet(Public_SN).\n3- No need to add more than 8 GiB storage size. It\u0026rsquo;s best practice to add tags.\n4- We shall create a security group for the NAT instance. We will open ports 80 and 443 only. These are the two port, which we need to allow traffic from our MySQL database instance. Then, let\u0026rsquo;s review and launch our NAT instance.\n5- let\u0026rsquo;s review, launch our NAT instance and download our Key pair name.\n6- In any NAT instance, we must stop source/destination checking. This allows the NAT instance to send and receive traffic when the source or rhe destination is not itself.\nStep 5: Private Route Table 1- Now, we will create a private route table using the steps for the public route table.\n2- Click on \u0026ldquo;add route\u0026rdquo;, choose Destination as 0.0.0.0/0, select the Target as our NAT instance and click on Save routes.\n3- Now, we will need to associate our private subnet with this private route table. Let\u0026rsquo;s click on Subnet Associations, Edit subnet associations, check our private subnet and click Save.\nLet\u0026rsquo;s inspect our final associations to ensure all is in place. Step 6: MySQL Database Instance 1- Navigate to EC2, select Launch instance and select AMI Ubuntu Server 18.04 LTS.\n2- We will keep the Instance Type as t2.micro. On the Configure Instance Details, select our VPC and place the instance in the private subnet.\n3- No changes to the storage size but let\u0026rsquo;s add a tag. We will configure a security group to open ports 22 and 8065. We will need to open port 22 so we can SSH from the Mattermost web app into the MySQL database instance. Port 8065 is for the MySQL instance to communicate with the Mattermost web app.\n4- Let\u0026rsquo;s create a new key pair and launch the instance.\nStep 7: Mattermost Instance 1- We will choose the same AMI we used previously for the MySQL instance, which is Ubuntu Server 20.04 LTS.\n2- On Configure Instance Details, make sure that you select the correct VPC and Public Subnet.\n3- We will configure a security group to open ports 22 and 3306. We will need to open port 22 so we can SSH to the Mattermost web app and port 3306 to run the Mattermost web app.\n4- We will not generate new key pair instead we will use MySQL key pair. We will this key pair to SSH into the Mattermost instance as well as SSH into MySQL database instance. I have refrained from generating a different key pair to avoid confusion; however, in a production scenario, I will use different key pairs.\nStep 8: MySQL Installation 1- We will use the below scp command to copy MySQL database pem file into the Mattermost instance. Replace the x.xxx.xxx.xx with your Mattermost instance\u0026rsquo;s public IP address. We could use this command even if we are not SSH into the instance.\nExample:\nscp -i\u0026lt;application server pem file\u0026gt; \u0026lt;database server pem file\u0026gt; ubuntu@\u0026lt;application public IP address\u0026gt;:/home/ubuntu\rActual:\nscp -i MySQL.pem MySQL.pem ubuntu@3.235.137.90:/home/ubuntu\rThe result:\n2- We will SSH into the Mattermost instance. There are many methods to SSH into an instance depending on the device. To learn about how to SSH into our Ubuntu instance, please refer to my previous tutorial.\n3- From the Mattermost instance, we will SSH into the MySQL database instance. We will use the pem file copied in step 1. Also, since our MySQL instance is in a private subnet, it does not have a public IP address; therefore, we will use its private IP address to SSH into it from the Mattermost most instance. With the pem file and the private IP address to our MySQL instance, we can SSH from our Mattermost instance.\nFirstly, let\u0026rsquo;s secure the pem file in the Mattermost instance but restricting access to it using the command chmod 600.\nsudo chmod 600 MySQL.pem\r4- Now, we will attempt to SSH into MySQL database instance.\nExample:\nssh -i \u0026lt;database server pem file\u0026gt; ubuntu@\u0026lt;private IP of database server\u0026gt;\rActual:\nssh -i MySQL.pem ubuntu@10.0.2.60\r5- Now, we have successfully SSH into the MySQL database instance, let\u0026rsquo;s install MySQL database.\nAs a best practice, let\u0026rsquo;s update the instance first.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rwget https://raw.githubusercontent.com/OmarCloud20/aws-tutorials/main/install_mysql.sh\r6- To configure MySQL, let\u0026rsquo;s run the following commands.\nchmod 700 install_mysql.sh\rsudo ./install_mysql.sh\rWe have installed and configured MySQL database successfully. Let\u0026rsquo;s exit the instance by typing exit.\nStep 9: Mattermost Installation 1- We will use the following bash file to install and configure Mattermost app.\nwget https://raw.githubusercontent.com/OmarCloud20/aws-tutorials/main/mattermost_install.sh\rchmod 700 mattermost_install.sh\r2- We will use MySQL instance private IP address to connect Mattermost to the database.\nExample:\nsudo ./mattermost_install.sh \u0026lt;private IP of MySQL server\u0026gt;\rActual:\nsudo ./mattermost_install.sh 10.0.2.60\r3- Final configuration:\nsudo chown -R mattermost:mattermost /opt/mattermost\rsudo chmod -R g+w /opt/mattermost\rcd /opt/mattermost\rsudo -u mattermost ./bin/mattermost\rStep 10: Running Mattermost Web Application To inspect the deployment of the Mattermost web application, let\u0026rsquo;s navigate to the browser URL bar and type in, :8065\nCongratulation, we have installed and configured Mattermost application and connect it with its MySQL database on Ubuntu 20.04 and Ubuntu 18.04 LTS respectively.\nConclusion: By the end of this tutorial, we have successfully created Mattermost web app, MySQL database and NAT instances on AWS EC2.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/realvnc/",
	"title": "Install RealVNC Server on Ubuntu 20.04 for Remote Access",
	"tags": [],
	"description": "",
	"content": "What is a RealVNC Server? RealVNC allows us to interact with our Raspberry Pis graphically via Virtual Network Computing (VNC). The RealVNC server comes preinstalled with the Raspberry Pi OS. It\u0026rsquo;s extremely secure, convenient and reliable. Until the moment of writing this tutorial, 5/15/2021, RealVNC allows us to have up to 5 subscribed devices to remote into via cloud connectivity for FREE. By using this feature, we can remote into our devices from anywhere in the world without a VPN, a port-forwarding or a firewall configuration. If you\u0026rsquo;d like to subscribe more devices, it won\u0026rsquo;t be covered under the free subscription plan. However, you could access more devices from within the local network using direct feature.\nSince we have installed Ubuntu, which is a 64bit-OS, on our RPI-4, and RealVNC only provides a 32bit server for Raspberry Pi OS, we will need to go through some hoops to get RealVNC installed and running. During this tutorial, I will go over how to install RealVNC on Ubuntu 20.04 LTS.\nI have successfully installed and tested RealVNC based on multiple resources such, resource 1 and resource 2.\n\rStep 1: Install RealVNC Server on our Ubuntu 20.04 LTS Let\u0026rsquo;s navigate to our terminal. As a best practice, we shall update and upgrade our operating system.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rLet\u0026rsquo;s download the arm64 package from the Raspberry Pi Foundation\u0026rsquo;s site using the following command.\nwget https://archive.raspberrypi.org/debian/pool/main/r/realvnc-vnc/realvnc-vnc-server_6.7.2.43081_arm64.deb\rNow, we will install the package using dpkg command:\nsudo dpkg -i realvnc-vnc-server_6.7.2.43081_arm64.deb\rHere, we will add specific files to /user/lib/ folder, so let cd into it:\ncd /usr/lib/aarch64-linux-gnu\rLet\u0026rsquo;s add the following 10 files to the folder:\nsudo ln libvcos.so /usr/lib/libvcos.so.0\rsudo ln libvchiq_arm.so /usr/lib/libvchiq_arm.so.0\rsudo ln libbcm_host.so /usr/lib/libbcm_host.so.0\rsudo ln libmmal.so /usr/lib/libmmal.so.0\rsudo ln libmmal_core.so /usr/lib/libmmal_core.so.0\rsudo ln libmmal_components.so /usr/lib/libmmal_components.so.0\rsudo ln libmmal_util.so /usr/lib/libmmal_util.so.0\rsudo ln libmmal_vc_client.so /usr/lib/libmmal_vc_client.so.0\rsudo ln libvcsm.so /usr/lib/libvcsm.so.0\rsudo ln libcontainers.so /usr/lib/libcontainers.so.0\rFinally, let\u0026rsquo;s enable and start the following services:\nsudo systemctl enable vncserver-virtuald.service\rsudo systemctl enable vncserver-x11-serviced.service\rsudo systemctl start vncserver-virtuald.service\rsudo systemctl start vncserver-x11-serviced.service\rLet\u0026rsquo;s reboot our Raspberry Pi for these changes to take effect.\nsudo reboot\rNote: if RealVNC server GUI has not started automatically, follow the following steps. On the default display manager, select lightdm.\nsudo apt-get install lightdm\rsudo reboot\r\rThis installation will also work on Ubuntu 20.10 as well.\n\rStep 2: Signup for a Free RealVNC Team\u0026rsquo;s Account   Signup for a free RealVNC Team\u0026rsquo;s Account.\n  On your RPI-4, signin with your credentials to add this RPI-4 to your Computers' account.\n  Notice, the Connectivity is Direct and Cloud. Now, your RPI-4 is ready to receive a secured VNC connection whether it\u0026rsquo;s a direct or cloud connection.\nTo manage your devices, navigate to Computers under TEAM on your profile page and you shall see all subscribed devices to Cloud. Remember, you may only add up to 5 devices using the free subscription.  Step 3: Install RealVNC Viewer Now, we have successfully installed RealVNC server, signup for an account and subscribed our device, we will need a method to remote into our RPI-4. Navigate to RealVNC to download a VNC Viewer to your viewer device. This could be your iPhone, Android phone, Mac Pro or your Windows machine.\n After installing the VNC Viewer, signin with your credentials and you shall see all subscribed devices for cloud connection automatically. You may also add direct connection device on the Address book section.  Double click on your RPI-4, in my case it\u0026rsquo;s p4. Click on Continue.  Enter your RPI-4\u0026rsquo;s username and password to connect via VNC. These are the credentials you\u0026rsquo;re using to signin into your RPI-4.  Conclusion: This concludes our RealVNC server installation on a Ubuntu 20.04 LTS (or 20.10) on a RPI 4. We have successfully installed and configured RealVNC server on our RPI-4 arm64. Moreover, we have installed RealVNC Viewer and established a cloud VNC connection securely to our RPI-4. Cheers!\n"
},
{
	"uri": "https://omar2cloud.github.io/cont/pages/",
	"title": "Pages organization",
	"tags": [],
	"description": "",
	"content": "In Hugo, pages are the core of your site. Once it is configured, pages are definitely the added value to your documentation site.\nFolders Organize your site like any other Hugo project. Typically, you will have a content folder with all your pages.\ncontent\r├── level-one\r│ ├── level-two\r│ │ ├── level-three\r│ │ │ ├── level-four\r│ │ │ │ ├── _index.md \u0026lt;-- /level-one/level-two/level-three/level-four\r│ │ │ │ ├── page-4-a.md \u0026lt;-- /level-one/level-two/level-three/level-four/page-4-a\r│ │ │ │ ├── page-4-b.md \u0026lt;-- /level-one/level-two/level-three/level-four/page-4-b\r│ │ │ │ └── page-4-c.md \u0026lt;-- /level-one/level-two/level-three/level-four/page-4-c\r│ │ │ ├── _index.md \u0026lt;-- /level-one/level-two/level-three\r│ │ │ ├── page-3-a.md \u0026lt;-- /level-one/level-two/level-three/page-3-a\r│ │ │ ├── page-3-b.md \u0026lt;-- /level-one/level-two/level-three/page-3-b\r│ │ │ └── page-3-c.md \u0026lt;-- /level-one/level-two/level-three/page-3-c\r│ │ ├── _index.md \u0026lt;-- /level-one/level-two\r│ │ ├── page-2-a.md \u0026lt;-- /level-one/level-two/page-2-a\r│ │ ├── page-2-b.md \u0026lt;-- /level-one/level-two/page-2-b\r│ │ └── page-2-c.md \u0026lt;-- /level-one/level-two/page-2-c\r│ ├── _index.md \u0026lt;-- /level-one\r│ ├── page-1-a.md \u0026lt;-- /level-one/page-1-a\r│ ├── page-1-b.md \u0026lt;-- /level-one/page-1-b\r│ └── page-1-c.md \u0026lt;-- /level-one/page-1-c\r├── _index.md \u0026lt;-- /\r└── page-top.md \u0026lt;-- /page-top\r _index.md is required in each folder, it’s your “folder home page”\n\rTypes Hugo-theme-learn defines two types of pages. Default and Chapter. Both can be used at any level of the documentation, the only difference being layout display.\nA Chapter displays a page meant to be used as introduction for a set of child pages. Commonly, it contains a simple title and a catch line to define content that can be found under it. You can define any HTML as prefix for the menu. In the example below, it\u0026rsquo;s just a number but that could be an icon.\n+++ title = \u0026#34;Basics\u0026#34; chapter = true weight = 5 pre = \u0026#34;\u0026lt;b\u0026gt;1. \u0026lt;/b\u0026gt;\u0026#34; +++ ### Chapter 1  # Basics Discover what this Hugo theme is all about and the core-concepts behind it. To tell Hugo-theme-learn to consider a page as a chapter, set chapter=true in the Front Matter of the page.\nA Default page is any other content page.\n+++ title = \u0026#34;Installation\u0026#34; weight = 15 +++ The following steps are here to help you initialize your new website. If you don\u0026rsquo;t know Hugo at all, we strongly suggest you to train by following this great documentation for beginners.\nCreate your project Hugo provides a new command to create a new website.\nhugo new site \u0026lt;new_project\u0026gt;\rHugo-theme-learn provides archetypes to help you create this kind of pages.\nFront Matter configuration Each Hugo page has to define a Front Matter in yaml, toml or json.\nHugo-theme-learn uses the following parameters on top of Hugo ones :\n+++ # Table of content (toc) is enabled by default. Set this parameter to true to disable it. # Note: Toc is always disabled for chapter pages disableToc = \u0026#34;false\u0026#34; # If set, this will be used for the page\u0026#39;s menu entry (instead of the `title` attribute) menuTitle = \u0026#34;\u0026#34; # The title of the page in menu will be prefixed by this HTML content pre = \u0026#34;\u0026#34; # The title of the page in menu will be postfixed by this HTML content post = \u0026#34;\u0026#34; # Set the page as a chapter, changing the way it\u0026#39;s displayed chapter = false # Hide a menu entry by setting this to true hidden = false # Display name of this page modifier. If set, it will be displayed in the footer. LastModifierDisplayName = \u0026#34;\u0026#34; # Email of this page modifier. If set with LastModifierDisplayName, it will be displayed in the footer LastModifierEmail = \u0026#34;\u0026#34; +++ Add icon to a menu entry In the page frontmatter, add a pre param to insert any HTML code before the menu label. The example below uses the Github icon.\n+++ title = \u0026#34;Github repo\u0026#34; pre = \u0026#34;\u0026lt;i class=\u0026#39;fab fa-github\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; \u0026#34; +++ Ordering sibling menu/page entries Hugo provides a flexible way to handle order for your pages.\nThe simplest way is to set weight parameter to a number.\n+++ title = \u0026#34;My page\u0026#34; weight = 5 +++ Using a custom title for menu entries By default, Hugo-theme-learn will use a page\u0026rsquo;s title attribute for the menu item (or linkTitle if defined).\nBut a page\u0026rsquo;s title has to be descriptive on its own while the menu is a hierarchy. We\u0026rsquo;ve added the menuTitle parameter for that purpose:\nFor example (for a page named content/install/linux.md):\n+++ title = \u0026#34;Install on Linux\u0026#34; menuTitle = \u0026#34;Linux\u0026#34; +++ Homepage To configure your home page, you basically have three choices:\n Create an _index.md document in content folder and fill the file with Markdown content Create an index.html file in the static folder and fill the file with HTML content Configure your server to automatically redirect home page to one your documentation page  "
},
{
	"uri": "https://omar2cloud.github.io/aws/lambda/",
	"title": "AWS Lambda Custom Layers and a Lambda Function in Python",
	"tags": [],
	"description": "",
	"content": "What is AWS Lambda? AWS Lambda is a serverless compute service, which allow the user to run codes without provisioning or managing servers. With Lambda, the user does not manage runtimes nor admin the server. Utilizing Lambda is as simple as uploading a code in a ZIP file or a container image, and Lambda automatically allocates compute execution power and runs the code based on the incoming request or event. Lambda functions can be written in many flavors such as, Node.js, Python, Go, Java, and more. For more information about Lambda.\nIn this tutorial, we will explore two important aspects of AWS Lambda, which are Custom Layers and Lambda Function in Python using layers.\nTutorial Scenario:  A user uploads a PDF to S3 bucket. The bucket PUT event triggers a lambda function. The lambda function inspects the JSON and extract the bucket and the file names from it. The file is read using boto3 - AWS Python library. Apache Tika library is to parse the PDF and to extract metadata and content. The outcome text is saved in a different/destination S3 bucket.  Then, We shall create a common layer containing the 3rd part library dependency of Apache Tika. Finally, code a lambda function in python.  Step 1: Spin a Ubuntu 18.04 LTS instance and Configure App File:  Spin a Ubuntu 18.04 LTS instance. Ensure port 22 is open to SSH. SSh into the instance and run the following commands:  As a best practice, let\u0026rsquo;s update our Ubuntu operating system.\nsudo apt update\rLet\u0026rsquo;s install the zip utility.\nsudo apt install zip -y\rpip is the package installer for Python. Let\u0026rsquo;s install pip.\nsudo apt install python3-pip\rThe chown command allows us to change the user and/or group ownership of a given file or directory.\nsudo chown ubuntu:ubuntu -R /opt\rWe will cd into opt directory. cd is the command used to move between directories/folders.\ncd /opt\rmkdir is the command to create directories.\nmkdir -p appzip/python\rLet\u0026rsquo;s cd into app directory.\ncd appzip/\rNow, we will use the zip utility to zip the appzip.\nzip -r appzip.zip python\rWe will download the zip file to our device. We will use Secure Copy scp, which is command line utility to allow us to securely copy files and directories between devices.\nFrom your device\u0026rsquo;s terminal, use the following command to download the zip file to your local device. For more information, visit AWS.\nscp -i \u0026lt;Name\u0026gt;.pem ubuntu@\u0026lt;EC2-Public-IP OR DNS\u0026gt;:/opt/appfolder/appfolder.zip ./appfolder.zip\rI\u0026rsquo;m using AWS CLIv2 via my Windows Command Line:\nC:\\Users\\Omar-PC\\Desktop\u0026gt;scp -i Lambda.pem ubuntu@XX.XXX.XX.XXX:/opt/appfolder/appfolder.zip C:\\Users\\Omar-PC\\Desktop\rStep 2: AWS Lambda Layer Configuration On AWS lambda, click on \u0026ldquo;Layers\u0026rdquo;, give it a name, description and upload the zip file (appzip). So far, we created a lambda layer as simple as that.\nStep 3: Lambda Role and S3 configuration  Create an IAM role which allows the lambda function to access other needed resources.   Add the following policies to the role, \u0026ldquo;AmazonS3Full Access and CloudWatchFull Access.\n  Name the role, lambda-pdf-extractor.\n   On S3, create two buckets:\nA. First bucket name: source[5 random numbers]\nB. Second bucket name: destination[5 random numbers]\n  Note: S3 bucket names are global names, which means they are unique. Two identical bucket names don\u0026rsquo;t exist.\nUpload a random PDF to the sourcexxxxxx bucket (under 200kb of size). Save the name of the PDF, we will need it down the road.  Step 4: Python Lambda Function Now, it\u0026rsquo;s time to create the lambda function using the layer and the role we have previously created.\n On the Lambda console, click on \u0026ldquo;Create function.\u0026rdquo; Function name: PDFExtractor Select \u0026ldquo;Author from scratch.\u0026rdquo; Runtime: Python 3.6. Permissions: \u0026ldquo;Use an existing role\u0026rdquo; and select the role we have previously created, lambda-pdf-extractor. Click on \u0026ldquo;Create function.\u0026rdquo;   On the lambda function code editor, remove the existing code and replace it with the code form my GitHub repo.\n  On Layers section, add a Custom layers by selecting our previously created Layer (TikaLayer) version 1.\n  On Configuration, Environment variables, add the following:     key value     TARGET_BUCKET destinationxxxxx    Note: replace destinationxxxxx with the name of your destination S3 bucket.\n On Configuration, General configuration, edit to the following:\nA. Memory: 256 MB\nB. Timeout: 2 min\nC. On the Existing role, select our previously created role, lambda=pdf-extractor\n   Next, select Test, and on the Template dropdown menu, select \u0026ldquo;Amazon S3 Put\u0026rdquo;\n  Name the event: s3put\n  On the JSON section, replace the JSON with this one from my Github. Let\u0026rsquo;s ensure that you update the following:\nA. The AWS region. In my case, it is us-east-1\nB. The bucket name. This should match the name of your S3 source bucket.\nC. The object key. The object name should match the name of your PDF.\n  Click on Save changes.\n  Step 5: Testing the Python Lambda Function Now, we are ready to test our function by selecting \u0026ldquo;Code, our \u0026ldquo;s3put\u0026rdquo; and click \u0026ldquo;test\u0026rdquo; button. If you have received status code 200, it means our code executed successfully.\nIf the function fails, it will state error \u0026ldquo;Unable to start the Tika server.\u0026rdquo; If this occurs, run the test one more time.\nWhen the lambda function executed successfully, the extracted PDF is saved in our S3 destination bucket. It includes the result of the PDF text extraction, as shown below.\nConclusion: By the end of this tutorial, we have successfully created our AWS Lambda Function which extracts texts from uploaded PDFs in a S3 source bucket. The extracted texts are saved in a designated S3 destination bucket. We have achieved this task by utilizing a Custom Layer and a Lambda Function in Python using layers.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/headless/",
	"title": "Setting UP RPI to Run Headless",
	"tags": [],
	"description": "",
	"content": "What is Headless? Headless is the term used when we run our RPI without a monitor or a keyboard. I like to setup and configure my RPI using a monitor at least for the first time only, but if we are hosting a server with no desktop GUI on a RPI, setting up headless is the way to go.\nPlease, refer to Raspberry Pi Foundation for information on how to setup RPI headless\n\rMy RPI does not boot without HDMI connected! I have been there. We would like to remote into our RPIs headless via vnc or ssh..etc. However, we will face some hiccups along the way.\nModify Config.txt to resolves headless and vnc resolution: We need to make minor modifications to the configuration file on our RPI to boot up without monitor. Moreover, we will stumble into resolution hiccups after making this modifications. Here is the resolution:\nRPI running Raspberry Pi OS : The following command to access the configuration text file.\nsudo nano /boot/config.txt\rLet\u0026rsquo;s make the necessary modifications according to the below image.\nTo save the file:\n hold and click: CTRL-x type: y click: enter.  RPI running Ubuntu: The following command to access the configuration text file.\nsudo nano /boot/firmware/config.txt\rThe above process should allow the RPI to boot up without a monitor or keyboard. The resolution distortion should be resolved as well.\nUpdate (6/8/2021): I recently provisioned a new RPI-4 to run Ubuntu Desktop 21.04. I also followed the same steps above to run the RPI-4 without a monitor; however, the RPI-4 had not worked without a monitor. Therefore, I have done my research and the work around relies on the Config.txt file.\nLet\u0026rsquo;s start by accessing the configuration text file using the following command on our terminal:\nsudo nano /boot/firmware/config.txt\rThen, we will perform the following:\n1- Comment out dtoverlay=vc4-kms-v3d\n2- Add the following:\n A. hdmi_force_hotplug=1\rB. framebuffer_width=1920\rC. framebuffer_height=1080\r The Config.txt should like the image below:\nFinally, to save the file in the nano text editor:\n hold and click: CTRL-x type: y click: enter  Conclusion: Now, we are able to run Ubuntu 20.04 on RPI-4 without a need for a monitor. To access the RPI-4 remotely, refer to my tutorial about RealVNC.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/lambda_web/",
	"title": "Application Load Balancer with Lambda Backend",
	"tags": [],
	"description": "",
	"content": "Application Load Balancer An Application Load Balancer has the ability to send traffic to a specific target group (TG) based on path. The default path is to send all the traffic to a given Target Group.. Usually a TG comprises of infrastructure components either on the cloud (EC2 instances) or from on-prem data-center (via IP).\nHowever, there is an exciting feature by which the target group can contain a lambda function as the backend. By the end of this tutorial, we should successfully utilize an ALB to direct traffic to a Lambda function backend. The Lambda function is written in Python and is target group for the ALB.\nPlease note that some parts of the images in this tutorial will be pixilated or hidden to hide all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rArchitecture Implementation - Tutorial Scenario:  Create the lambda function using the console, name it \u0026ldquo;web-backend.\u0026rdquo; Create a lambda IAM role. Create an instance of ALB. When creating the TG, select lambda. Select \u0026ldquo;web-backend\u0026rdquo; lambda from the dropdown. Enable health check. Create the ALB and TG and the wait for status to change from provisioning to active before attempting to access the DNS.  Step 1: Lambda Role  Create an IAM role which allows the lambda function to access other needed resources.  Add the following policy to the role CloudWatchFullAccess.  Step 2: Python Lambda Function Lets' create a lambda function and attach the role we have previously created.\n On the Lambda console, click on \u0026ldquo;Create function.\u0026rdquo; Function name: web_backend Select \u0026ldquo;Author from scratch.\u0026rdquo; Runtime: Python 3.7. Permissions: \u0026ldquo;Use an existing role\u0026rdquo; and select the role we have previously created, Lambda_web_Backend. Click on \u0026ldquo;Create function.\u0026rdquo;  On the lambda function code editor, remove the existing code and replace it with the code from my GitHub repo. Then, click Deploy.  Step 3: Create an Instance of Application Load Balancer  Navigate to EC2 console, select Load Balancers and Create Load Balancer.  Select an Application Load Balancer as the type of a load balancer.  Configure the ALB as shown below. At least, select two AVailability Zones.  On the configuration of the security group, create a new security group and make sure you have port 80 open to receive http traffic.  Add a new target group as a Lambda function and enable Health check. Also, add /lambda/ to the health check path as shown below.  We will select our lambda function from the drop down menu to register it as a target. If you have multiple versions of lambda function, select the version you\u0026rsquo;d like to register as a target.  Review the configuration and then click on Create.  We will need to wait until our ALB status is updated from provisioning to active. Then, we get the DNS name of the ALB and paste it on the browser.  Conclusion: Congratulations, we have successfully employed an Application Load Balancer to direct traffic to our Lambda function backend. This Lambda function is written in Python and is target group for the ALB.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/rpidock/",
	"title": "Docker on Ubuntu 20.04 Raspberry Pi 4",
	"tags": [],
	"description": "",
	"content": "\rAs of 11/25/21, if you were unable to install Docker on RPI-4 running Ubuntu 21.10, you may need to install extra kernel modules by running the following command $ sudo apt install linux-modules-extra-raspi\n\rUpdating the Operating System Now, we have a secured connection with our instance, let\u0026rsquo;s update and upgrade the operating system. Updating and upgrading the operating system of the instance is always a good habit to have. We will run the following update/upgrade commands. The -y in the end of the command line will automatically enters \u0026ldquo;yes\u0026rdquo; as a confirmation before installing the updates.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rCleaning up after an update If this is not the packages first time to run an update, there might have some unnecessary packages left for cleaning. Removing these packages will free space and prevent your system from cluttering. The following command shall do the job.\nsudo apt autoremove\rStep 1: Install Docker There are different methods to install Docker Engine. The following approach is the method from the official Docker site.\nWe will setup Docker\u0026rsquo;s repository and then install from them for ease of installation and upgrade tasks as it\u0026rsquo;s the recommended approach.\n1 - Setup the Repository:\n sudo apt-get update\r sudo apt-get install \\\rapt-transport-https \\\rca-certificates \\\rcurl \\\rgnupg \\\rlsb-release\r2- Add Docker\u0026rsquo;s Official GPG Key:\nThe commands below is to download and install Docker’s official GPG key, which is used to validate packages installed from Docker’s repository making sure they’re trusted.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\r\rIf you were unable to download the GPG key, check if you have the CURL command-line utility installed on your Ubuntu 20.04 by typing $ curl \u0026ndash;version. If you need to install the CURL, type in $ sudo apt-get install curl -y\n\rcurl --version\rsudo apt-get install curl -y\r3 - Setup the Stable Repository:\nDocker Engine is supported on x86_64 (or amd64), armhf, and arm64 architectures. We should know that RPI 4 is an arm64 architecture, but the Raspberry Pi OS is a 32-bit operating system. We have decided to utilize Ubuntu 20.04 LTS on our RPI 4 because it\u0026rsquo;s a 64-bit operating system. Therefore, we will need to install Docker Engine that supports 64-bit operating system on an arm64 architecture such as our current Ubuntu 20.04 LTS on RPI 4.\nI would like to suggest studying the differences between amd64, 64 and armhf.\nNow, let\u0026rsquo;s use the following command to setup the stable repository for arm64.\n echo \\\r\u0026quot;deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\r$(lsb_release -cs) stable\u0026quot; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null\r4 - Install Docker Engine:\nFirstly, we will update the apt package index and then install the latest version of Docker Engine and containerd.\nsudo apt-get update\rsudo apt-get install docker-ce docker-ce-cli containerd.io\rLet\u0026rsquo;s verify that we have installed Docker Engine by running the hello-world image.\nsudo docker run hello-world\rThe response should be similar to the below:\nHello from Docker!\rThis message shows that your installation appears to be working correctly.\rTo generate this message, Docker took the following steps:\r1. The Docker client contacted the Docker daemon.\r2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub.\r(arm64v8)\r3. The Docker daemon created a new container from that image which runs the\rexecutable that produces the output you are currently reading.\r4. The Docker daemon streamed that output to the Docker client, which sent it\rto your terminal.\rTo try something more ambitious, you can run an Ubuntu container with:\r$ docker run -it ubuntu bash\rShare images, automate workflows, and more with a free Docker ID:\rhttps://hub.docker.com/\rFor more examples and ideas, visit:\rhttps://docs.docker.com/get-started/\rLet\u0026rsquo;s run the following command to find out the Docker version we have just installed.\ndocker version\rYou might notice that we have received a permission denied message because we have not used sudo in front of the command and we are not the root user. But, what if we don\u0026rsquo;t want to use sudo all the times; in this case, we will assign the non-root user to docker group as follows:\nsudo usermod -aG docker $USER\rNote: exit out of the terminal for the change to take effect, and check the docker verion once more.\nNow, we have installed Docker successfully, let\u0026rsquo;s move on to installing Docker-Compose.\nStep 2: Install Docker-Compose Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration. To learn more about all the features of Compose, see the list of features.\nIf you try to install Docker-Compose from Docker\u0026rsquo;s official website, you will face some hiccups as shown below.\nsudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose\rDocker does not have a release for Docker-Compose for arm64 yet; however, I have a shell script which will allow us to download a compatible Docker-Compose container with arm64 from my GitHub.\nTo install it, run the commands below to download version 1.28.5 As of this writing, this was the current version.\nsudo curl -L --fail https://github.com/OmarCloud20/docker-compose/releases/download/1.28.5/run.sh -o /usr/local/bin/docker-compose\rif 1.28.5 is not th latest Docker-Compose version, you could use the below command to download the latest version.\nsudo curl -L --fail https://github.com/AppTower/docker-compose/releases/download/latest/run.sh -o /usr/local/bin/docker-compose\rLet\u0026rsquo;s apply executable permissions to the binary:\nsudo chmod +x /usr/local/bin/docker-compose\rLet\u0026rsquo;s test our installation.\ndocker-compose --version\rWe should see a result similar to the below.\ndocker-compose version 1.28.5, build c4eb3a1f\rReboot your RPI 4.\nsudo reboot\rSo far, we have installed Docker and Docker-Compose successfully. We will move on to installing Portainer.\nSome useful Docker commands: To list all Docker images:\ndocker images\rTo list all Docker image port information:\ndocker ps\rPrior to removing an image, make sure it\u0026rsquo;s container is not running.\ndocker stop [container ID]\rTo force a removal of a Docker image (portainer/portainer:tag is an example of the image name and tag):\ndocker image rm -f portainer/portainer:latest\rTo force removal of a Docker image by the Image ID.\ndocker rmi -f [image ID]\rStep 3: Install Portainer Now that Docker and Docker Composer are installed, follow the steps below to get Portainer setup.\nYou can use Docker command to deploy the Portainer Server; note the agent is not needed on standalone hosts, however, it does provide additional functionality if used.\nTo get the server installed, run the commands below. Firstly, let\u0026rsquo;s get to home directory.\ncd ~/\rLet\u0026rsquo;s run the following command to download the Portainer image that is compatible with our RPI 4 64arm processor.\ndocker pull portainer/portainer-ce:latest\rIf you download a Docker image that is incompatible with the RPI 4 arm64 architecture, you will get a warning once you try to create a container as shown below.\nWARNING: The requested image's platform (linux/arm/v7) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\rWe will create Portainer data volume first.\ndocker volume create portainer_data\rNow, we will create a container which will run Portainer. We will dedicate port 9000 to Portainer; however, you could allocate another port if you\u0026rsquo;re using port 9000 with another application.\nsudo docker run -d -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce\rYou’ll just need to access the port 9000 of the Docker engine where Portainer is running using your browser.\nNote: the -v /var/run/docker.sock:/var/run/docker.sock option can be used in Linux environments only.\nAfter a successful pull, you should get a similar message as below:\nlatest: Pulling from portainer/portainer\rd1e017099d17: Pull complete a7dca5b5a9e8: Pull complete Digest: sha256:4ae7f14330b56ffc8728e63d355bc4bc7381417fa45ba0597e5dd32682901080\rStatus: Downloaded newer image for portainer/portainer:latest\r2fd5f4a0883a9d358ad424fd963699445be8839f3e6a2cf73d55778bcc268523\rAt this point, all you need to do is access Portainer portal to manage Docker. Open your web browser and browse to the server’s hostname or IP address followed by port #9000\nhttp://localhost:9000 or http://your_EC2_instance_public_ip_adress:9000\nYou should get Portainer login page to create an admin password.\nSubmit a new password.\nNow, you see some options to choose the environment you want to manage. Since we installed Docker on the same instance, select to connect and manage Docker locally.\nYou’ll be directed to Portainer dashboard where you can start managing Docker. If you see a notification for upgrade, click on it and proceed with the upgrade process.\nConclusion: By the end of this tutorial, we have successfully installed Portainer Docker management UI on our Ubuntu 20.01 RPI 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/psswd/",
	"title": "How To Reset My Portainer Password",
	"tags": [],
	"description": "",
	"content": "Resetting Admin password in Portainer running as container Portainer does not have a password reset feature. We will have to you use a password container helper.\n1- Firstly, we will stop the Portainer container.\ndocker container stop portainer\r2- We will run the helper using the same bind-mount/volume for the data volume\ndocker run --rm -v portainer_data:/data portainer/helper-reset-password\rThe response should be similar to the below.\n2020/06/04 00:13:58 Password succesfully updated for user: admin\r2020/06/04 00:13:58 Use the following password to login: \u0026amp;_4#\\3^5V8vLTd)E\u0026quot;NWiJBs26G*9HPl1\r3- Finally, we will restart the Portainer container.\ndocker container start portainer\rNow, we have our username and password to log in into Portainer portal. "
},
{
	"uri": "https://omar2cloud.github.io/rasp/docker-on-ec2/",
	"title": "Basic Docker Commands",
	"tags": [],
	"description": "",
	"content": "Basic Commands of Docker The heart of the Docker echo system consists of two parameters: the images and the containers.\nTo list out the docker processors:\nps -ef | grep [d]ocker\rTo list out all images on the local machine:\ndocker images\rTo check for any running containers/processors:\ndocker ps -a\rDocker run reference - Foreground vs. Detached Foreground In foreground mode, docker run can start the process in the container and attach the console to the process’s standard input, output, and standard error.\nTo create a new container from the docker image and remove it once its executed. If the image busybox does not exist locally, it will be pulled from the Docker Hub, create a container and execute a command \u0026ldquo;Hello Docker\u0026rdquo;:\ndocker run --rm busybox:latest /bin/echo \u0026quot;Hello Docker\u0026quot;\rNote: busybox is a tiny utility image used for testing\nLet\u0026rsquo;s check the docker images:\ndocker images\rNow, let\u0026rsquo;s check whether there is a container running.\ndocker ps -a\rYou might notice the docker container busybox was not running because it was removed as per our previous command. Also, note that the image itself won\u0026rsquo;t be removed, just the container. However, if we run the same command without the \u0026ndash;rm, the container will be persistent.\nTo remove a container, you could use the container\u0026rsquo;s name:\ndocker rm modest_heisenberg\rOr the container\u0026rsquo;s ID:\ndocker rm 61d53262896d If you decided to use the container\u0026rsquo;s ID, you don\u0026rsquo;t have to type the entire ID; only the unique first few numbers should be sufficient. However, using the name is more convenient\nThe preferred storage driver, for all currently supported Linux distributions, and requires no extra configuration is overlays. This is where Docker saves the images.\nsudo ls -al /var/lib/docker/overlay2\rCreating multiple containers from the same image scenario sudo run -it --rm busybox:latest\rNote that the (-it) is for interactive terminal\nNow, we have logged in into the container. If we run a ps -ef command, we shall see the processors. We could also run the command ls -al to see all directories in this container.\nWe can also interact with the container and create a directory using mkdir command and create a text file named test.txt. Then, we will check with the text file is created successfully and exit the container.\nmkdir test\rcd test/\recho \u0026quot;This is just a sample text file for testing\u0026quot; \u0026gt; test.txt\rls -al\rNow, if you go back and run docker ps -a, you won\u0026rsquo;t find busybox container running. Notice that containers are ephemeral; however, there are ways to make the file system durable such as using volume mapping.\nDetached To start a container in detached mode, we will use -d=true or just -d option. By design, containers started in detached mode exit when the root process used to run the container exits, unless you also specify the \u0026ndash;rm option. If you use -d with \u0026ndash;rm, the container is removed when it exits or when the daemon exits, whichever happens first.\nLet\u0026rsquo;s run a Centos image in detached mode:\ndocker run -d centos tail -f /dev/null\rLet\u0026rsquo;s check if the image was downloaded successfully:\ndocker images\rdocker ps -a\rLet\u0026rsquo;s check on the processor:\nps -ef | grep [d]ocker\rWe can stop the processor using the process ID:\nNote 1139 is the process ID\nExample:\nsudo kill -9 [process ID]\rActual:\nsudo kill -9 1139\rThe status of the container has changed to Existed. Now, let\u0026rsquo;s run another container from the same Docker Centos image. We can run multiple containers from the same image independently.\nTo get into the container:\nExample:\ndocker exec -it [Container Name] bash\rActual:\ndocker exec -it heuristic_wilbur bash\rWe will attempt to stop our Centos container and then remove the two Centos containers.\nTo stop a container:\ndocker stop heuristic_wilbur To remove containers:\ndocker rm heuristic_wilbur funny_galileo\rTo remove the images:\ndocker images\rExample:\ndocker rmi [Image ID]\rActual:\ndocker rmi a0477e85b8ae\r"
},
{
	"uri": "https://omar2cloud.github.io/rasp/dynamodb/",
	"title": "AWS DynamoDB on Raspberry Pi 4 Running Ubuntu 21.04",
	"tags": [],
	"description": "",
	"content": "What Is Amazon DynamoDB? Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. With DynamoDB, we can create database tables that can store and retrieve any amount of data and serve any level of request traffic. In addition to the Amazon DynamoDB web service, AWS provides a downloadable version of DynamoDB that we can run on our computers and is perfect for development and testing of our code.\nThis tutorial will walk you through how to deploy DynamoDB locally on a Raspberry Pi 4 running Ubuntu 21.04 using Docker-Compose version 3. We will also deploy DynamoDB-Admin, which is a GUI (Graphical User Interface) for DynamoDB. It\u0026rsquo;s a great tool to interact with DynamoDB visually and is built by Aaron Shafovaloff. For more information about Dynamodb-Admin, here is the link to the GitHub repo. And, for more information about Amazon DynamoDB-local Docker image, here is the link to Amazon Documentation.\nIf you don\u0026rsquo;t have Docker, Docker-Compose and Portainer installed on your RPI-4, I\u0026rsquo;d suggest you install them first. Please, refer to this Tutorial.\n\rInstalling DynamoDB and DynamoDB-Admin locally using Docker-Compose v3: There is more than one way to skin a cat (I\u0026rsquo;ve always wondered why does it have to be a poor cat). I have tried to spin a docker container for DynamoDB on my RPI-4, and then installed the DynamoDB-Admin locally on my RPI-4 (not a container). I have also tried to run a Docker-Compose for DynamoDB and DynamoDB-Admin images using Portainer. However, I had faced some hiccups because the community version of Portainer currently does not support Docker-Compose 3 (as of writing this tutorial 11/14/21).\nAlthough there are multiple ways to install the DynamoDB locally on our RPI-4, I have found the best way that fits my needs is by using Docker-Compose version 3 (by creating a yml file). The following steps will walk you through how to run DynamoDB and DynamoDB-Admin locally using Docker-Compose version 3. This method works great for me. Then, we will interact with DynamoDB using AWS Python SDK - Boto3 and create a table.\nStep 1: Creating Folders  Create a DynamoDB folder on your RPI-4 Desktop or on any other directory of your choosing. This would be our project folder. Create a folder inside the DynamoDB folder and name it dynamodb-volume. We will use this folder to host our DynamoDB database down the road. Create a file named docker-compose.yml inside the DynamoDB (not inside the dynamodb-volume). We will run this file to spin our containers later.  You have the option to create these folders and docker-compose file programmatically using the CLI (Command Line Interface) or using the GUI (Graphical User Interface) and any text editor of your choosing. The hierarchy should look like the below image.\nStep 2: Composing Docker-Compose.yml File  Add this text to the Docker-Compose.yml file and read the below discussion prior to saving the file.  version: '3.8'\rservices:\rdynamodb-local:\rcommand: \u0026quot;-jar DynamoDBLocal.jar -sharedDb -port 8000 -dbPath ./data\u0026quot;\rimage: amazon/dynamodb-local:latest\rcontainer_name: dynamodb-local\rports:\r- \u0026quot;8000:8000\u0026quot;\rvolumes:\r- \u0026quot;/home/pi/Desktop/DynamoDB/dynamodb-volume:/home/dynamodblocal/data\u0026quot;\rworking_dir: /home/dynamodblocal\rdynamodb-admin:\rimage: omarcloud20/dynamodb-admin-arm64\rcontainer_name: dynamodb-admin\rports:\r- \u0026quot;8001:8001\u0026quot;\renvironment:\rDYNAMO_ENDPOINT: \u0026quot;http://dynamodb-local:8000\u0026quot;\rAWS_REGION: \u0026quot;us-east-1\u0026quot;\rAWS_ACCESS_KEY_ID: local\rAWS_SECRET_ACCESS_KEY: local\rdepends_on:\r- dynamodb-local\r\rIf you have created the dynamodb-volume folder in a different directory, you may need to point to the directory location in the volumes line: \u0026ldquo;/home/pi/Desktop/Dynamodb/dynamodb-volume\u0026rdquo;\n\rPrior to moving to the next step, let\u0026rsquo;s have a brief discussion about the parameters of this file.\n Although we don\u0026rsquo;t have to add the command variables, We have added them just in case we need to modify them in the future such as changing the port number. To have data persistency and database accessability, we have added a persistent volume (dynamodb-volume). By doing so, we have ensured that our DynamoDB tables won\u0026rsquo;t get deleted if we delete the DynamoDB container. Moreover, we have access to the database in case we decide to move it to a different host. For my case, it\u0026rsquo;s ideal and better than creating a Docker volume for it. The current DynamoDB-Admin image is amd64 architecture which is incompatible with our RPI-4 arm64 architecture. Therefore, I had to build a RPI-4 compatible image (omarcloud20/dynamodb-admin-arm64) for the DynamoDB-Admin application. The environment variables AWS_REGION, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are required dummy variables to run the Docker-Compose but are not required to be valid AWS keys to access DynamoDB Local. By default, the DynamoDB local is listening on port 8000 and the DynamoDB-Admin is on port 8001.  Step 3: Run Docker-Compose  Let\u0026rsquo;s cd into the directory where we have saved docker-compose.yml file. In my case, it\u0026rsquo;s /home/p4/Desktop/DynamoDB. Now, let\u0026rsquo;s run the below command to execute docker-compose. Note, the flag -d is used to keep docker-compose running in the background.  docker-compose up -d\rStep 4: Access DynamoDB-Admin UI  Let\u0026rsquo;s open a browser on the RPI-4 and type in http://localhost:8001, we should be able to see the DynamoDB-Admin UI. We could also access the DynamoDB-Admin UI from any other device on the same network by typing in the browser the IP address of the RPI-4 and port 8001, http://rpi4-ip-address:8001. For example, http://192.168.1.131:8000  We could also verify the status of the DynamoDB and DynamoDB-Admin from Portainer as shown below.  Step 5: Install and Configure AWS CLI and Boto3  Let\u0026rsquo;s install AWS CLI on our RPI-4:  sudo apt install awscli -y\r\rIf you would like to install AWS CLI version 2 instead of version 1, here is the link to Amazon installation document.\n\rNow, we need to to configure AWS CLI by running the below command:  aws configure\rAs part of the configuration process, we will be prompted for the following:\n AWS Access Key ID: type local AWS Secret Access Key: type local Default region name: type us-east-1 Default output format: \u0026ldquo;leave empty and hit enter\u0026rdquo;  The AWS SDK for Python is called Boto3. We will use it to write a Python code to interact with our locally hosted DynamoDB. For more information about Boto3. Now, let\u0026rsquo;s run the following command to install boto3 on our RPI-4.  pip install boto3\ror\npip3 install boto3\r\rIf you encountered any issues, run this command to find out whether or not you have pip installed: pip --version or pip3 --version. If you don\u0026rsquo;t have pip installed, please refer to pip installation instructions.\n\rStep 6: Getting Started Developing with Python and DynamoDB  Let\u0026rsquo;s write a simple Python code to create a table called Movies. Once again, you have the option to create the Python file using the CLI or the GUI. Let\u0026rsquo;s use the CLI and cd into our DynamoDB folder. Then, let\u0026rsquo;s use nano text editor to create a Python file name MoviesCreateTable.py.  nano MoviesCreateTable.py\rWe will copy the below code and past it onto nano and save the file by:   Ctrl + x (control and x) type in y Finally hit enter  def create_movie_table(dynamodb=None):\rif not dynamodb:\rdynamodb = boto3.resource('dynamodb', endpoint_url=\u0026quot;http://localhost:8000\u0026quot;)\rtable = dynamodb.create_table(\rTableName='Movies',\rKeySchema=[\r{\r'AttributeName': 'year',\r'KeyType': 'HASH' # Partition key\r},\r{\r'AttributeName': 'title',\r'KeyType': 'RANGE' # Sort key\r}\r],\rAttributeDefinitions=[\r{\r'AttributeName': 'year',\r'AttributeType': 'N'\r},\r{\r'AttributeName': 'title',\r'AttributeType': 'S'\r},\r],\rProvisionedThroughput={\r'ReadCapacityUnits': 10,\r'WriteCapacityUnits': 10\r}\r)\rreturn table\rif __name__ == '__main__':\rmovie_table = create_movie_table()\rprint(\u0026quot;Table status:\u0026quot;, movie_table.table_status)\rNow, we are ready to run the Python code using the following the command:  python3 MoviesCreateTable.py\r If we refresh the browser, you shall see our newly created table on the DynamoDB-Admin UI as shown below. Hooryah.   We can also create a DynamoDB table from the DynamoDB-Admin UI. Let\u0026rsquo;s create one.\n  Let\u0026rsquo;s verify that all tables are stored in our dynamodb-volume. By running the following AWS command, we will get a list of all of our local DynamoDB tables:  aws dynamodb list-tables --endpoint-url http://localhost:8000 --region us-east-1\rNote that the Movies table was created by running the Python code using boto3 and the Omar_Table was created on the DynamoDB-Admin UI.\nFor more about developing with Python and DynamoDB such as loading data, query and scan tables, please refer to Amazon documentation.\n\rConclusion: By the end of this tutorial, we have successfully configured Docker containers for AWS DynamoDB-local and DynamoDB-Admin application using Docker-Compose version 3 on our RPI 4 running Ubuntu 21.04 OS. We have also created DynamoDB tables using AWS Python SDK (boto3) and DynamoDB-Admin UI. This concludes our tutorial and remember learning never stops, cheers.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/prj2/",
	"title": "Invoice Processing Application",
	"tags": [],
	"description": "",
	"content": "Invoice Processing Application The Invoice Processing Application is to parse the content of the uploaded text format invoices to S3 bucket using a Python custom code running on a Ubuntu EC2 to convert them into CSV records. Once a record is processed, it will be saved in DynamoDB for retention and the converted CSV record is saved in S3 destination bucket. AWS Athena is to query the CSV records to aggregate expenses grouped by date.\nPlease note that some parts of the images in this tutorial will be pixilated or hidden to hide all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rArchitecture Implementation - Tutorial Scenario:  The customer uploads the invoice data to S3 bucket in a text format as per their guidelines and policies. This bucket will have a policy to auto delete any content that is more than 1 day old (24 hours). An event will trigger in the bucket that will place a message in SNS topic A custom program running in EC2 will subscribe to the SNS topic and get the message placed by S3 event The program will use S3 API to read from the bucket, parse the content of the file and create a CSV record along with saving the original record in DynamoDB The program will use S3 API to write CSV record to destination S3 bucket as new S3 object. Athena is used to query the CSV file (query to show aggregated expenses grouped by date).  Step 1: S3 Buckets  Navigate to S3 using the Services button at the top of the screen. Select \u0026ldquo;Create Bucket\u0026rdquo; Enter a source bucket name and use the default options for the rest of the fields. Repeat 1 to 3 steps to create a target bucket  Step 2: SNS Subscription Lets' create a lambda function and attach the role we have previously created.\n Navigate to SNS -\u0026gt; Topics -\u0026gt; Standard Click on \u0026ldquo;Create Topic\u0026rdquo; Enter the following fields Name: S3toEC2Topic Click on Create Topic  Let\u0026rsquo;s modification of SNS Access Policy as follows:  {\r\u0026quot;Version\u0026quot;: \u0026quot;2008-10-17\u0026quot;,\r\u0026quot;Id\u0026quot;: \u0026quot;__default_policy_ID\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;__default_statement_ID\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Principal\u0026quot;: {\r\u0026quot;AWS\u0026quot;: \u0026quot;*\u0026quot;\r},\r\u0026quot;Action\u0026quot;: \u0026quot;SNS:Publish\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:sns:us-east-1:Your AWS Account Number:S3toEC2Topic\u0026quot;,\r\u0026quot;Condition\u0026quot;: {\r\u0026quot;StringEquals\u0026quot;: {\r\u0026quot;aws:SourceAccount\u0026quot;: \u0026quot;Your AWS Account Number\u0026quot;\r},\r\u0026quot;ArnLike\u0026quot;: {\r\u0026quot;aws:SourceArn\u0026quot;: \u0026quot;arn:aws:s3:*:*:\u0026quot;S3 Source Bucket Name\u0026quot;\r}\r}\r}\r]\r}\rNote: A. Replace SNS-topic-ARN as shown below.\nB. Replace bucket-name with your source bucker name as shown below.\nC. Replace bucket-owner-account-id with yours as shown below.\nStep 3: Configuring SNS Notification for S3   Navigate to S3 and select the source bucket, which we have previously created.\n  Select Properties and scroll down to Event Notifications and select it.\n  Follow the following details:\nA. Name: S3PutEvent B. Select PUT from the list C. Destination: SNS Topic D. SNS: S3toEC2Topic\n  Save changes.\n  Step 4: Spin an EC2 Instance to Run the Custom Code   Navigate to EC2 -\u0026gt; Instances\n  Create an EC2 instance with the following parameters:\nA. AMI: Ubuntu 18.04 LTS B. VPC: Default C. Security Group: Open ports 22 and 8080\n  Note: to enhance the security, you could select opening port 22 ONLY to your home public IP address. This will limit the SSH access to request coming from your home IP address.\nStep 5: IAM role for the EC2 instance   Navigate to IAM -\u0026gt; Roles -\u0026gt; Create role\n  Attach the following two policies:\nA. AmazonS3FullAccess\nB. AmazonDynamoDBFullAccess\n  Name the role, EC2S3Access\n  Let\u0026rsquo;s go back to the EC2 and attach this role to it as shown below.  Step 6: Uploading the Python Code   Download the application zip file from my GitHub repo and save it to your local device.\n  Open the code using any code editor of your choice. I\u0026rsquo;m using Visual Studio Code as a code/text editor.\n  Unzip the file and navigate to docproc-new -\u0026gt; api -\u0026gt; views.py.\n  Open views.py with your choice of code editor and ensure that:\nA. The name of the S3 target bucket must match the name of our target bucket\u0026rsquo;s name (line 19).\nB. The region should match your region. Mine is us-east-1.\n  Then, let\u0026rsquo;s save the python file.\nNow, we need to copy folder docproc-new from our local device to the home folder of our Ubuntu EC2 instance using scp command as shown below.  Note: first you may need to navigate (cd) to where you have saved the pem file.\nscp -i \u0026lt;Ubuntu pem file\u0026gt; ./docproc-new ubuntu@\u0026lt;Ubuntu public ip\u0026gt;:/home/ubuntu\rIf your transfer is successful, you shall see a message similar to the below.\nStep 7: Source Code Installation   Let\u0026rsquo;s SSH into our Ubuntu EC2 instance. If you need help SSH\u0026rsquo;ing into the instance, please refer to my previous tutorial about SSH.\n  Now, we will run the following commands on our Ubuntu instance to update the operating system, install Python, Django, Boto3 and a virtual environment:\n  sudo apt update\rsudo apt install python-pip -y\rpython -m pip install --upgrade pip setuptools\rsudo apt install virtualenv -y\rvirtualenv ~/.virtualenvs/djangodev\rsource ~/.virtualenvs/djangodev/bin/activate\rpip install django\rpip install boto3\rLet\u0026rsquo;s create a directory and copy the source code.  sudo cp -r docproc-new /opt\rsudo chown ubuntu:ubuntu -R /opt\rcd /opt/docproc-new\rNow, let\u0026rsquo;s run the python code:  python manage.py runserver 0:8080\rNote: make sure the development server is running on the CLI/Terminal\nStep 8: SNS Subscription   Let\u0026rsquo;s navigate to SNS in the AWS Console and select the topic S3toEC2Topic\n  Click on Create Subscription\n  Enter the following details:\nProtocol : HTTP\nEndpoint : http://IP-address:8080/sns, where the IP Address is the public IP of the EC2 instance.\n  In the EC2 terminal window, look for the field \u0026ldquo;SubscribeURL\u0026rdquo; and copy the entire link given without the prentices. This should be the token used to confirm the subscription.  Note: If a message is seen \u0026ldquo;ValueError: No JSON object could be decoded\u0026rdquo;, it can be safely ignored.\nPaste that URL into a browser window, http://\u0026lt;instance public IP address\u0026gt;:8080/sns*, to verify the SNS subscription (Ignore any messages received in the web browser).  When EC2 instance is stopped, its public IPv4 is released, and a new IPv4 is assigned to the instance once it starts back up. To read more about AWS Stop and start your instance.\n\rStep 9: Generation of CSV File In the project folder, we have docproc-invoice.txt which we will upload to our S3 source bucket.\n Navigate to S3 in the AWS Console. Upload the sample invoice file to the source S3 bucket using the default options.  *Note: you should be able to track the processing of the file on command line/terminal as shown below.\nVerify that a CSV file is generated in the target S3 bucket.  Step 10: Create a Table in DynamoDB  Navigate to DynamoDB using the Services Menu. Click on tables on the left side. Select the table \u0026ldquo;invoice\u0026rdquo; Click on the \u0026ldquo;Items\u0026rdquo; tab and verify that a record has been created in the table with the contents of the invoice file.  Step 11: AWS Athena Querying CSV File  Navigate to AWS Athena in the AWS Console. Paste the following command in the query editor and Run query to create a database: create database proj2db;  Run the following query to create the table based on the generated CSV file:  Note: add your S3 target bucket name instead of \u0026lt;target S3 bucket\u0026gt;.\nCREATE EXTERNAL TABLE IF NOT EXISTS proj2db.invoice (\r`customer-id` string,\r`inv-id` string,\r`date` string,\r`from` string,\r`to` string,\r`amount` float,\r`sgst` float,\r`total` float,\r`inwords` string\r)\rROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\rWITH SERDEPROPERTIES (\r'serialization.format' = ',',\r'field.delim' = ','\r) LOCATION 's3://\u0026lt;target S3 bucket\u0026gt;/'\rTBLPROPERTIES ('has_encrypted_data'='false');\rPaste the following query and run it to show aggregated expenses by date:  SELECT sum(\u0026quot;amount\u0026quot;), \u0026quot;date\u0026quot; FROM \u0026quot;proj2db\u0026quot;.\u0026quot;invoice\u0026quot; GROUP BY \u0026quot;date\u0026quot;;\rShown below is the output of the Athena query to show aggregated expenses by date. Since we have only uploaded one invoice, we don\u0026rsquo;t have lots of the invoices to query.\nConclusion: By the end of this tutorial, we have successfully created an Invoice Processing Application web. The application has utilized S3, SNS, EC2, DynamoDB and Athena.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/prj3/",
	"title": "Docker Image on ECS Fargate",
	"tags": [],
	"description": "",
	"content": "A Sample Web Application Containerized as a Docker image Deployed on AWS ECS Fargate In this tutorial, we will package and containerize a sample web application as a Docker image running on Apache Tomcat having JRE-8 as a runtime. Then, we will push this new Docker image to our public repository at Dockerhub. The web application, which is packaged into a Docker image will be deployed on AWS ECS Fargate cluster. Finally, to view and access our web application cluster, we will utilize a DNS of a load balancer.\nThe below diagram is our design architecture for this tutorial.\nIt\u0026rsquo;s optional to either utilize Ubuntu EC2 instance or your own device to package and push the Docker image to Dockerhub. I expect that you have Docker installed on your instance or device, if not, please refer to my previous tutorial install Docker.\n\rArchitecture Implementation - Tutorial Scenario:  Download the WAR file from my GitHub Web application needs to be packaged as a Docker image running on Tomcat having JRE8 - you will have to write a Dockerfile Once the image is created, run and verify image by accessing web application using ec2 instance public-ip Sign up for docker hub and create public repository. Tag the image appropriately and push to Docker Hub Repository. Using AWS ECS Fargate create a cluster, task and service(s).  Step 1: Create a Docker image  Navigate to folder /opt and create helloworld folder.  sudo chown ubuntu:ubuntu -R /opt\rcd /opt\rmkdir helloworld\rcd helloworld/\rDownload the War file from my GitHub to /opt/helloworld  wget https://github.com/OmarCloud20/aws-tutorials/raw/main/prj3/HelloWorld.war\rCreate a Docker file in the helloworld folder.  cd /opt/helloworld/\rsudo nano Dockerfile\rThen, add or type the following sentences into the Dockerfile. Note, you could add your name and email address as a maintainer.\nFROM tomcat:jre8\rMAINTAINER [your name]\rCOPY HelloWorld.war /usr/local/tomcat/webapps\rNote: to save the file in nano text editor, hold ^x, type yes and click enter.\n Verify that the file is created.  ls -al\rBuild the Docker image using the Docker build command:  docker build -t helloworld .\rNote: if you receive any permission error during the build, add sudo to the command line\nsudo docker build -t helloworld .\rRun the image created above using the command given below. We will use -p to port mapping port 8080 of the container to port 80 of the host:  docker run -d -p 80:8080 helloworld\r Let\u0026rsquo;s check whether or not our container is running:  docker ps -a\r Now, let\u0026rsquo;s verify that our application can be accessed from the device using the URL:\n\u0026lt;public IP address of instance\u0026gt;/HelloWorld\u0026gt;\n  Step 2: Push the Packaged Web Application Docker Image to Docker Hub  Create a new free account at Docker Hub. Create a new public repository in your Dockerhub account.  Login to the Dockerhub account from the CLI using the command below:  Example:\ndocker login --username=\u0026lt;DockerHub username\u0026gt;\rActual:\ndocker login --username=omarcloud20\rNote: you will be prompted to enter your password.\nWe will need to tag the image which we will upload to Docker hub using the Image ID, then we will use the tag command as shown below:  To find the Image ID:\ndocker images\rThen, we use the tag command:\nExample:\ndocker tag \u0026lt;image id\u0026gt; \u0026lt;dockerhub username\u0026gt;/\u0026lt;repository name\u0026gt;:latest\rActual:\ndocker tag c0bd35d35ced omarcloud20/project3:latest\rFinally, we will push the image to Dockerhub:  Example:\ndocker push \u0026lt;image id\u0026gt; \u0026lt;dockerhub username\u0026gt;/\u0026lt;repository name\u0026gt;:latest\rActual:\ndocker push c0bd35d35ced omarcloud20/project3:latest\r Verfiy that the image is uploaded to your public dockerhub account.  Step 3: Create an AWS ECS cluster to run the Docker image   Navigate to the ECS service in your AWS account and click Get Started.\n  Under Container and Task Definitions, select Custom-\u0026gt;Configure, and enter the following values:\nA. Container name. Mine is project3.\nB. Image location: docker.io/dockerhub username/dockerhub repository:latest\nC. Memory Limits: Soft limit - 256MB\nD. Port mappings: 8080 - tcp\nE. Click update and hen Next\n  On the defining the Service, select Application Load Balancer and click Next.  On the defining the Cluster, enter a Cluster name and click Next.  On the final review, click Create.  AWS ECS Fargate will create all necessary services and the cluster on your behalf. If you have encounter any errors, you may need to repeat the process one more time.  Step 4: Verification of Running Container on ECS Fargate   Navigate to EC2 using the Services Menu.\n  Navigate to Load balancer.\n  Select the Load Balancer that has been created by the ECS Cluster.\n  Take note of the DNS of the load balancer and visit the below URL to verify that the ECS cluster is running the container:\n\u0026lt;DNS of load balancer\u0026gt;:8080/HelloWorld\n  Conclusion: Congratulations, we have successfully packaged a sample web application into a Docker image utilizing Apache Tomcat and JRE-8 runtime. From our newly created Dockerhub public account, we pushed the new image to AWS ECS Fargate cluster.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/gitlab_runner/",
	"title": "GitLab Runner on RPI-4 (Build, Push Docker images to Docker Hub using GitLab Runner on GitLab)",
	"tags": [],
	"description": "",
	"content": "What is GitLab Runner GitLab Runner is an agent that runs GitLab CI/CD (Continues Integration/Continuous Deployment) jobs in a pipeline. It\u0026rsquo;s heavily utilized in the world of DevOps to provision and configure infrastructure. The GitLab Runner can be installed as a binary on Linux, MacOS or Windows. It can also be installed as a container.\nOn this tutorial, I will walk through installing and configuring GitLab Runner as a container using a Docker image on a RPI-4..yaay. I will make it very swift to get you started and won\u0026rsquo;t feel a thing. I will not bore you with details, but if there are useful links for further study, I will definitely throw it in. The goal is to get you started with GitLab Runners and the rest is on you.\nAs a bonus, we will run our first job of building docker images and push them to Docker hub. Enough reading, let\u0026rsquo;s get our hands dirty, I mean our keyboards 😉\nTo lean more about GitLab Runners, refer to GitLab official documentation.\nSenstive information will be pixilated or erased. This should not alter the quality of the tutorial.\n\rRun a GitLab Runner Container on a RPI-4 If you don\u0026rsquo;t have Docker installed on your RPI-4, you may refer to my Docker on Ubuntu 20.04 Raspberry Pi 4 tutorial.\n Let\u0026rsquo;s create a persistent Docker volume for the runner. On your RPI-4 terminal, run the following command:  docker volume create gitlab-runner-volume\rNote: you can change the volume name gitlab-runner-volume to any name of your chosen, but you should be consistent as we will use the volume name to bind the container to the RPI-4 local host.\nRun the below commands to start GitLab Runner container:  docker run -d --name gitlab-runner --restart always --env TZ=US \\\r-v gitlab-runner-volume:/etc/gitlab-runner \\\r-v /var/run/docker.sock:/var/run/docker.sock \\\rgitlab/gitlab-runner:alpine\rThe only parameters you have the options to change are:\n the name flag, gitlab-runner the volume name, gitlab-runner-volume the gitlab runner image, gitlab/gitlab-runner:alpine the env flag, TZ=US  For more information about the installation process, here is the link GitLab\u0026rsquo;s official documentation.\nCapture the GitLab Runner token:  We do need to head to our GitLab account and grab the runner\u0026rsquo;s token. If you don\u0026rsquo;t have a GitLab account, you can create one for free. Here is the link.\nNow, we need to create a repository to host our project. From GitLab, let\u0026rsquo;s click on create New project. On the Create new project, select create blank project. Then, give the project a name and click Create project. In my case, I named my repo, GitLab-Runner-RPI-4.\nFrom the repository, let\u0026rsquo;s click on settings, CI/CD and then Expand on the Runners section as shown below.\n Capture the token and save it on a note pad. We will need the token for the next step. Disable Enable shared runners for this project.  Register the GitLab Runner:  Replace the token place holder after registration-token with the one from our note pad, and then run the following commands on your RPI-4 terminal:\ndocker run --rm -it -v gitlab-runner-volume:/etc/gitlab-runner gitlab/gitlab-runner:alpine register -n \\\r--url https://gitlab.com/ \\\r--registration-token GR1348941EDhyNWqfxPttukrGVKJd \\\r--executor docker \\\r--description \u0026quot;My Docker Runner\u0026quot; \\\r--docker-image \u0026quot;docker:20.10.12-dind-alpine3.15\u0026quot; \\\r--docker-privileged \\\r--docker-volumes \u0026quot;/certs/client\u0026quot;\rIf everything went smooth, you should not exhibit no errors as shown below.\nTo confirm that the GitLab Runner container is running, run the below Docker command:\ndocker ps -a\rAlright, this is a great indication that we have successfully configured GitLab Runner on RPI-4. Now, we are ready to make some actions 💥\nBuild and Push Docker images to Docker Hub using GitLab Runner on GitLab  First of all, we need get a token from our Docker hub account to avoid using account password. The token is to allow GitLab Runner to authenticate and push Docker images to our Docker hub repository. If you don\u0026rsquo;t know what Docker hub is, it is the world\u0026rsquo;s largest library for container images.   On Docker hub account settings, click on Security, New Access Token and generate a Read, Write, Delete token. Docker hub free account allows for one active token. Capture the token and save to a note pad.  From GitLab repo, which have we have created previously:   Click on Clone and copy the Clone with HTTPS link. On your RPI-4 terminal and in a folder of your chosen, run the below command. This is where the repo will be saved locally:  git clone `your-Clone-with-HTTPS-link`\rNote: the git clone command will prompt you to enter your GitLab credentials for authentication.\ncd into the repo and either use a text editor or the terminal to edit the .gitlab-ci.yml file. The file should only contain the following code. Be very careful with the indentation. Lastly, save the file:  image: docker:19.03.12\rvariables:\rIMAGE_NAME: \u0026quot;test:1.0.0\u0026quot;\rDOCKER_TLS_CERTDIR: \u0026quot;/certs\u0026quot;\rservices:\r- docker:19.03.12-dind\rbefore_script:\r- docker info\rbuild image:\rstage: build script: - docker build -t $REGISTRY_USER/$IMAGE_NAME .\r- docker login -u $REGISTRY_USER -p $DOCKER_HUB_TOKEN\r- docker push $REGISTRY_USER/$IMAGE_NAME\rWe will create a simple Dockerfile. The runner will use this Dockerfile to build a Docker image and push it to Docker hub. For right now, we will keep the Dockerfile VERY simple. Once the image is built from the Dockerfile, it will echo Hello World. With your preferred text editor or terminal, create and name a file Dockerfile without any extension. Then save it in the same local repo.  FROM alpine\rRUN echo \u0026quot;Hello World\u0026quot;\rNote: In the local repo, we should have two files, Dockerfile and .gitlab-ci.yml. A README file might be there as well.\nWe will need to head back to GitLab repo to create two variables. On Settings, click on CI/CD, then expand Variables. We will click on Add variable to create two variables:    Key: REGISTRY_USER Value: your Docker hub username NOT the email\n  Key: DOCKER_HUB_TOKEN Value: the token which we generated from Docker hub\n  Note: the flags should be unchecked for simplicity for the two variables.\nNow, we push the local repo to GitLab repo (remote repo) to start the GitLab Runner pipeline process. Yes, once the remote repo is updated, the runner will be triggered. Let\u0026rsquo;s get back to our terminal on the RPI-4 and from within the local repo, run the following git commands:  git add -A\rgit commit -am \u0026quot;first test\u0026quot;\rgit push\rIf you head back to our GitLab repo and click on CI/CD under Pipelines, you would notice that the pipeline is running.\nMoreover, if you click on the running status and you would be directed to the current stage, which is Build.\nHere, click on build image, you shall see more details about the current build status. The goal is to see Job succeeded. Once you see Job succeeded and passed status in green, you can start doing your victory dance 👯‍♀️ 👯‍♀️\nIf we head back to Docker hub account, we should see that our Docker image test:1.0.0 has been successfully pushed to the repo.\nDocker hub allows Personal (free account), one private repository and unlimited number of public repositories. Therefore, if you get denied message to access the resource on the pushing images to Docker hub step, ensure your repository is public.\n\rConclusion: By the end of this tutorial, we have successfully configured a GitLab Runner on a RPI-4, created a GitLab repo and registered GitLab Runner to it. Finally, we created a Hello World Docker image from a Dockerfile and had the runner building the Docker image and pushing it to our Docker hub account. Now, off you go and the sky is the limit.\n"
},
{
	"uri": "https://omar2cloud.github.io/cont/archetypes/",
	"title": "Archetypes",
	"tags": [],
	"description": "",
	"content": "Using the command: hugo new [relative new content path], you can start a content file with the date and title automatically set. While this is a welcome feature, active writers need more: archetypes.\nIt is pre-configured skeleton pages with default front matter. Please refer to the documentation for types of page to understand the differences.\nChapter To create a Chapter page, run the following commands\nhugo new --kind chapter \u0026lt;name\u0026gt;/_index.md\rIt will create a page with predefined Front-Matter:\n+++ title = \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; date = {{ .Date }} weight = 5 chapter = true pre = \u0026#34;\u0026lt;b\u0026gt;X. \u0026lt;/b\u0026gt;\u0026#34; +++ ### Chapter X  # Some Chapter title Lorem Ipsum. Default To create a default page, run either one of the following commands\n# Either\rhugo new \u0026lt;chapter\u0026gt;/\u0026lt;name\u0026gt;/_index.md\r# Or\rhugo new \u0026lt;chapter\u0026gt;/\u0026lt;name\u0026gt;.md\rIt will create a page with predefined Front-Matter:\n+++ title = \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; date = {{ .Date }} weight = 5 +++ Lorem Ipsum. "
},
{
	"uri": "https://omar2cloud.github.io/aws/rekognition/",
	"title": "Image Analysis with AWS Rekognition",
	"tags": [],
	"description": "",
	"content": "AWS Rekognition Amazon Rekognition is part of AWS cognitive services, which requires no machine learning expertise to use. It\u0026rsquo;s a simple way to analysis images and videos for any application using proven record of high scalability and deep learning technology. Rekognition technology is utilized to identify and detect objects, shapes, people, texts, and activities in media contents. For more about Amazon Rekognition.\nIn this tutorial, we will explore the important aspect of AWS Rekognition and practically detect people and text on sample picture and video utilizing boto3 - AWS Python library..\nTutorial Scenario:   Create an IAM user to access AWS Rekognition service.\n  Install and configure AWS CLI-v2 on Raspberry Pi 4.\nA. Install Python, pip and zip utility.\nB. Install boto3 - AWS Python library.\n  Run aws-rekognition sample application to identify objects, people and texts.\n  Please note that some parts of the images in this tutorial will be pixilated or hidden to hide all sensitive account information..\n\rStep 1: Create an IAM User:  Navigate to Identity and Access Management (IAM) and click Add user. Then, add a user name and select Access type as Programmatic access as shown below.  On the Set permissions screen, select Attach existing policies directly. On the Filter policies, type in the term rekognition and check AmazonRekognitionFullAccess.   As a best practice and a good habit to acquire, add a tag and then click on Next.   Download and save the csv file in a secure location. This file contains the new user credentials needed to sign programmatic requests that you make to AWS.\n  Step 2: Install and configure AWS CLI-v2 on Raspberry Pi 4: Prior to installing AWS CLI-v2, we have to make sure Python and some utilities are installed. As a best practice, let\u0026rsquo;s update our Ubuntu operating system.\nsudo apt update\rTo find more information about our hardware architecture:\nuname -a\rLet\u0026rsquo;s check out whether or not we have Python and pip installed and their versions if we do:\npython3 ---version\rpip --version\rIf we don\u0026rsquo;t have python3 installed, we need to install it. In my case, pip was not installed on my device, so I installed it. pip is the package installer for Python. For more information pip.\nsudo apt install python3-pip\rLet\u0026rsquo;s install the zip utility to unzip files, if you don\u0026rsquo;t have it installed.\nsudo apt install zip -y\rLet\u0026rsquo;s install Python (Boto3) library.\npip install boto3\rInstall the latest AWS Command Line Interface - CLI verizon 2 for Linux ARM architecture. Raspberry Pi is an arm64 architecture.\ncurl \u0026quot;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026quot; -o \u0026quot;awscliv2.zip\u0026quot;\runzip awscliv2.zip\rsudo ./aws/install\rNow, we are ready to configure aws on our RPI-4, which is the fastest way to set up our AWS CLI installation. When you enter this aws configure command, the AWS CLI prompts you for four pieces of information:\n Access key ID Secret access key AWS Region Output format  The access keys consist of an access key ID and secret access key, which are used to sign programmatic requests that you make to AWS.If you don\u0026rsquo;t have access keys, you can create them from the AWS Management Console. As a best practice, do not use the AWS account root user access keys for any task where it\u0026rsquo;s not required.\nLet\u0026rsquo;s start with aws configuration by using the following command in our CLI.\naws configure\rNow, let\u0026rsquo;s setup our file system and add a folder for our sample app. The chown command allows us to change the user and/or group ownership of a given file or directory.\nNote: replace ubuntu by the name of you RPI 4. For example, raspberry:raspberry\nsudo chown ubuntu:ubuntu -R /opt\rWe will cd into opt directory. cd is the command used to move between directories/folders.\ncd /opt\rmkdir is the command to create directories.\nmkdir aws-rekognition\rLet\u0026rsquo;s cd into our sample app folder and download it.\ncd aws-rekognition\rNow, we will download the aws-rekognition sample application in the aws-rekognition folder.\nwget https://github.com/OmarCloud20/aws-tutorials/raw/main/Rekognition/aws-rekognition.zip\runzip aws-rekognition.zip\rIf you don\u0026rsquo;t have the unzip utility, run the following command to install it.\nsudo apt install zip -y\rAs we have downloaded our sample application zip file and unzipped it, we have also confirmed that all files are downloaded successfully in the proper directory, which is aws-rekognition.\nStep 3: Run aws-rekognition sample application: Finally, we completed all necessary configuration and it\u0026rsquo;s time to run our application. We need to cd into our directory as shown below.\ncd /opt/aws-rekognition/\rls Once we run the python code, rekognition.py, we will be prompted to enter the picture name including its file extension. For the below example, the picture is Truck.jpg.\npython3 rekognition.py\rFor rhe second example, the picture is jobs.jpg.\nConclusion: By the end of this tutorial, we have successfully utilized AWS Rekognition service to analyze images. Rekognition is one of AWS cognitive services, and we have made several API calls to it via boto3 - AWS Python library from within a Raspberry Pi 4.\nWithout any machine learning expertise, we have identified and detected objects, shapes, people, texts, and activities in several images successfully.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/codepipeline/",
	"title": "Create a Simple AWS CodePipeline from S3",
	"tags": [],
	"description": "",
	"content": "AWS CodePipeline AWS CodePipeline is a continuous delivery service offered by AWS to model, visualize, and automate the release of applications. This service allows for rapid modeling and configuring of different steps of an application release process. It automates the necessary process of change of application deployment. For information about AWS CodePipeline.\nLearning Outcomes and Tutorial Scenario: In this tutorial, we will create a two-stage pipeline that uses a versioned S3 bucket and CodeDeploy to release a sample application.\n Creating an S3 bucket for the sample application. Spinning Two Amazon Linux EC2 instances and install the CodeDeploy agent. Creating an application in CodeDeploy. Creating our first pipeline in CodePipeline. Upload a second version of our sample application to our S3 bucket and wait for CodePipeline to automate the release.  When AWS S3 is the source provider for a pipeline, zip your source file or files into a single .zip and upload it to the source bucket. Using an unzipped file, may fail downstream.\n\rStep 1: Creating an S3 bucket for a sample application  Navigate to S3 and create a source bucket and enable versioning. Make sure you create a bucket in the region where you intend to create your pipeline.  Next, download a sample application from AWS site and save it into a folder or directory on your local device. Do not unzip the file. Navigate to the newly created S3 source bucker and upload the sample application zip file.  Step 2: Spinning Two Amazon Linux-2 EC2 instance and install the CodeDeploy agent  Navigate to IAM to create a role. Choose Create role, Select type of trusted entity as EC2 and select AmazonEC2RoleforAWSCodeDeploy as a policy.  Enter a name for the role and click on Create role.  3. Next, navigate to create an EC2 and choose Amazon Linux 2 AMI.\nEnter in Number of instances, 2. And, Auto-assign Public IP and choose Enable. In the IAM role, choose the IAM role which we have created in steps 1 and 2. Expand Advanced Details, and in User data, enter the following:  *Note: you will need to replace the region to match your region on the wget command. Mine is us-east-1\n#!/bin/bash\ryum -y update\ryum install -y ruby\ryum install -y aws-cli\rcd /home/ec2-user\rwget https://aws-codedeploy-us-east-1.s3.us-east-1.amazonaws.com/latest/install\rchmod +x ./install\r./install auto\rOn the Add Tags, add a key name to a Value of EC2_Pipeline or any other name of your choice as shown below. This is very important for next steps. Also, on Configure Security Group page, allow ports 22 and 80 communications so we can SSH (if needed) and access the public instance endpoint.  Note: to enhance the security of the SSH access, you may need to limit the source to your own local IP address by selecting My IP on the Source.\nChoose Review and Launch. On the Review Instance Launch page, choose Launch. When prompted for a key pair, name and download the key pair.  It may take a few minutes for the instance to be ready for you to connect to it. Check that your instance has passed its status checks. You can view this information in the Status Checks column.  Step 3: Creating an application in CodeDeploy  Navigate to CodeDeploy menu, choose Applications and Create application. Enter a name for the application. In Compute Platform, choose EC2/On-premises and choose Create application.  To create a deployment group in CodeDeploy  Navigate to IAM to create a role. Choose Create role, Select type of trusted entity as CodeDeploy. The policy suggested would be AWSCodeDeployRole.  Enter a name for the role and click on Create role.   On the page that displays your application, choose Create deployment group.  Enter a name for the Deployment group name. In Service Role, choose a service role that trusts AWS CodeDeploy with. Under Deployment type, choose In-place. Under Environment configuration, choose Amazon EC2 Instances. Choose Name in the Key field, and in the Value field. Under Deployment configuration, choose CodeDeployDefault.OneAtaTime. Under Load Balancer, clear Enable load balancing. You do not need to set up a load balancer or choose a target group for this tutorial. Leave the defaults for the Advanced section and Create deployment group.  Step 4: Creating our first pipeline in CodePipeline To create a CodePipeline automated release process:\n  Navigate to Pipeline menu and Create pipeline.\n  Enter a name for the pipeline.\n  In Service role, choose new service role to allow CodePipeline to create a new service role in IAM. In Role name, the role and policy name both default to this format: AWSCodePipelineServiceRole-region-pipeline_name.\n  Leave the defaults for the Advanced section and then choose Next.\n  In Step 2: Add source stage, in Source provider, choose Amazon S3. In Bucket, enter the name of the S3 bucket we have previously created. In S3 object key, enter the object key with or without a file path, and remember to include the file extension. For example, for SampleApp_Windows.zip, enter the sample file name as shown in this example:  SampleApp_Linux.zip\rChoose Next step and leave defaults for Amazon CloudWatch Events as recommended. Choose Next.  In Step 3: Add build stage, choose Skip build stage, and then accept the warning message by choosing Skip again. Choose Next. In Step 4: Add deploy stage, in Deploy provider, choose AWS CodeDeploy. The Region field defaults to the same AWS Region as your pipeline. In Application name, enter the name your chose previously for the application, or choose the Refresh button, and then choose the application name from the list. In Deployment group, enter the name your chose previously for the deployment group, or choose it from the list, and then choose Next.  In Step 5: Review, review the information, and then choose Create pipeline. The pipeline starts to run. You can view progress and success and failure messages as the CodePipeline sample deploys a webpage to each of the Ubuntu EC2 instances in the CodeDeploy deployment.  To verify your pipeline ran successfully On the Description tab, in Public DNS, copy the address, and then paste it into the address bar of your web browser. View the index page for the sample application you uploaded to your S3 bucket.\nNotice, there are two different public IP addresses for the sample application running on two different EC2 instances.\nStep 5: Upload Version 2 of Sample Application to S3 bucket  We will unzip SampleApp_linux.zip to modify the html file with your choice of code editor. I\u0026rsquo;m using V.S. Code to change the background color as well as adding version 2 to the text as shown below.  Let\u0026rsquo;s re-zip the SampleApp_Linux and upload the zip file to our S3 bucket. Navigate to CodePipeline to witness the deployment.  4.\tOnce the deployment is successful, let\u0026rsquo;s refresh the browser to inspect the version 2 of our sample application. Conclusion: By the end of this tutorial, we have successfully created a simple pipeline in CodePipeline. The pipeline has two stages:\n  A source stage named Source, which detects changes in the versioned sample application stored in the S3 bucket and pulls those changes into the pipeline.\n  A Deploy stage that deploys those changes to two EC2 instances with CodeDeploy.\n  Congratulations!\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/terraform/",
	"title": "Create a Terraform Template to Spin EC2 Instance from a RaspberryPi 4",
	"tags": [],
	"description": "",
	"content": "What is Terraform? Terraform is an Infrastructure as a Code - IaaC- service. It is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. For more information about Terraform.\nDuring this tutorial, we will create a sample Terraform template to spin an AWS EC2 instance from the Command Line Interface (CLI) on a Raspberry Pi 4.\nLearning Outcomes:  Download Terraform on a Raspberry Pi 4 arm64 architecture. Create AWS Identity and Access Management (IAM) user to run the Terraform sample template from the RPI-4. Create a Terraform sample template to spin an AWS EC2. Learn Terraform commands.  What you do you need to complete the tutorial:  AWS Account Credentials. A Raspberry Pi 4. Shell script environment (any text editor of your choice).  Step 1: Download Terraform on Raspberry Pi 4  First of all, if you\u0026rsquo;d like to check your device\u0026rsquo;s architecture, run the following command:  uname -a\rNavigate to Terraform download page and download the proper package for your operating system and architecture. In my case, I chose to download the Linux package arm64 to my RPI-4.  Now, let navigate to the folder where the package is downloaded and unzip it using the unzip utility. If you don\u0026rsquo;t have the unzip utility, use the following command to install it.\nsudo apt install zip -y\rNow, let\u0026rsquo;s unzip the package as shown below:\nWe will run the following commands to move the Terraform zip file to our PATH. Firstly, we need to find the PATH.  echo $PATH\rThen, we will move the Terraform zip file to our main PATH, which is /usr/local/bin/. This will allow us to run Terraform commands from any location without any restrictions.\nsudo mv terraform /usr/local/bin/\rThen, we will cd into the the bin folder as shown below:\nNow, let\u0026rsquo;s run the command terraform. If the results as shown below, we have installed Terraform successfully.\nterraform\rThe results should be as shown below:\nStep 2: Create AWS Identity and Access Management (IAM) user Part 1:  Navigate to AWS console. Go to IAM and click on Users on the left hand side menu. Click on Add user and then add a user name and select access type as Programmatic access as shown below. Then, click on Next.  On Set permissions, click on Attach existing policies directly. search for XX policy and check it. Then, click Next.  On Add tags, it\u0026rsquo;s a best practice to use tags; therefore, adda key and value as shown below. Then, click Next.  Finally, review the add user details and permissions summary and then click on Create user.  Now, we have obtained the Access Key ID and Secret Access Key, let\u0026rsquo;s download the csv file and save it in a secure location for future reference.  Part 2: Install the latest AWS Command Line Interface - CLI version 2 for Linux ARM architecture since my Raspberry Pi 4 is an arm64 architecture. However, you shall Choose the appropriate package for your operating system and device architecture.\ncurl \u0026quot;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026quot; -o \u0026quot;awscliv2.zip\u0026quot;\runzip awscliv2.zip\rsudo ./aws/install\rWe will configure the aws on our device (RPI-4 in my case), which is the fastest way to set up our AWS CLI installation. When you enter this aws configure command, the AWS CLI prompts you for four pieces of information:\n Access key ID Secret access key AWS Region Output format  The access keys consist of an access key ID and a secret access key, which are used to sign programmatic requests that you make to AWS. If you don\u0026rsquo;t have access keys, you can create them from the AWS Management Console. As a best practice, do not use the AWS account root user access keys for any task where it\u0026rsquo;s not required.\n\rLet\u0026rsquo;s start with aws configuration by using the following command in our CLI.\nExample:\naws configure --profile [your profile name]\rActual:\naws configure --profile Terraform\rTo Check if the CLI and profile have been configured properly, run the following command which will lists all of your AWS EC2 instance resources using credentials and settings, which we have defined in the [your profile name] profile\naws ec2 describe-instances --profile [your profile name]\rStep 3: Create a Terraform Sample Template to Spin an AWS EC2 Terraform Commands    Commands Usage     terraform init Download any plugins required to run templates   terraform plan Will give you a list of resources that will be created/deleted   terraform apply WIll create/delete resources   terraform destroy Will delete all the resources created by Terraform   terraform fmt Will format the file with proper indentation    Note: you could always reference Terraform Providers for more commands per provider.\nCreating a Terraform Sample Template  Utilize any Shell script environment or text/code editor of your choice to start the Terraform language code for Terraform sample template as shown below and save the file with a .tf extension:  provider \u0026quot;aws\u0026quot; {\rprofile = \u0026quot;Terraform\u0026quot;\rregion = \u0026quot;us-east-1\u0026quot;\r}\rresource \u0026quot;aws_instance\u0026quot; \u0026quot;demo_instance\u0026quot; {\rami = \u0026quot;ami-042e8287309f5df03\u0026quot;\rinstance_type = \u0026quot;t2.micro\u0026quot;\rtags = {\rName = \u0026quot;Terraform_Demo\u0026quot;\r}\r}\rNote: every AMI has a unique ID and the Ubuntu 20.04 LTS we have chosen has the following AMI ID: ami-042e8287309f5df03. If you would like to spin a different AMI, replace the AMI ID. Also, don\u0026rsquo;t forget to replace the profile name with the name of AWS profile which we have created earlier (in my case is Terraform).\nNow, we are ready to run Terraform command. Let\u0026rsquo;s start with running terraform init, where we have the Template file saved.  terraform init\rThe terraform plan command will inspect the template for resources.  terraform plan\rFinally, we are ready to apply changes. Let\u0026rsquo;s run terraform apply. You will be prompted to enter yes to apply the changes requested by the template. Then, the resources requested will be created as shown below.  terraform apply\rWe will head to AWS console to verify the status of our newly spun EC2. Let\u0026rsquo;s pay close attention to the name of the instance and the AMI ID which should match the names we chose in our Terraform sample template.  Once we have completed the tutorial, we can utilize terraform destroy command to terminate the resources we have created and spun by our sample template. This command will only destroy the resources built by Terraform.  terraform destroy\rConclusion: Congratulations!! We have created a Terraform sample template and ran it to spin an AWS EC2 instance successfully from the Command Line Interface (CLI) on a Raspberry Pi 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/s3/",
	"title": "Create a Static Sample Website on S3 Utilizing AWS CloudFormation",
	"tags": [],
	"description": "",
	"content": "AWS S3 Hosted Website: The objective of tutorial is to host a sample static website on AWS S3, obtain a free domain name from Freenom and assign it to the S3 website and utilize AWS Route 53 as DNS hosting provider. Moreover, an AWS CloudFront distribution is employed to serve the website over AWS\u0026rsquo;s fast content network service with low latency and Lambda@Edge is to add security headers to all web server responses.\nFinally, we will deploy the architecture using AWS CloudFormation to automate the deployment of our website.\nSome parts of the images in this tutorial will be pixilated or hidden to protect all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rTutorial Scenario: Part 1: We should complete this portion of the scenario:\n Register a free domain name at Freenom. Please, refer to my previous tutorial - Step 1 to complete this task. Create a hosted zone in Route 53. Let\u0026rsquo;s navigate to Route 53 on AWS console and select Create hosted zone. This service will cost us ONLY $0.50/month. We could use Freenom DNS service;however, we won\u0026rsquo;t be able to use AWS Certificate Manager to obtain the SSL/TLS certificate.  3. Add the domain name and select Public hosted zone. As a best practice, add a tag to this service.\nOnce we have the hosted zone created, AWS provides us with nameservers to route our traffic to. We will replace our assigned nameservers by the ones we have at Freenom for our domain name as shown below.  Part 2: This portion of the tutorial is completed by our CloudFormation template to automate deployment of our architecture as follows:\n Create S3 buckets and upload a sample website. Create an AWS CloudFront distribution to serve website\u0026rsquo;s traffic through Amazon Content Delivery Network (CDN). Utilize AWS Certificate Manager (ACM) and Obtain SSL/TLS certificate to serve the domain\u0026rsquo;s website securely via HTTPS protocol. Utilize Lambda@Edge to add security headers to every server response. Add a CNAME record for the S3 website endpoint in the hosted zone.  CloudFormation Template Configuration:  Navigate to CloudFormation on AWS console and click on Create stack. Download CloudFormation template to your local device. Then, select Template is ready, Upload a template file and click on Choose file to upload. Click Next.  On Specify stack details, add a stack name, your domain name. My registered domain name is omartesting2021.tk. Click Next.  As a best practice, add a tag to track down the CloudFormation stack. Then, create a new SNS topic by entering a name for your topic and email address to receive updates via email. You will have to confirm the SNS topic subscription once you receive the confirmation email; otherwise, the SNS topic may not work. The SNS topics will update you on the progress of the stack building.  Note: don\u0026rsquo;t forget to select the SNS topic you have created from the dropdown menu.\nOn Capabilities section, select the two options as shown below. These two options are to allow the stack to create an IAM role and CAPABILITY_AUTO_EXPAND to name the resources dynamically. Finally, click on Create stack.  Now, let\u0026rsquo;s grab a cup of coffee (don\u0026rsquo;t forget to add some of that hazelnut creamer 😄 ) and wait on the SNS updates via emails. The CloudFront propagation may take some time. We will definitely appreciate the fact that we have subscribed to an SNS topic to get live updates.  We can also monitor the progress of resource provisioning from within CloudFormation, and once all resources are created successfully, we can access our sample static website from our domain name, www.omartesting2021.tk.\nWe can go back to our created S3 buckets and upload our site files to replace the sample website in the S3 bucket that has bucketroot.\nConclusion: Congratulations. We have successfully hosted a sample website on S3, registered a free domain name from Freenom and assign it to the S3 website via Route 53 hosted zone DNS management. In addition, we have distributed our website access via AWS\u0026rsquo;s low latency content network utilizing CloudFront service and Lambda@Edge to enhance security of the website. Enjoy it.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/_gitlab_fargate/",
	"title": "GitLab Runner on AWS Fargate",
	"tags": [],
	"description": "",
	"content": "What is GitLab Runner GitLab Runner is an application that works with GitLab CI/CD Continues Integration/Continuous Deployment method. Once GitLab Runner is installed and registered, it can run jobs in a pipeline. It\u0026rsquo;s an open source and written in Go language.\nFor more information about GitLab Runner, refer to GitLab\u0026rsquo;s official documentations.\nWhat is AWS Fargate AWS Fargate is a serverless technology that you can use with Amazon ECS to run containers without managing servers or clusters of AWS EC2 instances.\nFor more information about AWS Fargate, refer to AWS\u0026rsquo;s official documentations.\nWhy Running GitLab Runner on AWS Fargate If you have a team of engineers working on several projects and they are heavily utilizing your GitLab runner, there might be limitations with the numbers of the jobs running concurrently. Majority of the time, engineers would have to wait in line for the runner to complete a task prior to executing the next task in the pipeline. AWS Fargate custom executer driver for GitLab Runner comes very handy to resolve this dilemma. It will automatically autoscale and launch containers on AWS ECS to execute each GitLab task without negatively affect the performance. This solution generates an efficient and cost effective scaling capacity.\nGitLab has a well documented process of using and implementing the AWS Fargate custom executor driver. The purpose of this tutorial is to simplify the process.\nNote, GitLab Runner provides several options for different types of executors based on different scenarios. For this tutorial, I have chosen to work with the AWS Fargate executor driver; however, you may find a different executor that suites your needs best.\nLearning Outcomes:  GitLab Runner basic understating.  XXXX XXXXX  What you do you need to complete the tutorial:  GitLab account. It\u0026rsquo;s free to signup for GitLab account. AWS account to create and configure EC2, ECS and ECR resources. Knowledge on how to create and push a Docker image.  Step 1: Building a GitLab Runner and Fargate Driver Container Image As of writing of this tutorial (2/27/22), AWS Fargate does not support running containers in privileged mode. As a result, customers can\u0026rsquo;t build container images inside Fargate containers as AWS imposes security best practices. To read more about this topic, refer to AWS\u0026rsquo;s Building container images on Amazon ECS on AWS Fargate article, dated 31st of March, 2021.\nKaniko is a tool to build container images from a Dockerfile inside a container without the need to have privileged mode. Kaniko resolves the AWS Fargate constrained environment of building a container image inside Fargate containers.\n First of all, if you\u0026rsquo;d like to check your device\u0026rsquo;s architecture, run the following command:  uname -a\rNavigate to Terraform download page and download the proper package for your operating system and architecture. In my case, I chose to download the Linux package arm64 to my RPI-4.  Now, let navigate to the folder where the package is downloaded and unzip it using the unzip utility. If you don\u0026rsquo;t have the unzip utility, use the following command to install it.\nsudo apt install zip -y\rNow, let\u0026rsquo;s unzip the package as shown below:\nWe will run the following commands to move the Terraform zip file to our PATH. Firstly, we need to find the PATH.  echo $PATH\rThen, we will move the Terraform zip file to our main PATH, which is /usr/local/bin/. This will allow us to run Terraform commands from any location without any restrictions.\nsudo mv terraform /usr/local/bin/\rThen, we will cd into the the bin folder as shown below:\nNow, let\u0026rsquo;s run the command terraform. If the results as shown below, we have installed Terraform successfully.\nterraform\rThe results should be as shown below:\nStep 2: Create AWS Identity and Access Management (IAM) user Part 1:  Navigate to AWS console. Go to IAM and click on Users on the left hand side menu. Click on Add user and then add a user name and select access type as Programmatic access as shown below. Then, click on Next.  On Set permissions, click on Attach existing policies directly. search for XX policy and check it. Then, click Next.  On Add tags, it\u0026rsquo;s a best practice to use tags; therefore, adda key and value as shown below. Then, click Next.  Finally, review the add user details and permissions summary and then click on Create user.  Now, we have obtained the Access Key ID and Secret Access Key, let\u0026rsquo;s download the csv file and save it in a secure location for future reference.  Part 2: Install the latest AWS Command Line Interface - CLI version 2 for Linux ARM architecture since my Raspberry Pi 4 is an arm64 architecture. However, you shall Choose the appropriate package for your operating system and device architecture.\ncurl \u0026quot;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026quot; -o \u0026quot;awscliv2.zip\u0026quot;\runzip awscliv2.zip\rsudo ./aws/install\rWe will configure the aws on our device (RPI-4 in my case), which is the fastest way to set up our AWS CLI installation. When you enter this aws configure command, the AWS CLI prompts you for four pieces of information:\n Access key ID Secret access key AWS Region Output format  The access keys consist of an access key ID and a secret access key, which are used to sign programmatic requests that you make to AWS. If you don\u0026rsquo;t have access keys, you can create them from the AWS Management Console. As a best practice, do not use the AWS account root user access keys for any task where it\u0026rsquo;s not required.\n\rLet\u0026rsquo;s start with aws configuration by using the following command in our CLI.\nExample:\naws configure --profile [your profile name]\rActual:\naws configure --profile Terraform\rTo Check if the CLI and profile have been configured properly, run the following command which will lists all of your AWS EC2 instance resources using credentials and settings, which we have defined in the [your profile name] profile\naws ec2 describe-instances --profile [your profile name]\rStep 3: Create a Terraform Sample Template to Spin an AWS EC2 Terraform Commands    Commands Usage     terraform init Download any plugins required to run templates   terraform plan Will give you a list of resources that will be created/deleted   terraform apply WIll create/delete resources   terraform destroy Will delete all the resources created by Terraform   terraform fmt Will format the file with proper indentation    Note: you could always reference Terraform Providers for more commands per provider.\nCreating a Terraform Sample Template  Utilize any Shell script environment or text/code editor of your choice to start the Terraform language code for Terraform sample template as shown below and save the file with a .tf extension:  provider \u0026quot;aws\u0026quot; {\rprofile = \u0026quot;Terraform\u0026quot;\rregion = \u0026quot;us-east-1\u0026quot;\r}\rresource \u0026quot;aws_instance\u0026quot; \u0026quot;demo_instance\u0026quot; {\rami = \u0026quot;ami-042e8287309f5df03\u0026quot;\rinstance_type = \u0026quot;t2.micro\u0026quot;\rtags = {\rName = \u0026quot;Terraform_Demo\u0026quot;\r}\r}\rNote: every AMI has a unique ID and the Ubuntu 20.04 LTS we have chosen has the following AMI ID: ami-042e8287309f5df03. If you would like to spin a different AMI, replace the AMI ID. Also, don\u0026rsquo;t forget to replace the profile name with the name of AWS profile which we have created earlier (in my case is Terraform).\nNow, we are ready to run Terraform command. Let\u0026rsquo;s start with running terraform init, where we have the Template file saved.  terraform init\rThe terraform plan command will inspect the template for resources.  terraform plan\rFinally, we are ready to apply changes. Let\u0026rsquo;s run terraform apply. You will be prompted to enter yes to apply the changes requested by the template. Then, the resources requested will be created as shown below.  terraform apply\rWe will head to AWS console to verify the status of our newly spun EC2. Let\u0026rsquo;s pay close attention to the name of the instance and the AMI ID which should match the names we chose in our Terraform sample template.  Once we have completed the tutorial, we can utilize terraform destroy command to terminate the resources we have created and spun by our sample template. This command will only destroy the resources built by Terraform.  terraform destroy\rConclusion: Congratulations!! We have created a Terraform sample template and ran it to spin an AWS EC2 instance successfully from the Command Line Interface (CLI) on a Raspberry Pi 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/cont/markdown/",
	"title": "Markdown syntax",
	"tags": [],
	"description": "",
	"content": "\rThis page is a shameful copy of the great Grav original page. Only difference is information about image customization (resizing, add CSS classes\u0026hellip;)\n\rLet\u0026rsquo;s face it: Writing content for the Web is tiresome. WYSIWYG editors help alleviate this task, but they generally result in horrible code, or worse yet, ugly web pages.\nMarkdown is a better way to write HTML, without all the complexities and ugliness that usually accompanies it.\nSome of the key benefits are:\n Markdown is simple to learn, with minimal extra characters so it\u0026rsquo;s also quicker to write content. Less chance of errors when writing in markdown. Produces valid XHTML output. Keeps the content and the visual display separate, so you cannot mess up the look of your site. Write in any text editor or Markdown application you like. Markdown is a joy to use!  John Gruber, the author of Markdown, puts it like this:\n The overriding design goal for Markdown’s formatting syntax is to make it as readable as possible. The idea is that a Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. While Markdown’s syntax has been influenced by several existing text-to-HTML filters, the single biggest source of inspiration for Markdown’s syntax is the format of plain text email. \u0026ndash; John Gruber\n Grav ships with built-in support for Markdown and Markdown Extra. You must enable Markdown Extra in your system.yaml configuration file\nWithout further delay, let us go over the main elements of Markdown and what the resulting HTML looks like:\nBookmark this page for easy future reference!\n\rHeadings Headings from h1 through h6 are constructed with a # for each level:\n# h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading Renders to:\nh1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading HTML:\n\u0026lt;h1\u0026gt;h1 Heading\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;h2 Heading\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;h3 Heading\u0026lt;/h3\u0026gt; \u0026lt;h4\u0026gt;h4 Heading\u0026lt;/h4\u0026gt; \u0026lt;h5\u0026gt;h5 Heading\u0026lt;/h5\u0026gt; \u0026lt;h6\u0026gt;h6 Heading\u0026lt;/h6\u0026gt; Comments Comments should be HTML compatible\n\u0026lt;!-- This is a comment --\u0026gt; Comment below should NOT be seen:\nHorizontal Rules The HTML \u0026lt;hr\u0026gt; element is for creating a \u0026ldquo;thematic break\u0026rdquo; between paragraph-level elements. In markdown, you can create a \u0026lt;hr\u0026gt; with any of the following:\n ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks  renders to:\n Body Copy Body copy written as normal, plain text will be wrapped with \u0026lt;p\u0026gt;\u0026lt;/p\u0026gt; tags in the rendered HTML.\nSo this body copy:\nLorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad. renders to this HTML:\n\u0026lt;p\u0026gt;Lorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad.\u0026lt;/p\u0026gt; Emphasis Bold For emphasizing a snippet of text with a heavier font-weight.\nThe following snippet of text is rendered as bold text.\n**rendered as bold text** renders to:\nrendered as bold text\nand this HTML\n\u0026lt;strong\u0026gt;rendered as bold text\u0026lt;/strong\u0026gt; Italics For emphasizing a snippet of text with italics.\nThe following snippet of text is rendered as italicized text.\n_rendered as italicized text_ renders to:\nrendered as italicized text\nand this HTML:\n\u0026lt;em\u0026gt;rendered as italicized text\u0026lt;/em\u0026gt; Strikethrough In GFM (GitHub flavored Markdown) you can do strikethroughs.\n~~Strike through this text.~~ Which renders to:\nStrike through this text.\nHTML:\n\u0026lt;del\u0026gt;Strike through this text.\u0026lt;/del\u0026gt; Blockquotes For quoting blocks of content from another source within your document.\nAdd \u0026gt; before any text you want to quote.\n\u0026gt; **Fusion Drive** combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. Renders to:\n Fusion Drive combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined.\n and this HTML:\n\u0026lt;blockquote\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Fusion Drive\u0026lt;/strong\u0026gt; combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined.\u0026lt;/p\u0026gt; \u0026lt;/blockquote\u0026gt; Blockquotes can also be nested:\n\u0026gt; Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. \u0026gt; \u0026gt; \u0026gt; Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. \u0026gt; \u0026gt; Mauris sit amet ligula egestas, feugiat metus tincidunt, luctus libero. Donec congue finibus tempor. Vestibulum aliquet sollicitudin erat, ut aliquet purus posuere luctus. Renders to:\n Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi.\n Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam.\n Mauris sit amet ligula egestas, feugiat metus tincidunt, luctus libero. Donec congue finibus tempor. Vestibulum aliquet sollicitudin erat, ut aliquet purus posuere luctus.\n Notices The old mechanism for notices overriding the block quote syntax (\u0026gt;\u0026gt;\u0026gt;) has been deprecated. Notices are now handled via a dedicated plugin called Markdown Notices\n\rLists Unordered A list of items in which the order of the items does not explicitly matter.\nYou may use any of the following symbols to denote bullets for each list item:\n* valid bullet - valid bullet + valid bullet For example\n+ Lorem ipsum dolor sit amet + Consectetur adipiscing elit + Integer molestie lorem at massa + Facilisis in pretium nisl aliquet + Nulla volutpat aliquam velit - Phasellus iaculis neque - Purus sodales ultricies - Vestibulum laoreet porttitor sem - Ac tristique libero volutpat at + Faucibus porta lacus fringilla vel + Aenean sit amet erat nunc + Eget porttitor lorem Renders to:\n Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit  Phasellus iaculis neque Purus sodales ultricies Vestibulum laoreet porttitor sem Ac tristique libero volutpat at   Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem  And this HTML\n\u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;Lorem ipsum dolor sit amet\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Consectetur adipiscing elit\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Integer molestie lorem at massa\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Facilisis in pretium nisl aliquet\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Nulla volutpat aliquam velit \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;Phasellus iaculis neque\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Purus sodales ultricies\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Vestibulum laoreet porttitor sem\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Ac tristique libero volutpat at\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Faucibus porta lacus fringilla vel\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Aenean sit amet erat nunc\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Eget porttitor lorem\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; Ordered A list of items in which the order of items does explicitly matter.\n1. Lorem ipsum dolor sit amet 4. Consectetur adipiscing elit 2. Integer molestie lorem at massa 8. Facilisis in pretium nisl aliquet 4. Nulla volutpat aliquam velit 99. Faucibus porta lacus fringilla vel 21. Aenean sit amet erat nunc 6. Eget porttitor lorem Renders to:\n Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem  And this HTML:\n\u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;Lorem ipsum dolor sit amet\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Consectetur adipiscing elit\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Integer molestie lorem at massa\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Facilisis in pretium nisl aliquet\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Nulla volutpat aliquam velit\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Faucibus porta lacus fringilla vel\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Aenean sit amet erat nunc\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Eget porttitor lorem\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; TIP: If you just use 1. for each number, Markdown will automatically number each item. For example:\n1. Lorem ipsum dolor sit amet 1. Consectetur adipiscing elit 1. Integer molestie lorem at massa 1. Facilisis in pretium nisl aliquet 1. Nulla volutpat aliquam velit 1. Faucibus porta lacus fringilla vel 1. Aenean sit amet erat nunc 1. Eget porttitor lorem Renders to:\n Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem  Code Inline code Wrap inline snippets of code with `.\nIn this example, `\u0026lt;section\u0026gt;\u0026lt;/section\u0026gt;` should be wrapped as **code**. Renders to:\nIn this example, \u0026lt;section\u0026gt;\u0026lt;/section\u0026gt; should be wrapped as code.\nHTML:\n\u0026lt;p\u0026gt;In this example, \u0026lt;code\u0026gt;\u0026amp;lt;section\u0026amp;gt;\u0026amp;lt;/section\u0026amp;gt;\u0026lt;/code\u0026gt; should be wrapped as \u0026lt;strong\u0026gt;code\u0026lt;/strong\u0026gt;.\u0026lt;/p\u0026gt; Indented code Or indent several lines of code by at least two spaces, as in:\n// Some comments line 1 of code line 2 of code line 3 of code Renders to:\n// Some comments\rline 1 of code\rline 2 of code\rline 3 of code\r HTML:\n\u0026lt;pre\u0026gt; \u0026lt;code\u0026gt; // Some comments line 1 of code line 2 of code line 3 of code \u0026lt;/code\u0026gt; \u0026lt;/pre\u0026gt; Block code \u0026ldquo;fences\u0026rdquo; Use \u0026ldquo;fences\u0026rdquo; ``` to block in multiple lines of code.\nSample text here... HTML:\n\u0026lt;pre\u0026gt; \u0026lt;code\u0026gt;Sample text here...\u0026lt;/code\u0026gt; \u0026lt;/pre\u0026gt; Syntax highlighting GFM, or \u0026ldquo;GitHub Flavored Markdown\u0026rdquo; also supports syntax highlighting. To activate it, simply add the file extension of the language you want to use directly after the first code \u0026ldquo;fence\u0026rdquo;, ```js, and syntax highlighting will automatically be applied in the rendered HTML.\nSee Code Highlighting for additional documentation.\nFor example, to apply syntax highlighting to JavaScript code:\n```js grunt.initConfig({ assemble: { options: { assets: \u0026#39;docs/assets\u0026#39;, data: \u0026#39;src/data/*.{json,yml}\u0026#39;, helpers: \u0026#39;src/custom-helpers.js\u0026#39;, partials: [\u0026#39;src/partials/**/*.{hbs,md}\u0026#39;] }, pages: { options: { layout: \u0026#39;default.hbs\u0026#39; }, files: { \u0026#39;./\u0026#39;: [\u0026#39;src/templates/pages/index.hbs\u0026#39;] } } } }; ``` Renders to:\ngrunt.initConfig({ assemble: { options: { assets: \u0026#39;docs/assets\u0026#39;, data: \u0026#39;src/data/*.{json,yml}\u0026#39;, helpers: \u0026#39;src/custom-helpers.js\u0026#39;, partials: [\u0026#39;src/partials/**/*.{hbs,md}\u0026#39;] }, pages: { options: { layout: \u0026#39;default.hbs\u0026#39; }, files: { \u0026#39;./\u0026#39;: [\u0026#39;src/templates/pages/index.hbs\u0026#39;] } } } }; Tables Tables are created by adding pipes as dividers between each cell, and by adding a line of dashes (also separated by bars) beneath the header. Note that the pipes do not need to be vertically aligned.\n| Option | Description | | ------ | ----------- | | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | Renders to:\n   Option Description     data path to data files to supply the data that will be passed into templates.   engine engine to be used for processing templates. Handlebars is the default.   ext extension to be used for dest files.    And this HTML:\n\u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;Option\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Description\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;data\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;path to data files to supply the data that will be passed into templates.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;engine\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;engine to be used for processing templates. Handlebars is the default.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;ext\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;extension to be used for dest files.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; Right aligned text Adding a colon on the right side of the dashes below any heading will right align text for that column.\n| Option | Description | | ------:| -----------:| | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. |    Option Description     data path to data files to supply the data that will be passed into templates.   engine engine to be used for processing templates. Handlebars is the default.   ext extension to be used for dest files.    Links Basic link [Assemble](http://assemble.io) Renders to (hover over the link, there is no tooltip):\nAssemble\nHTML:\n\u0026lt;a href=\u0026#34;http://assemble.io\u0026#34;\u0026gt;Assemble\u0026lt;/a\u0026gt; Add a tooltip [Upstage](https://github.com/upstage/ \u0026#34;Visit Upstage!\u0026#34;) Renders to (hover over the link, there should be a tooltip):\nUpstage\nHTML:\n\u0026lt;a href=\u0026#34;https://github.com/upstage/\u0026#34; title=\u0026#34;Visit Upstage!\u0026#34;\u0026gt;Upstage\u0026lt;/a\u0026gt; Named Anchors Named anchors enable you to jump to the specified anchor point on the same page. For example, each of these chapters:\n# Table of Contents * [Chapter 1](#chapter-1) * [Chapter 2](#chapter-2) * [Chapter 3](#chapter-3) will jump to these sections:\n## Chapter 1 \u0026lt;a id=\u0026#34;chapter-1\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; Content for chapter one. ## Chapter 2 \u0026lt;a id=\u0026#34;chapter-2\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; Content for chapter one. ## Chapter 3 \u0026lt;a id=\u0026#34;chapter-3\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; Content for chapter one. NOTE that specific placement of the anchor tag seems to be arbitrary. They are placed inline here since it seems to be unobtrusive, and it works.\nImages Images have a similar syntax to links but include a preceding exclamation point.\n![Minion](https://octodex.github.com/images/minion.png) or\n![Alt text](https://octodex.github.com/images/stormtroopocat.jpg \u0026#34;The Stormtroopocat\u0026#34;) Like links, Images also have a footnote style syntax\nAlternative usage : note images ![Alt text][id] With a reference later in the document defining the URL location:\n[id]: https://octodex.github.com/images/dojocat.jpg \u0026quot;The Dojocat\u0026quot;\r Resizing image Add HTTP parameters width and/or height to the link image to resize the image. Values are CSS values (default is auto).\n![Minion](https://octodex.github.com/images/minion.png?width=20pc) ![Minion](https://octodex.github.com/images/minion.png?height=50px) ![Minion](https://octodex.github.com/images/minion.png?height=50px\u0026amp;width=300px) Add CSS classes Add a HTTP classes parameter to the link image to add CSS classes. shadowand border are available but you could define other ones.\n![stormtroopocat](https://octodex.github.com/images/stormtroopocat.jpg?classes=shadow) ![stormtroopocat](https://octodex.github.com/images/stormtroopocat.jpg?classes=border) ![stormtroopocat](https://octodex.github.com/images/stormtroopocat.jpg?classes=border,shadow) Lightbox Add a HTTP featherlight parameter to the link image to disable lightbox. By default lightbox is enabled using the featherlight.js plugin. You can disable this by defining featherlight to false.\n![Minion](https://octodex.github.com/images/minion.png?featherlight=false) "
},
{
	"uri": "https://omar2cloud.github.io/cont/syntaxhighlight/",
	"title": "Code highlighting",
	"tags": [],
	"description": "",
	"content": "Learn theme uses highlight.js to provide code syntax highlighting.\nMarkdown syntax Wrap the code block with three backticks and the name of the language. Highlight will try to auto detect the language if one is not provided.\n```json [ { \u0026#34;title\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: [12000, 20000], \u0026#34;description\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;sensitive\u0026#34;: false} }, { \u0026#34;title\u0026#34;: \u0026#34;oranges\u0026#34;, \u0026#34;count\u0026#34;: [17500, null], \u0026#34;description\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;sensitive\u0026#34;: false} } ] ``` Renders to:\n[ { \u0026#34;title\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: [12000, 20000], \u0026#34;description\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;sensitive\u0026#34;: false} }, { \u0026#34;title\u0026#34;: \u0026#34;oranges\u0026#34;, \u0026#34;count\u0026#34;: [17500, null], \u0026#34;description\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;sensitive\u0026#34;: false} } ] Supported languages Learn theme ships with its own version of highlight.js to support offline browsing. The included package supports 38 common languages, as described on the highlight.js download page.\nIdentifying failed language detection Highlight will write a warning to the browser console if a requested language was not found. For example, the following code block references an imaginary language foo. An error will be output to the console on this page.\n```foo bar ``` Could not find the language 'foo', did you forget to load/include a language module?(anonymous) @ highlight.pack.js\rSupporting additional languages To support languages other than the 38 common languages included in the default highlight.js you will need to download your own version of highlight.js and add it to your site content.\nDownload custom highlight.js Visit https://highlightjs.org/download/ and select your desired language support. Note that more languages means greater package size.\nAdd custom highlight.js to static resources Inside the zip archive downloaded from highlight.js extract the file named highlight.pack.js. Move this file to the new location\nstatic/js/highlight.pack.js\rDo not replace the existing file at themes/hugo-theme-learn/static/js/highlight.pack.js.\nIncluding the file in the correct path will override the theme default highlight.pack.js and prevent issues caused in the future if the theme default file is updated.\nFurther usage information See https://highlightjs.org/usage/\n"
},
{
	"uri": "https://omar2cloud.github.io/cont/menushortcuts/",
	"title": "Menu extra shortcuts",
	"tags": [],
	"description": "",
	"content": "You can define additional menu entries or shortcuts in the navigation menu without any link to content.\nBasic configuration Edit the website configuration config.toml and add a [[menu.shortcuts]] entry for each link your want to add.\nExample from the current website:\n[[menu.shortcuts]] name = \u0026quot;\u0026lt;i class='fab fa-github'\u0026gt;\u0026lt;/i\u0026gt; Github repo\u0026quot;\ridentifier = \u0026quot;ds\u0026quot;\rurl = \u0026quot;https://github.com/matcornic/hugo-theme-learn\u0026quot;\rweight = 10\r[[menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fas fa-camera'\u0026gt;\u0026lt;/i\u0026gt; Showcases\u0026quot;\rurl = \u0026quot;/showcase\u0026quot;\rweight = 11\r[[menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fas fa-bookmark'\u0026gt;\u0026lt;/i\u0026gt; Hugo Documentation\u0026quot;\ridentifier = \u0026quot;hugodoc\u0026quot;\rurl = \u0026quot;https://gohugo.io/\u0026quot;\rweight = 20\r[[menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fas fa-bullhorn'\u0026gt;\u0026lt;/i\u0026gt; Credits\u0026quot;\rurl = \u0026quot;/credits\u0026quot;\rweight = 30\r By default, shortcuts are preceded by a title. This title can be disabled by setting disableShortcutsTitle=true. However, if you want to keep the title but change its value, it can be overriden by changing your local i18n translation string configuration.\nFor example, in your local i18n/en.toml file, add the following content\n[Shortcuts-Title]\rother = \u0026quot;\u0026lt;Your value\u0026gt;\u0026quot;\r Read more about hugo menu and hugo i18n translation strings\nConfiguration for Multilingual mode When using a multilingual website, you can set different menus for each language. In the config.toml file, prefix your menu configuration by Languages.\u0026lt;language-id\u0026gt;.\nExample from the current website:\n[Languages]\r[Languages.en]\rtitle = \u0026quot;Documentation for Hugo Learn Theme\u0026quot;\rweight = 1\rlanguageName = \u0026quot;English\u0026quot;\r[[Languages.en.menu.shortcuts]] name = \u0026quot;\u0026lt;i class='fab fa-github'\u0026gt;\u0026lt;/i\u0026gt; Github repo\u0026quot;\ridentifier = \u0026quot;ds\u0026quot;\rurl = \u0026quot;https://github.com/matcornic/hugo-theme-learn\u0026quot;\rweight = 10\r[[Languages.en.menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fas fa-camera'\u0026gt;\u0026lt;/i\u0026gt; Showcases\u0026quot;\rurl = \u0026quot;/showcase\u0026quot;\rweight = 11\r[[Languages.en.menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fas fa-bookmark'\u0026gt;\u0026lt;/i\u0026gt; Hugo Documentation\u0026quot;\ridentifier = \u0026quot;hugodoc\u0026quot;\rurl = \u0026quot;https://gohugo.io/\u0026quot;\rweight = 20\r[[Languages.en.menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fas fa-bullhorn'\u0026gt;\u0026lt;/i\u0026gt; Credits\u0026quot;\rurl = \u0026quot;/credits\u0026quot;\rweight = 30\r[Languages.fr]\rtitle = \u0026quot;Documentation du thème Hugo Learn\u0026quot;\rweight = 2\rlanguageName = \u0026quot;Français\u0026quot;\r[[Languages.fr.menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fab fa-github'\u0026gt;\u0026lt;/i\u0026gt; Repo Github\u0026quot;\ridentifier = \u0026quot;ds\u0026quot;\rurl = \u0026quot;https://github.com/matcornic/hugo-theme-learn\u0026quot;\rweight = 10\r[[Languages.fr.menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fas fa-camera'\u0026gt;\u0026lt;/i\u0026gt; Vitrine\u0026quot;\rurl = \u0026quot;/showcase\u0026quot;\rweight = 11\r[[Languages.fr.menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fas fa-bookmark'\u0026gt;\u0026lt;/i\u0026gt; Documentation Hugo\u0026quot;\ridentifier = \u0026quot;hugodoc\u0026quot;\rurl = \u0026quot;https://gohugo.io/\u0026quot;\rweight = 20\r[[Languages.fr.menu.shortcuts]]\rname = \u0026quot;\u0026lt;i class='fas fa-bullhorn'\u0026gt;\u0026lt;/i\u0026gt; Crédits\u0026quot;\rurl = \u0026quot;/credits\u0026quot;\rweight = 30\r Read more about hugo menu and hugo multilingual menus\n"
},
{
	"uri": "https://omar2cloud.github.io/cont/icons/",
	"title": "Icons and logos",
	"tags": [],
	"description": "",
	"content": "The Learn theme for Hugo loads the Font Awesome library, allowing you to easily display any icon or logo available in the Font Awesome free collection.\nFinding an icon Browse through the available icons in the Font Awesome Gallery. Notice that the free filter is enabled, as only the free icons are available by default.\nOnce on the Font Awesome page for a specific icon, for example the page for the heart, copy the HTML reference and paste into the markdown content.\nThe HTML to include the heart icon is:\n\u0026lt;i class=\u0026quot;fas fa-heart\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;\rIncluding in markdown Paste the \u0026lt;i\u0026gt; HTML into markup and Font Awesome will load the relevant icon.\nBuilt with \u0026lt;i class=\u0026quot;fas fa-heart\u0026quot;\u0026gt;\u0026lt;/i\u0026gt; from Grav and Hugo\rWhich appears as\nBuilt with from Grav and Hugo\nCustomising icons Font Awesome provides many ways to modify the icon\n Change colour (by default the icon will inherit the parent colour) Increase or decrease size Rotate Combine with other icons  Check the full documentation on web fonts with CSS for more.\n"
},
{
	"uri": "https://omar2cloud.github.io/cont/i18n/",
	"title": "Multilingual and i18n",
	"tags": [],
	"description": "",
	"content": "Learn theme is fully compatible with Hugo multilingual mode.\nIt provides:\n Translation strings for default values (English and French). Feel free to contribute ! Automatic menu generation from multilingual content In-browser language switching  Basic configuration After learning how Hugo handle multilingual websites, define your languages in your config.toml file.\nFor example with current French and English website.\n# English is the default language defaultContentLanguage = \u0026#34;en\u0026#34; # Force to have /en/my-page and /fr/my-page routes, even for default language. defaultContentLanguageInSubdir= true [Languages] [Languages.en] title = \u0026#34;Documentation for Hugo Learn Theme\u0026#34; weight = 1 languageName = \u0026#34;English\u0026#34; [Languages.fr] title = \u0026#34;Documentation du thème Hugo Learn\u0026#34; weight = 2 languageName = \u0026#34;Français\u0026#34; Then, for each new page, append the id of the language to the file.\n Single file my-page.md is split in two files:  in English: my-page.en.md in French: my-page.fr.md   Single file _index.md is split in two files:  in English: _index.en.md in French: _index.fr.md    Be aware that only translated pages are displayed in menu. It\u0026rsquo;s not replaced with default language content.\n\rUse slug Front Matter parameter to translate urls too.\n\rOverwrite translation strings Translations strings are used for common default values used in the theme (Edit this page button, Search placeholder and so on). Translations are available in french and english but you may use another language or want to override default values.\nTo override these values, create a new file in your local i18n folder i18n/\u0026lt;idlanguage\u0026gt;.toml and inspire yourself from the theme themes/hugo-theme-learn/i18n/en.toml\nBy the way, as these translations could be used by other people, please take the time to propose a translation by making a PR to the theme !\nDisable language switching Switching the language in the browser is a great feature, but for some reasons you may want to disable it.\nJust set disableLanguageSwitchingButton=true in your config.toml\n[params] # When using mulitlingual website, disable the switch language button. disableLanguageSwitchingButton = true "
},
{
	"uri": "https://omar2cloud.github.io/cont/tags/",
	"title": "Tags",
	"tags": ["documentation", "tutorial"],
	"description": "",
	"content": "Learn theme support one default taxonomy of gohugo: the tag feature.\nConfiguration Just add tags to any page:\n--- date: 2018-11-29T08:41:44+01:00 title: Theme tutorial weight: 15 tags: [\u0026#34;tutorial\u0026#34;, \u0026#34;theme\u0026#34;] --- Behavior The tags are displayed at the top of the page, in their insertion order.\nEach tag is a link to a Taxonomy page displaying all the articles with the given tag.\nList all the tags In the config.toml file you can add a shortcut to display all the tags\n[[menu.shortcuts]] name = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-tags\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Tags\u0026#34; url = \u0026#34;/tags\u0026#34; weight = 30 "
},
{
	"uri": "https://omar2cloud.github.io/cont/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Chapter 2 Content Find out how to create and organize your content quickly and intuitively. \u0026ndash;\u0026gt;\n"
},
{
	"uri": "https://omar2cloud.github.io/",
	"title": "Learn with Omar",
	"tags": [],
	"description": "",
	"content": "My Journey Sharing knowledge with the community as an AWS Solutions Architect Associate and a Mechanical Engineer adds greatly to self-growth, self-motivation and enhance the knowledge base as well. The sense of purpose is what make a difference; therefore, I would like to share my journey with the community.\nMajority of the tutorials I\u0026rsquo;ve added to the site, and will continue adding, are gained throughout my Post Graduate at the University of Texas - Austin and Great Learning program. Moreover, as I\u0026rsquo;m in ❤️ with Raspberry Pis, you will find lots of tutorials/projects utilizing these pretty little things.\nI strongly believe that developing a self-learning skill contributes significantly in sharpening more crucial skills as a result such as, problem solving, time management and critical thinking.\nThis is my journey to the cloud. Buckle up and let\u0026rsquo;s take this trip and learn together.\n"
},
{
	"uri": "https://omar2cloud.github.io/tags/documentation/",
	"title": "documentation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://omar2cloud.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://omar2cloud.github.io/tags/tutorial/",
	"title": "tutorial",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://omar2cloud.github.io/aws/_codecommit/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "AWS CodeCommit, Elastic Beanstalk and CodePipeline AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. CodeCommit eliminates the need to operate your own source control system or worry about scaling its infrastructure. You can use CodeCommit to securely store anything from source code to binaries, and it works seamlessly with your existing Git tools. For more information about AWS CodeCommit.\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. For more information about AWS Elastic Beanstalk.\nAWS CodePipeline is a continuous delivery service offered by AWS to model, visualize, and automate the release of applications. This service allows for rapid modeling and configuring of different steps of an application release process. It automates the necessary process of change of application deployment. For information about AWS CodePipeline.\nLearning Outcomes and Tutorial Scenario: In this tutorial, we will create a two-stage pipeline that uses a AWS CodeCommit as repository and AWS Elastic Beanstalk to release a sample application.\n Creating a CodeCommit repository and upload the sample application zip file. Creating an application in Elastic Beanstalk. Creating a pipeline in CodePipeline.  Step 1: Creating a CodeCommit repository for the sample application.  Download the sample application from AWS GitHub and save it into a folder or directory on your local device. Do not unzip the file.  Navigate to CodeCommit and create a repo by giving it a name and an optional description.  Next, upload the sample application zip file to the repo. You may need to give the Author name, Email address and description. Then, click on COmmit change as shown below.  Step 2: Creating an application in Elastic Beanstalk  Navigate to Elastic Beanstalk and click on Create Application. On the left hand menu, click on Environments and select Web server environment. Then, enter a name for the application, Application tag, choose PHP for a platform as shown below and then click on Create application.  Note: it will take a few minutes to setup the environment. Once the environment is setup, you should see the screen below\nstopped here.\nClick on the Application to create a new environment and then click on Create a new environment. Select Web server enviornment and click Select.  Step 4: Creating our first pipeline in CodePipeline To create a CodePipeline automated release process:\n  Navigate to Pipeline menu and Create pipeline.\n  Enter a name for the pipeline.\n  In Service role, choose new service role to allow CodePipeline to create a new service role in IAM. In Role name, the role and policy name both default to this format: AWSCodePipelineServiceRole-region-pipeline_name.\n  Leave the defaults for the Advanced section and then choose Next.\n  In Step 2: Add source stage, in Source provider, choose Amazon S3. In Bucket, enter the name of the S3 bucket we have previously created. In S3 object key, enter the object key with or without a file path, and remember to include the file extension. For example, for SampleApp_Windows.zip, enter the sample file name as shown in this example:  aws-codepipeline-s3-aws-codedeploy_linux.zip\rChoose Next step and leave defaults for Amazon CloudWatch Events as recommended. Choose Next.  In Step 3: Add build stage, choose Skip build stage, and then accept the warning message by choosing Skip again. Choose Next. In Step 4: Add deploy stage, in Deploy provider, choose AWS CodeDeploy. The Region field defaults to the same AWS Region as your pipeline. In Application name, enter the name your chose previously for the application, or choose the Refresh button, and then choose the application name from the list. In Deployment group, enter the name your chose previously for the deployment group, or choose it from the list, and then choose Next.  In Step 5: Review, review the information, and then choose Create pipeline. The pipeline starts to run. You can view progress and success and failure messages as the CodePipeline sample deploys a webpage to each of the Ubuntu EC2 instances in the CodeDeploy deployment.  To verify your pipeline ran successfully On the Description tab, in Public DNS, copy the address, and then paste it into the address bar of your web browser. View the index page for the sample application you uploaded to your S3 bucket.\nConclusion: By the end of this tutorial, we have successfully created a simple pipeline in CodePipeline. The pipeline has two stages:\n  A source stage named Source, which detects changes in the versioned sample application stored in the S3 bucket and pulls those changes into the pipeline.\n  A Deploy stage that deploys those changes to EC2 instances with CodeDeploy.\n  Congratulations!\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/_s3.en-copy/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "AWS S3 Hosted Website: The objective of tutorial is to host a sample static website on AWS S3, obtain a free domain name from Freenom and assign it to the S3 website. Moreover, an AWS CloudFront distribution is employed to serve the website over AWS\u0026rsquo;s fast content network service with low latency and Lambda@Edge is to add security headers to all web server responses.\nFinally, we will deploy the architecture using AWS CloudFormation to automate the deployment of our website.\nTutorial Scenario:  Register a free domain name from Freenom. Please, refer to my previous tutorial to complete this task Create an AWS S3 to host a sample static website\u0026rsquo;s content Create an AWS CloudFront distribution to serve website\u0026rsquo;s traffic through Amazon Content Delivery Network (CDN) Utilize AWS Certificate Manager (ACM) and Obtain SSL/TLS certificate to serve the domain\u0026rsquo;s website securely with HTTPS protocol. Utilize Lambda@Edge to add security header to every server response Add CNAME records for the S3 website endpoint to Cloudflare DNS management Configure AWS CloudFormation to deploy the solution  Step 6: Add CNAME records Unfortunately, Amazon does not allow unreputable domain registrar to use S3 endpoints. The workaround this barrier is to use Cloudflare DNS management. It\u0026rsquo;s a free, a secure and a reliable DNS service.\nWe will need to add a Canonical name- CNAME to map our domain name to S3 website endpoint.\n  First of all, sigup for a free Cloudflare account.\n  Click on Add a Site and enter a name of your choosing for the site. You may refer to my previous tutorial to complete this task. Free Domain Name\n  Once the site registration is completed and verfied, let\u0026rsquo;s click on DNS and Add record. We will add two CNAME records type as shown below:\n  My website domain name is omartesting.tk, which is exactly the S3 bucket name.\n   Type Name Content TTL Proxy status     CNAME [your website name/bucket name] [bucket endpoint] Auto Proxied   CNAME www [bucket endpoint] Auto Proxied    Now, let navigate to the folder where the package is downloaded and unzip it using the unzip utility. If you don\u0026rsquo;t have the unzip utility, use the following command to install it.\nsudo apt install zip -y\rStep 7: Configure AWS CloudFormation to deploy the solution The access keys consist of an access key ID and a secret access key, which are used to sign programmatic requests that you make to AWS. If you don\u0026rsquo;t have access keys, you can create them from the AWS Management Console. As a best practice, do not use the AWS account root user access keys for any task where it\u0026rsquo;s not required.\n\r   Commands Usage     terraform init Download any plugins required to run templates   terraform plan Will give you a list of resources that will be created/deleted   terraform apply WIll create/delete resources   terraform destroy Will delete all the resources created by Terraform   terraform fmt Will format the file with proper indentation    provider \u0026quot;aws\u0026quot; {\rprofile = \u0026quot;Terraform\u0026quot;\rregion = \u0026quot;us-east-1\u0026quot;\r}\rresource \u0026quot;aws_instance\u0026quot; \u0026quot;demo_instance\u0026quot; {\rami = \u0026quot;ami-042e8287309f5df03\u0026quot;\rinstance_type = \u0026quot;t2.micro\u0026quot;\rtags = {\rName = \u0026quot;Terraform_Demo\u0026quot;\r}\r}\rConclusion: Congratulations!! We have created a Terraform sample template and ran it to spin an AWS EC2 instance successfully from the Command Line Interface (CLI) on a Raspberry Pi 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/new-folder/configuration/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Global site parameters On top of Hugo global configuration, Hugo-theme-learn lets you define the following parameters in your config.toml (here, values are default).\nNote that some of these parameters are explained in details in other sections of this documentation.\n[params] # Prefix URL to edit current page. Will display an \u0026#34;Edit this page\u0026#34; button on top right hand corner of every page. # Useful to give opportunity to people to create merge request for your doc. # See the config.toml file from this documentation site to have an example. editURL = \u0026#34;\u0026#34; # Author of the site, will be used in meta information author = \u0026#34;\u0026#34; # Description of the site, will be used in meta information description = \u0026#34;\u0026#34; # Shows a checkmark for visited pages on the menu showVisitedLinks = false # Disable search function. It will hide search bar disableSearch = false # Javascript and CSS cache are automatically busted when new version of site is generated. # Set this to true to disable this behavior (some proxies don\u0026#39;t handle well this optimization) disableAssetsBusting = false # Set this to true to disable copy-to-clipboard button for inline code. disableInlineCopyToClipBoard = false # A title for shortcuts in menu is set by default. Set this to true to disable it. disableShortcutsTitle = false # If set to false, a Home button will appear below the search bar on the menu. # It is redirecting to the landing page of the current language if specified. (Default is \u0026#34;/\u0026#34;) disableLandingPageButton = true # When using mulitlingual website, disable the switch language button. disableLanguageSwitchingButton = false # Hide breadcrumbs in the header and only show the current page title disableBreadcrumb = true # If set to true, prevents Hugo from including the mermaid module if not needed (will reduce load times and traffic) disableMermaid = false # Specifies the remote location of the mermaid js customMermaidURL = \u0026#34;https://unpkg.com/mermaid@8.8.0/dist/mermaid.min.js\u0026#34; # Hide Next and Previous page buttons normally displayed full height beside content disableNextPrev = true # Order sections in menu by \u0026#34;weight\u0026#34; or \u0026#34;title\u0026#34;. Default to \u0026#34;weight\u0026#34; ordersectionsby = \u0026#34;weight\u0026#34; # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;\u0026#34; # Provide a list of custom css files to load relative from the `static/` folder in the site root. custom_css = [\u0026#34;css/foo.css\u0026#34;, \u0026#34;css/bar.css\u0026#34;] # Change the title separator. Default to \u0026#34;::\u0026#34;. titleSeparator = \u0026#34;-\u0026#34; Activate search If not already present, add the follow lines in the same config.toml file.\n[outputs] home = [ \u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] Learn theme uses the last improvement available in hugo version 20+ to generate a json index file ready to be consumed by lunr.js javascript search engine.\n Hugo generate lunrjs index.json at the root of public folder. When you build the site with hugo server, hugo generates it internally and of course it doesn’t show up in the filesystem\n Mermaid The mermaid configuration parameters can also be set on a specific page. In this case, the global parameter would be overwritten by the local one.\n Example:\nMermaid is globally disabled. By default it won\u0026rsquo;t be loaded by any page.\nOn page \u0026ldquo;Architecture\u0026rdquo; you need a class diagram. You can set the mermaid parameters locally to only load mermaid on this page (not on the others).\n You also can disable mermaid for specific pages while globally enabled.\nHome Button Configuration If the disableLandingPage option is set to false, an Home button will appear on the left menu. It is an alternative for clicking on the logo. To edit the appearance, you will have to configure two parameters for the defined languages:\n[Lanugages] [Lanugages.en] ... landingPageURL = \u0026#34;/en\u0026#34; landingPageName = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-home\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Redirect to Home\u0026#34; ... [Lanugages.fr] ... landingPageURL = \u0026#34;/fr\u0026#34; landingPageName = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-home\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Accueil\u0026#34; ... If those params are not configured for a specific language, they will get their default values:\nlandingPageURL = \u0026#34;/\u0026#34; landingPageName = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-home\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Home\u0026#34; The home button is going to looks like this:\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/new-folder/installation/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "The following steps are here to help you initialize your new website. If you don\u0026rsquo;t know Hugo at all, we strongly suggest you learn more about it by following this great documentation for beginners.\nCreate your project Hugo provides a new command to create a new website.\nhugo new site \u0026lt;new_project\u0026gt;\rInstall the theme Install the Hugo-theme-learn theme by following this documentation\nThis theme\u0026rsquo;s repository is: https://github.com/matcornic/hugo-theme-learn.git\nAlternatively, you can download the theme as .zip file and extract it in the themes directory\nBasic configuration When building the website, you can set a theme by using --theme option. However, we suggest you modify the configuration file (config.toml) and set the theme as the default. You can also add the [outputs] section to enable the search functionality.\n# Change the default theme to be use when building the site with Hugo theme = \u0026#34;hugo-theme-learn\u0026#34; # For search functionality [outputs] home = [ \u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] Create your first chapter page Chapters are pages that contain other child pages. It has a special layout style and usually just contains a chapter name, the title and a brief abstract of the section.\n### Chapter 1\r# Basics\rDiscover what this Hugo theme is all about and the core concepts behind it.\rrenders as\nHugo-theme-learn provides archetypes to create skeletons for your website. Begin by creating your first chapter page with the following command\nhugo new --kind chapter basics/_index.md\rBy opening the given file, you should see the property chapter=true on top, meaning this page is a chapter.\nBy default all chapters and pages are created as a draft. If you want to render these pages, remove the property draft: true from the metadata.\nCreate your first content pages Then, create content pages inside the previously created chapter. Here are two ways to create content in the chapter:\nhugo new basics/first-content.md\rhugo new basics/second-content/_index.md\rFeel free to edit thoses files by adding some sample content and replacing the title value in the beginning of the files.\nLaunching the website locally Launch by using the following command:\nhugo serve\rGo to http://localhost:1313\nYou should notice three things:\n You have a left-side Basics menu, containing two submenus with names equal to the title properties in the previously created files. The home page explains how to customize it by following the instructions. When you run hugo serve, when the contents of the files change, the page automatically refreshes with the changes. Neat!  Build the website When your site is ready to deploy, run the following command:\nhugo\rA public folder will be generated, containing all static content and assets for your website. It can now be deployed on any web server.\nThis website can be automatically published and hosted with Netlify (Read more about Automated HUGO deployments with Netlify). Alternatively, you can use Github pages\n\r"
},
{
	"uri": "https://omar2cloud.github.io/aws/new-folder/requirements/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Thanks to the simplicity of Hugo, this page is as empty as this theme needs requirements.\nJust download latest version of Hugo binary (\u0026gt; 0.25) for your OS (Windows, Linux, Mac) : it\u0026rsquo;s that simple.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/new-folder/style-customization/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Hugo-theme-learn has been built to be as configurable as possible by defining multiple partials\nIn themes/hugo-theme-learn/layouts/partials/, you will find all the partials defined for this theme. If you need to overwrite something, don\u0026rsquo;t change the code directly. Instead follow this page. You\u0026rsquo;d create a new partial in the layouts/partials folder of your local project. This partial will have the priority.\nThis theme defines the following partials :\n header: the header of the content page (contains the breadcrumbs). Not meant to be overwritten custom-header: custom headers in page. Meant to be overwritten when adding CSS imports. Don\u0026rsquo;t forget to include style HTML tag directive in your file footer: the footer of the content page (contains the arrows). Not meant to be overwritten custom-footer: custom footer in page. Meant to be overwritten when adding Javacript. Don\u0026rsquo;t forget to include javascript HTML tag directive in your file favicon: the favicon logo: the logo, on top left hand corner. meta: HTML meta tags, if you want to change default behavior menu: left menu. Not meant to be overwritten menu-footer: footer of the the left menu search: search box toc: table of contents  Change the logo Create a new file in layouts/partials/ named logo.html. Then write any HTML you want. You could use an img HTML tag and reference an image created under the static folder, or you could paste a SVG definition !\nThe size of the logo will adapt automatically\n\rChange the favicon If your favicon is a png, just drop off your image in your local static/images/ folder and name it favicon.png\nIf you need to change this default behavior, create a new file in layouts/partials/ named favicon.html. Then write something like this:\n\u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;/images/favicon.png\u0026#34; type=\u0026#34;image/x-icon\u0026#34; /\u0026gt; Change default colors Hugo Learn theme let you choose between 3 native color scheme variants, but feel free to add one yourself ! Default color scheme is based on Grav Learn Theme.\nRed variant [params] # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;red\u0026#34; Blue variant [params] # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;blue\u0026#34; Green variant [params] # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;green\u0026#34; \u0026lsquo;Yours‘ variant First, create a new CSS file in your local static/css folder prefixed by theme (e.g. with mine theme static/css/theme-mine.css). Copy the following content and modify colors in CSS variables.\n:root{ --MAIN-TEXT-color:#323232; /* Color of text by default */ --MAIN-TITLES-TEXT-color: #5e5e5e; /* Color of titles h2-h3-h4-h5 */ --MAIN-LINK-color:#1C90F3; /* Color of links */ --MAIN-LINK-HOVER-color:#167ad0; /* Color of hovered links */ --MAIN-ANCHOR-color: #1C90F3; /* color of anchors on titles */ --MENU-HEADER-BG-color:#1C90F3; /* Background color of menu header */ --MENU-HEADER-BORDER-color:#33a1ff; /*Color of menu header border */ --MENU-SEARCH-BG-color:#167ad0; /* Search field background color (by default borders + icons) */ --MENU-SEARCH-BOX-color: #33a1ff; /* Override search field border color */ --MENU-SEARCH-BOX-ICONS-color: #a1d2fd; /* Override search field icons color */ --MENU-SECTIONS-ACTIVE-BG-color:#20272b; /* Background color of the active section and its childs */ --MENU-SECTIONS-BG-color:#252c31; /* Background color of other sections */ --MENU-SECTIONS-LINK-color: #ccc; /* Color of links in menu */ --MENU-SECTIONS-LINK-HOVER-color: #e6e6e6; /* Color of links in menu, when hovered */ --MENU-SECTION-ACTIVE-CATEGORY-color: #777; /* Color of active category text */ --MENU-SECTION-ACTIVE-CATEGORY-BG-color: #fff; /* Color of background for the active category (only) */ --MENU-VISITED-color: #33a1ff; /* Color of \u0026#39;page visited\u0026#39; icons in menu */ --MENU-SECTION-HR-color: #20272b; /* Color of \u0026lt;hr\u0026gt; separator in menu */ } body { color: var(--MAIN-TEXT-color) !important; } textarea:focus, input[type=\u0026#34;email\u0026#34;]:focus, input[type=\u0026#34;number\u0026#34;]:focus, input[type=\u0026#34;password\u0026#34;]:focus, input[type=\u0026#34;search\u0026#34;]:focus, input[type=\u0026#34;tel\u0026#34;]:focus, input[type=\u0026#34;text\u0026#34;]:focus, input[type=\u0026#34;url\u0026#34;]:focus, input[type=\u0026#34;color\u0026#34;]:focus, input[type=\u0026#34;date\u0026#34;]:focus, input[type=\u0026#34;datetime\u0026#34;]:focus, input[type=\u0026#34;datetime-local\u0026#34;]:focus, input[type=\u0026#34;month\u0026#34;]:focus, input[type=\u0026#34;time\u0026#34;]:focus, input[type=\u0026#34;week\u0026#34;]:focus, select[multiple=multiple]:focus { border-color: none; box-shadow: none; } h2, h3, h4, h5 { color: var(--MAIN-TITLES-TEXT-color) !important; } a { color: var(--MAIN-LINK-color); } .anchor { color: var(--MAIN-ANCHOR-color); } a:hover { color: var(--MAIN-LINK-HOVER-color); } #sidebar ul li.visited \u0026gt; a .read-icon { color: var(--MENU-VISITED-color); } #body a.highlight:after { display: block; content: \u0026#34;\u0026#34;; height: 1px; width: 0%; -webkit-transition: width 0.5s ease; -moz-transition: width 0.5s ease; -ms-transition: width 0.5s ease; transition: width 0.5s ease; background-color: var(--MAIN-LINK-HOVER-color); } #sidebar { background-color: var(--MENU-SECTIONS-BG-color); } #sidebar #header-wrapper { background: var(--MENU-HEADER-BG-color); color: var(--MENU-SEARCH-BOX-color); border-color: var(--MENU-HEADER-BORDER-color); } #sidebar .searchbox { border-color: var(--MENU-SEARCH-BOX-color); background: var(--MENU-SEARCH-BG-color); } #sidebar ul.topics \u0026gt; li.parent, #sidebar ul.topics \u0026gt; li.active { background: var(--MENU-SECTIONS-ACTIVE-BG-color); } #sidebar .searchbox * { color: var(--MENU-SEARCH-BOX-ICONS-color); } #sidebar a { color: var(--MENU-SECTIONS-LINK-color); } #sidebar a:hover { color: var(--MENU-SECTIONS-LINK-HOVER-color); } #sidebar ul li.active \u0026gt; a { background: var(--MENU-SECTION-ACTIVE-CATEGORY-BG-color); color: var(--MENU-SECTION-ACTIVE-CATEGORY-color) !important; } #sidebar hr { border-color: var(--MENU-SECTION-HR-color); } Then, set the themeVariant value with the name of your custom theme file. That\u0026rsquo;s it !\n[params] # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;mine\u0026#34; "
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/wordpress/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Install WordPress from Portainer on Ubuntu 20.04 LTS EC2 Instance This is a step by step tutorial on how to install WordPress Docker image using Portainer.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/wordpress/_wordpress/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "What is WordPress? WordPress is an open source software, which is widely used to create a beautiful website, blog, or app. To learn more about Wordpress.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/_wprpi/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "What is WordPress? WordPress is an open source software, which is widely used to create a beautiful website, blog, or app. To learn more about Wordpress.\nWe will install Wordpress as a Docker image using Portainer. The most interesting part is we will install WP as a stack including MySQL database using Docker-Compose stack.\nversion: '2'\rservices:\rdb:\rimage: mysql/mysql-server:latest\rvolumes:\r- db_data:/var/lib/mysql\rrestart: always\renvironment:\rMYSQL_ROOT_PASSWORD: ${MYSQL_DATABASE_PASSWORD}\rMYSQL_DATABASE: wordpress\rMYSQL_USER: wordpress\rMYSQL_PASSWORD: wordpress\rwordpress:\rimage: wordpress:latest\rports:\r- 8095:80\rrestart: always\renvironment:\rWORDPRESS_DB_HOST: db:3306\rWORDPRESS_DB_USER: wordpress\rWORDPRESS_DB_PASSWORD: wordpress\rvolumes:\rdb_data:\r"
},
{
	"uri": "https://omar2cloud.github.io/rasp/pi/_grafana/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "What is Grafana? Grafana is an open source visualization, analytics and alerting platform. It turns time series database data into well designed graphs for visualizations on dashboards.\nTo learn more about Grafana.\nPart 3: Install Grafana: 1- Installing Grafana from the APT repository allows it to automatically get updated when apt-get update is ran. Let\u0026rsquo;s install the latest OSS release:\nsudo apt-get install -y apt-transport-https\rsudo apt-get install -y software-properties-common wget\rwget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -\r\rThe installation part of this tutorial is obtained from Grafana\u0026rsquo;s official website.\n\r2- Let\u0026rsquo;s add this repository for stable releases:\necho \u0026quot;deb https://packages.grafana.com/oss/deb stable main\u0026quot; | sudo tee -a /etc/apt/sources.list.d/grafana.list\r3- Let\u0026rsquo;s run an update and install Grafana:\nsudo apt-get update\rsudo apt-get install grafana\r2- Let\u0026rsquo;s configure Grafana to start at boot and start the service:\nsudo systemctl daemon-reload\rsudo systemctl enable grafana-server\rsudo systemctl start grafana-server\r3- Verify the status of Grafana:\nsudo systemctl status grafana-server\r4- We have installed and configured Grafana service, let\u0026rsquo;s open up a browser and type in localhost:3000 or http://your_device_IP:3000\nThe default username is admin and the password is admin as well. We will be prompted to change the temporary password.\n5- Let\u0026rsquo;s navigate to Configuration and choose Data Sources. From Configuration, click on Add data source and select InfluxDB as our time series database.\n6- Now, we need to define the settings for our selected InfluxDB data source.\nA. Query Langauge: B. URL: \u0026lt;localhost:8086\u0026gt; or \u0026lt;RPI-4 IP address\u0026gt; C. Orginzation Name: D. Bucket Name:\rE. Token:\r To locate the token, we need to navigate back to InfluxDB. From Data, select Tokens. The\nConclusion: We have successfully installed InfluxDB 2.0 on our RPY-4 running Ubuntu 21.04. On the next tutorial, we will install Telegraf agent to collect metrics.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/pi/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Telegraf, InfluxDB and Grafana on Raspberry Pi 4 Running Ubuntu 21.04 The Telegraf, InfluxDB and Grafana is abbreviated as TIG stack. It is a platform of three open source tools built to collect, store, graph and alert time series data.\nIn the following tutorials, we will install Telegraf agent to collect metrics from our Raspberry Pi 4\u0026rsquo;s resources, install InfluxDB 2.0 database to store the collected time series and finally install Grafana data visualization platform to graph, monitor and analyze our metrics.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/pi/_influxdb/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "What is InfluxDB? InfluxDB is a time series database. It\u0026rsquo;s engineered to build IoT, analytics and monitoring. It is built to ingest millions of data points per second produced by sensors, systems, applications and infrastructure. By the end of this tutorial, we would have installed InfluxDB to monitor our Raspberry Pi 4 system\u0026rsquo;s metrics such as CPU, Disk space and memory.\nTo learn more about InfluxDB.\nPart 1: Install InfluxDB 2.0 1- We will install InfluxDB as a service with systemd. Please, refer to InfluxDB download portal for the latest version of InfluxDB. The latest version as of writing of this tutorial is v2.0.7.\nDownload and install InfluxDB v2.0.7:\nsudo wget https://dl.influxdata.com/influxdb/releases/influxdb2-2.0.7-arm64.deb\rsudo dpkg -i influxdb2-2.0.7-arm64.deb\r2- Start the InfluxDB service:\nsudo systemctl start influxdb\r3- Restart the RPI-4 and verify the status of InfluxDB:\nsudo systemctl status influxdb\rNote: to exit out of the status check, press Control+C\nIt\u0026rsquo;s important to verify that enabled is stated during InfluxDB status check. It forces InfluxDB to start automatically on startup/boot up. If the service is not enabled, you could enable it by the following command: sudo systemctl enable influxdb\n\r4- Let\u0026rsquo;s start InfluxDB daemon:\ninfluxd\r5- Now, let\u0026rsquo;s set up InfluxDB through the influx CLI. Let\u0026rsquo;s navigate back to the terminal and type in the below command. You should be prompted to enter a username, password, organization name, bucket name and a retention period. The bucket is equivalent to a database name. The retention period for the bucket is valid in nanoseconds, microsecond, milliseconds\u0026hellip;weeks. The retention period could also be empty for infinite retention period.\ninflux setup\rConclusion: We have successfully installed InfluxDB 2.0 on our RPI-4 running Ubuntu 21.04. On the next tutorial, we will install Telegraf agent to collect metrics.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/pi/_telegraf/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "What is Telegraf? Telegraf is an open source server agent written in Go. It is built to collect metrics from a wide array of inputs and write them into a wide array of outputs.\nTo learn more about Telegraf.\nPart 2: Install Telegraf: 1- We will install the latest version of Telegraf using the ap-get package manager. Firstly, let\u0026rsquo;s add the InfluxData repository by running the following commands (you could copy and paste all commands at once on the terminal, or one line at a time):\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -\rsource /etc/lsb-release\recho \u0026quot;deb https://repos.influxdata.com/${DISTRIB_ID,,} ${DISTRIB_CODENAME} stable\u0026quot; | sudo tee /etc/apt/sources.list.d/influxdb.list\r2- Now, we are ready to install and start the Telegraf agent service:\nsudo apt-get update sudo apt-get install telegraf\rsudo systemctl enable telegraf\rsudo systemctl start telegraf\rsudo systemctl status telegraf\r3- We will create a Telegraf configuration file as follows:\nsudo service influxdb status\r4- We ?? Do we need itt?\ninfluxd\r5- Now, we are ready to test the communication between Telegraf and InfluxDB using a token, which we will obtain from InfluxDB. Navigate to InfluxDB\u0026hellip;\nA. Manually: copy the token from InfluxDB UI and past it into Telegraf configuration file shown below.\nB. Programmatically:\n store credentials in the environment variable by running the following commands on terminal:  export INFLUX_HOST=http://192.168.1.117:8086\rexport INFLUX_TOKEN=YourInfluxDBAuthToken\rexport INFLUX_ORG=omar\rAdd the environment variable to the telegraf configuration file as shown below:  [[outputs.influxdb_v2]]\rurls = [\u0026quot;$INFLUX_HOST\u0026quot;]\rtoken = \u0026quot;$INFLUX_TOKEN\u0026quot;\rorganization = \u0026quot;$INFLUX_ORG\u0026quot;\rbucket = \u0026quot;waterpi\u0026quot;\r6- Finally, we need to configure our Telegraf configuration file so Telegraf outputs to InfluxDB v2. Notice, the configuration for InfluxDB differs from InfluxDB v2. The Telegraf configuration is saved in\n\r[agent]\r## Default data collection interval for all inputs\rinterval = \u0026quot;10s\u0026quot;\r## Rounds collection interval to 'interval'\r## ie, if interval=\u0026quot;10s\u0026quot; then always collect on :00, :10, :20, etc.\rround_interval = true\r## Telegraf will send metrics to outputs in batches of at most\r## metric_batch_size metrics.\r## This controls the size of writes that Telegraf sends to output plugins.\rmetric_batch_size = 1000\r## For failed writes, telegraf will cache metric_buffer_limit metrics for each\r## output, and will flush this buffer on a successful write. Oldest metrics\r## are dropped first when this buffer fills.\r## This buffer only fills when writes fail to output plugin(s).\rmetric_buffer_limit = 10000\r## Collection jitter is used to jitter the collection by a random amount.\r## Each plugin will sleep for a random time within jitter before collecting.\r## This can be used to avoid many plugins querying things like sysfs at the\r## same time, which can have a measurable effect on the system.\rcollection_jitter = \u0026quot;0s\u0026quot;\r## Default flushing interval for all outputs. Maximum flush_interval will be\r## flush_interval + flush_jitter\rflush_interval = \u0026quot;10s\u0026quot;\r## Jitter the flush interval by a random amount. This is primarily to avoid\r## large write spikes for users running a large number of telegraf instances.\r## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\rflush_jitter = \u0026quot;0s\u0026quot;\r## By default or when set to \u0026quot;0s\u0026quot;, precision will be set to the same\r## timestamp order as the collection interval, with the maximum being 1s.\r## ie, when interval = \u0026quot;10s\u0026quot;, precision will be \u0026quot;1s\u0026quot;\r## when interval = \u0026quot;250ms\u0026quot;, precision will be \u0026quot;1ms\u0026quot;\r## Precision will NOT be used for service inputs. It is up to each individual\r## service input to set the timestamp at the appropriate precision.\r## Valid time units are \u0026quot;ns\u0026quot;, \u0026quot;us\u0026quot; (or \u0026quot;µs\u0026quot;), \u0026quot;ms\u0026quot;, \u0026quot;s\u0026quot;.\rprecision = \u0026quot;\u0026quot;\r## Logging configuration:\r## Run telegraf with debug log messages.\rdebug = false\r## Run telegraf in quiet mode (error log messages only).\rquiet = false\r## Specify the log file name. The empty string means to log to stderr.\rlogfile = \u0026quot;\u0026quot;\r## Override default hostname, if empty use os.Hostname()\rhostname = \u0026quot;\u0026quot;\r## If set to true, do no set the \u0026quot;host\u0026quot; tag in the telegraf agent.\romit_hostname = false\r[[outputs.influxdb_v2]]\t## The URLs of the InfluxDB cluster nodes.\r##\r## Multiple URLs can be specified for a single cluster, only ONE of the\r## urls will be written to each interval.\r## urls exp: http://127.0.0.1:8086\rurls = [\u0026quot;$INFLUX_HOST\u0026quot;]\r## Token for authentication.\rtoken = \u0026quot;$INFLUX_TOKEN\u0026quot;\r## Organization is the name of the organization you wish to write to; must exist.\rorganization = \u0026quot;$INFLUX_ORG\u0026quot;\r## Destination bucket to write into.\rbucket = \u0026quot;waterpi\u0026quot;\r[[inputs.cpu]]\r## Whether to report per-cpu stats or not\rpercpu = true\r## Whether to report total system cpu stats or not\rtotalcpu = true\r## If true, collect raw CPU time metrics.\rcollect_cpu_time = false\r## If true, compute and report the sum of all non-idle CPU states.\rreport_active = false\r[[inputs.disk]]\r## By default stats will be gathered for all mount points.\r## Set mount_points will restrict the stats to only the specified mount points.\r# mount_points = [\u0026quot;/\u0026quot;]\r## Ignore mount points by filesystem type.\rignore_fs = [\u0026quot;tmpfs\u0026quot;, \u0026quot;devtmpfs\u0026quot;, \u0026quot;devfs\u0026quot;, \u0026quot;overlay\u0026quot;, \u0026quot;aufs\u0026quot;, \u0026quot;squashfs\u0026quot;]\r[[inputs.diskio]]\r[[inputs.mem]]\r[[inputs.net]]\r[[inputs.processes]]\r[[inputs.swap]]\r[[inputs.system]]\rConclusion: We have successfully installed Telegraf on our RPI-4 running Ubuntu 21.04. On the next tutorial, we will install Grafana and configure a dashboard to view, graph and analyze our collected metrics.\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/attachments/",
	"title": "Attachments",
	"tags": [],
	"description": "The Attachments shortcode displays a list of files attached to a page.",
	"content": "The Attachments shortcode displays a list of files attached to a page.\n\r\rAttachments\r\r\rBachGavotteShort.mp3\r\r(357 kb)\r\r\rCarroll_AliceAuPaysDesMerveilles.pdf\r\r(175 kb)\r\r\radivorciarsetoca00cape.pdf\r\r(361 kb)\r\r\rhugo.png\r\r(17 kb)\r\r\rmovieselectricsheep-flock-244-32500-2.mp4\r\r(340 kb)\r\r\r\rUsage The shortcurt lists files found in a specific folder. Currently, it support two implementations for pages\n  If your page is a markdown file, attachements must be placed in a folder named like your page and ending with .files.\n  content  _index.md page.files  attachment.pdf   page.md       If your page is a folder, attachements must be placed in a nested \u0026lsquo;files\u0026rsquo; folder.\n  content  _index.md page  index.md files  attachment.pdf           Be aware that if you use a multilingual website, you will need to have as many folders as languages.\nThat\u0026rsquo;s all!\nParameters    Parameter Default Description     title \u0026ldquo;Attachments\u0026rdquo; List\u0026rsquo;s title   style \u0026quot;\u0026quot; Choose between \u0026ldquo;orange\u0026rdquo;, \u0026ldquo;grey\u0026rdquo;, \u0026ldquo;blue\u0026rdquo; and \u0026ldquo;green\u0026rdquo; for nice style   pattern \u0026ldquo;.*\u0026rdquo; A regular expressions, used to filter the attachments by file name. The pattern parameter value must be regular expressions.    For example:\n To match a file suffix of \u0026lsquo;jpg\u0026rsquo;, use *.jpg (not *.jpg). To match file names ending in \u0026lsquo;jpg\u0026rsquo; or \u0026lsquo;png\u0026rsquo;, use .*(jpg|png)  Examples List of attachments ending in pdf or mp4 {{%attachments title=\u0026quot;Related files\u0026quot; pattern=\u0026quot;.*(pdf|mp4)\u0026quot;/%}}\r renders as\n\r\rRelated files\r\r\rCarroll_AliceAuPaysDesMerveilles.pdf\r\r(175 kb)\r\r\radivorciarsetoca00cape.pdf\r\r(361 kb)\r\r\rmovieselectricsheep-flock-244-32500-2.mp4\r\r(340 kb)\r\r\r\rColored styled box {{%attachments style=\u0026quot;orange\u0026quot; /%}}\r renders as\n\r\rAttachments\r\r\rBachGavotteShort.mp3\r\r(357 kb)\r\r\rCarroll_AliceAuPaysDesMerveilles.pdf\r\r(175 kb)\r\r\radivorciarsetoca00cape.pdf\r\r(361 kb)\r\r\rhugo.png\r\r(17 kb)\r\r\rmovieselectricsheep-flock-244-32500-2.mp4\r\r(340 kb)\r\r\r\r{{%attachments style=\u0026quot;grey\u0026quot; /%}}\r renders as\n\r\rAttachments\r\r\rBachGavotteShort.mp3\r\r(357 kb)\r\r\rCarroll_AliceAuPaysDesMerveilles.pdf\r\r(175 kb)\r\r\radivorciarsetoca00cape.pdf\r\r(361 kb)\r\r\rhugo.png\r\r(17 kb)\r\r\rmovieselectricsheep-flock-244-32500-2.mp4\r\r(340 kb)\r\r\r\r{{%attachments style=\u0026quot;blue\u0026quot; /%}}\r renders as\n\r\rAttachments\r\r\rBachGavotteShort.mp3\r\r(357 kb)\r\r\rCarroll_AliceAuPaysDesMerveilles.pdf\r\r(175 kb)\r\r\radivorciarsetoca00cape.pdf\r\r(361 kb)\r\r\rhugo.png\r\r(17 kb)\r\r\rmovieselectricsheep-flock-244-32500-2.mp4\r\r(340 kb)\r\r\r\r{{%attachments style=\u0026quot;green\u0026quot; /%}}\r renders as\n\r\rAttachments\r\r\rBachGavotteShort.mp3\r\r(357 kb)\r\r\rCarroll_AliceAuPaysDesMerveilles.pdf\r\r(175 kb)\r\r\radivorciarsetoca00cape.pdf\r\r(361 kb)\r\r\rhugo.png\r\r(17 kb)\r\r\rmovieselectricsheep-flock-244-32500-2.mp4\r\r(340 kb)\r\r\r\r"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/button/",
	"title": "Button",
	"tags": [],
	"description": "Nice buttons on your page.",
	"content": "A button is a just a clickable button with optional icon.\n{{% button href=\u0026quot;https://getgrav.org/\u0026quot; %}}Get Grav{{% /button %}}\r{{% button href=\u0026quot;https://getgrav.org/\u0026quot; icon=\u0026quot;fas fa-download\u0026quot; %}}Get Grav with icon{{% /button %}}\r{{% button href=\u0026quot;https://getgrav.org/\u0026quot; icon=\u0026quot;fas fa-download\u0026quot; icon-position=\u0026quot;right\u0026quot; %}}Get Grav with icon right{{% /button %}}\r\rGet Grav\r\r\rGet Grav with icon\r\rGet Grav with icon right\r\r\r"
},
{
	"uri": "https://omar2cloud.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/",
	"title": "Children",
	"tags": [],
	"description": "List the child pages of a page",
	"content": "Use the children shortcode to list the child pages of a page and the further descendants (children\u0026rsquo;s children). By default, the shortcode displays links to the child pages.\nUsage    Parameter Default Description     page current Specify the page name (section name) to display children for   style \u0026ldquo;li\u0026rdquo; Choose the style used to display descendants. It could be any HTML tag name   showhidden \u0026ldquo;false\u0026rdquo; When true, child pages hidden from the menu will be displayed   description \u0026ldquo;false\u0026rdquo; Allows you to include a short text under each page in the list. When no description exists for the page, children shortcode takes the first 70 words of your content. Read more info about summaries on gohugo.io   depth 1 Enter a number to specify the depth of descendants to display. For example, if the value is 2, the shortcode will display 2 levels of child pages. Tips: set 999 to get all descendants   sort none Sort children by Weight - to sort on menu order, Name - to sort alphabetically on menu label, Identifier - to sort alphabetically on identifier set in frontmatter, and URL - to sort by URL    Demo {{% children %}}\r \rpage 1\r\r\rpage 2\r\r\rpage 3\r\r\rpage test\r\r\rpage test\r\r\r{{% children description=\u0026quot;true\u0026quot; %}}\r "
},
{
	"uri": "https://omar2cloud.github.io/credits/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": "Contributors Thanks to them for making Open Source Software a better place !\nAnd a special thanks to @vjeantet for his work on docdock, a fork of hugo-theme-learn. v2.0.0 of this theme is inspired by his work.\nPackages and libraries  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support  Tooling  Netlify - Continuous deployement and hosting of this documentation Hugo  "
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/expand/",
	"title": "Expand",
	"tags": [],
	"description": "Displays an expandable/collapsible section of text on your page",
	"content": "The Expand shortcode displays an expandable/collapsible section of text on your page. Here is an example\n\r\rExpand me...\r\r\rLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\r Usage this shortcode takes exactly one optional parameter to define the text that appears next to the expand/collapse icon. (default is \u0026ldquo;Expand me\u0026hellip;\u0026quot;)\n{{%expand \u0026quot;Is this learn theme rocks ?\u0026quot; %}}Yes !.{{% /expand%}}\r \r\rIs this learn theme rocks ?\r\r\rYes !\r\r Demo {{%expand%}}\rLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod\rtempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,\rquis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo\rconsequat. Duis aute irure dolor in reprehenderit in voluptate velit esse\rcillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non\rproident, sunt in culpa qui officia deserunt mollit anim id est laborum.\r{{% /expand%}}\r \r\rExpand me...\r\r\rLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\r "
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/",
	"title": "Learn with Omar",
	"tags": [],
	"description": "",
	"content": "/*this page is a copy of the original index */\nOmar\u0026rsquo;s Tutorials There are thousdands and thousands\nLet\u0026rsquo;s take this journey and learn together.\nThis theme is a partial porting of the Learn theme of Grav,\nLearn theme works with a page tree structure to organize content : All contents are pages, which belong to other pages. read more about this\n\rMain features  Automatic Search Multilingual mode Unlimited menu levels Automatic next/prev buttons to navigate through menu entries Image resizing, shadow\u0026hellip; Attachments files List child pages Mermaid diagram (flowchart, sequence, gantt) Customizable look and feel and themes variants Buttons, Tip/Note/Info/Warning boxes, Expand  Contribute to this documentation Feel free to update this content, just click the Edit this page link displayed on top right of each page, and pullrequest it\nYour modification will be deployed automatically when merged.\n\rDocumentation website This current documentation has been statically generated with Hugo with a simple command : hugo -t hugo-theme-learn \u0026ndash; source code is available here at GitHub\nAutomatically published and hosted thanks to Netlify. Read more about Automated HUGO deployments with Netlify\n\r"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/mermaid/",
	"title": "Mermaid",
	"tags": [],
	"description": "Generation of diagram and flowchart from text in a similar manner as markdown",
	"content": "Mermaid is a library helping you to generate diagram and flowcharts from text, in a similar manner as Markdown.\nJust insert your mermaid code in the mermaid shortcode and that\u0026rsquo;s it.\nFlowchart example {{\u0026lt;mermaid align=\u0026quot;left\u0026quot;\u0026gt;}}\rgraph LR;\rA[Hard edge] --\u0026gt;|Link text| B(Round edge)\rB --\u0026gt; C{Decision}\rC --\u0026gt;|One| D[Result one]\rC --\u0026gt;|Two| E[Result two]\r{{\u0026lt; /mermaid \u0026gt;}}\r renders as\ngraph LR;\rA[Hard edge] --|Link text| B(Round edge)\rB -- C{Decision}\rC --|One| D[Result one]\rC --|Two| E[Result two]\r\rSequence example {{\u0026lt; mermaid \u0026gt;}}\rsequenceDiagram\rparticipant Alice\rparticipant Bob\rAlice-\u0026gt;\u0026gt;John: Hello John, how are you?\rloop Healthcheck\rJohn-\u0026gt;John: Fight against hypochondria\rend\rNote right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail...\rJohn--\u0026gt;Alice: Great!\rJohn-\u0026gt;Bob: How about you?\rBob--\u0026gt;John: Jolly good!\r{{\u0026lt; /mermaid \u0026gt;}}\r renders as\nsequenceDiagram\rparticipant Alice\rparticipant Bob\rAlice-John: Hello John, how are you?\rloop Healthcheck\rJohn-John: Fight against hypochondria\rend\rNote right of John: Rational thoughts prevail...\rJohn--Alice: Great!\rJohn-Bob: How about you?\rBob--John: Jolly good!\r\rGANTT Example {{\u0026lt; mermaid \u0026gt;}}\rgantt\rdateFormat YYYY-MM-DD\rtitle Adding GANTT diagram functionality to mermaid\rsection A section\rCompleted task :done, des1, 2014-01-06,2014-01-08\rActive task :active, des2, 2014-01-09, 3d\rFuture task : des3, after des2, 5d\rFuture task2 : des4, after des3, 5d\rsection Critical tasks\rCompleted task in the critical line :crit, done, 2014-01-06,24h\rImplement parser and jison :crit, done, after des1, 2d\rCreate tests for parser :crit, active, 3d\rFuture task in critical line :crit, 5d\rCreate tests for renderer :2d\rAdd to mermaid :1d\r{{\u0026lt; /mermaid \u0026gt;}}\r renders as\ngantt\rdateFormat YYYY-MM-DD\rtitle Adding GANTT diagram functionality to mermaid\rsection A section\rCompleted task :done, des1, 2014-01-06,2014-01-08\rActive task :active, des2, 2014-01-09, 3d\rFuture task : des3, after des2, 5d\rFuture task2 : des4, after des3, 5d\rsection Critical tasks\rCompleted task in the critical line :crit, done, 2014-01-06,24h\rImplement parser and jison :crit, done, after des1, 2d\rCreate tests for parser :crit, active, 3d\rFuture task in critical line :crit, 5d\rCreate tests for renderer :2d\rAdd to mermaid :1d\r\rClass example {{\u0026lt; mermaid \u0026gt;}}\rclassDiagram\rClass01 \u0026lt;|-- AveryLongClass : Cool\rClass03 *-- Class04\rClass05 o-- Class06\rClass07 .. Class08\rClass09 --\u0026gt; C2 : Where am i?\rClass09 --* C3\rClass09 --|\u0026gt; Class07\rClass07 : equals()\rClass07 : Object[] elementData\rClass01 : size()\rClass01 : int chimp\rClass01 : int gorilla\rClass08 \u0026lt;--\u0026gt; C2: Cool label\r{{\u0026lt; /mermaid \u0026gt;}}\r renders as\nclassDiagram\rClass01 C2 : Where am i?\rClass09 --* C3\rClass09 --| Class07\rClass07 : equals()\rClass07 : Object[] elementData\rClass01 : size()\rClass01 : int chimp\rClass01 : int gorilla\rClass08  C2: Cool label\r\rGit example {{\u0026lt; mermaid \u0026gt;}}\rgitGraph:\roptions\r{\r\u0026quot;nodeSpacing\u0026quot;: 150,\r\u0026quot;nodeRadius\u0026quot;: 10\r}\rend\rcommit\rbranch newbranch\rcheckout newbranch\rcommit\rcommit\rcheckout master\rcommit\rcommit\rmerge newbranch\r{{\u0026lt; /mermaid\u0026gt;}}\r renders as\ngitGraph:\roptions\r{\r\"nodeSpacing\": 150,\r\"nodeRadius\": 10\r}\rend\rcommit\rbranch newbranch\rcheckout newbranch\rcommit\rcommit\rcheckout master\rcommit\rcommit\rmerge newbranch\r\rState Diagrams {{\u0026lt; mermaid \u0026gt;}}\rstateDiagram-v2\ropen: Open Door\rclosed: Closed Door\rlocked: Locked Door\ropen --\u0026gt; closed: Close\rclosed --\u0026gt; locked: Lock\rlocked --\u0026gt; closed: Unlock\rclosed --\u0026gt; open: Open\r{{\u0026lt; /mermaid \u0026gt;}}\r renders as\nstateDiagram-v2\ropen: Open Door\rclosed: Closed Door\rlocked: Locked Door\ropen -- closed: Close\rclosed -- locked: Lock\rlocked -- closed: Unlock\rclosed -- open: Open\r\r"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/notice/",
	"title": "Notice",
	"tags": [],
	"description": "Disclaimers to help you structure your page",
	"content": "The notice shortcode shows 4 types of disclaimers to help you structure your page.\nNote {{% notice note %}}\rA notice disclaimer\r{{% /notice %}}\rrenders as\nA notice disclaimer\n\rInfo {{% notice info %}}\rAn information disclaimer\r{{% /notice %}}\rrenders as\nAn information disclaimer\n\rTip {{% notice tip %}}\rA tip disclaimer\r{{% /notice %}}\rrenders as\nA tip disclaimer\n\rWarning {{% notice warning %}}\rA warning disclaimer\r{{% /notice %}}\rrenders as\nA warning disclaimer\n\r"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-1/",
	"title": "page 1",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-1/children-1-1/",
	"title": "page 1-1",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-1/children-1-1/children-1-1-1/",
	"title": "page 1-1-1",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-1/children-1-1/children-1-1-1/children-1-1-1-1/",
	"title": "page 1-1-1-1",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-1/children-1-1/children-1-1-1/children-1-1-1-1/children-1-1-1-1-1/",
	"title": "page 1-1-1-1-1",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-2/",
	"title": "page 2",
	"tags": [],
	"description": "",
	"content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-3/",
	"title": "page 3",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page, not displayed in the menu\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-4/",
	"title": "page 4",
	"tags": [],
	"description": "This is a demo child page",
	"content": "This is a demo child page, not displayed in the menu\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/test/",
	"title": "page test",
	"tags": [],
	"description": "This is a page test",
	"content": "This is a test demo child page\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/test.fr/",
	"title": "page test",
	"tags": [],
	"description": "Ceci est une page test",
	"content": "Ceci est une page de demo\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-2/test3/",
	"title": "page test 3",
	"tags": [],
	"description": "This is a page test",
	"content": "This is a test 3 demo child page\n"
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/children/children-2/test3.fr/",
	"title": "page test 3",
	"tags": [],
	"description": "Ceci est une page test",
	"content": "Ceci est une page de demo test 3\n"
},
{
	"uri": "https://omar2cloud.github.io/showcase/",
	"title": "Showcase",
	"tags": [],
	"description": "",
	"content": "TAT by OVH Tshark.dev by Ross Jacobs inteliver by Amir Lavasani "
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/siteparam/",
	"title": "Site param",
	"tags": [],
	"description": "Get value of site params variables in your page.",
	"content": "siteparam shortcode is used to help you print values of site params.\nFor instance, in this current site, the editURL variable is used in config.toml\n[params] editURL = \u0026#34;https://github.com/matcornic/hugo-theme-learn/edit/master/exampleSite/content/\u0026#34; Use the siteparam shortcode to display its value.\n`editURL` Value : {{% siteparam \u0026quot;editURL\u0026quot; %}}\ris displayed as\neditURL Value :\n"
}]