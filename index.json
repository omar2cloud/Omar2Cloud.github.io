[
{
	"uri": "https://omar2cloud.github.io/aws/",
	"title": "AWS Tutorials",
	"tags": [],
	"description": "",
	"content": "AWS Tutorials Multiple AWS Tutorials "
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/domain/domain/",
	"title": "Create a Free Domain Name",
	"tags": [],
	"description": "",
	"content": "Tutorial Scenario:  Register a free domain at Freenom Signup for a free account at Cloudflare Add a site to Cloudflare  Step 1: Register a free domain at Freenom:  Navigate to Freenom and create a free account. Select Register a New Domain as show below.  Check the availability of a domain name of your choice. In my case, it\u0026rsquo;s mytunnel. Then, select the free domain TDL  Choose the Period for your selected domain. It ranges from 1 to 12 months. Then, click Continue.  Congratulations. You have successfully obtained your first free domain.\nStep 2: Signup for a free account at Cloudflare:   Navigate to Cloudflare and signup for a free account.\n  Click on Add a Site and type in the name of your new domain.   Select Get started for free and hit Continue.  We will add records later (CNAME). Click Continue.   We will copy our Cloudflare\u0026rsquo;s nameservers and paste them in our nameservers at Freenom as shown below.   Navigate back to Freenom to replace the nameservers. Click on Change Nameservers.\n  Paste the copied two Cloudflare\u0026rsquo;s nameservers and delete any other nameservers. Click Change Nameservers.  Navigate back to Cloudflare site and click on Finish later.  To check the status of nameservers change, click on Check nameservers.  Please wait until you receive an email stating Status Active. It will take several hours to complete the nameservers change.  Conclusion: By the end of this tutorial, we have successfully created registered a free domain, created a Cloudflare site and replaced the Freenom\u0026rsquo;s nameservers by the new Cloudflare\u0026rsquo;s nameservers.\n"
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/domain/",
	"title": "Domain Registration",
	"tags": [],
	"description": "",
	"content": "Free Domain Registration at Freenom Freenom is a free and paid domain provider. It offers free domains for 1 to 12 months with an option for unlimited renewal. It\u0026rsquo;s a very appealing service which doe snot require any payment method to obtain a domain with a Top Level Domain (TLD) of the following: .TK / .ML / .GA / .CF / .GQ.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/myrasp/",
	"title": "My First Raspberry Pi",
	"tags": [],
	"description": "",
	"content": "What is Raspberry Pi (RPI)? The Raspberry Pi is a small single-board computer capable of doing almost everything that an ordinary Linux-based computer can do. Once it connects to an HDMI display, a keyboard, and a mouse, we have have a fully functional computer for a pice of $35 (or even way less for the Raspberry Pi Zero). The Raspberry Pi is widely known and used for home lab environments, where anyone can experiment safely. To learn more about Raspberry Pi.\nNote that we will need to purchase a microSD card with a capacity of at least 8GB. We will also need a power supply to power the Raspberry Pi via USB-C for Raspberry Pi 4.\nRaspberry Pi 4 There are several models of RPI, and for most people Raspberry Pi 4 Model B is the one to choose. Raspberry Pi 4 Model B is the newest, fastest, and easiest to use.\nRaspberry Pi 4 comes with 2GB, 4GB, or 8GB of RAM. For most educational purposes and hobbyist projects, and for use as a desktop computer, 2GB is enough.\nRaspberry Zero Raspberry Pi Zero, Raspberry Pi Zero W, and Raspberry Pi Zero WH are smaller and require less power, so they’re useful for portable projects such as robots. It’s generally easier to start a project with Raspberry Pi 4, and to move to Raspberry Pi Zero when you have a working prototype that a smaller Raspberry Pi would be useful for. I\u0026rsquo;m using one of these amazing gems as a Twitter Bot. I will definitely put a tutorial together about how to create and host a Twitter Bot on a RPI Zero soon.\nUnpacking and Assembling My First Raspberry Pi 4 I purchased the latest Raspberry Pi 4 from Amazon which rocks a 1.5GHz 64-bit quad-core (ARMv8) CPU (4GB of RAM)—though you can step up to 8GB of RAM for a bit more money. Moreover, 2.4 Ghz and 5.0 GHz IEEE 802.11ac wireless, Bluetooth 5.0, BLE. It is very powerful for pretty little thing.\nI have had the starter kit with my RPI 4 which includes a premium case with integrated fan mount, 3.5A USB-C power supply with on/off power switch, set of heat sinks, micro HDMI to HDMI cable and 32 GB microSD card.\nThis is my RPI 4 after being fully assembled.\nWe will need a keyboard, a monitor and a mouse for the initial setup. However, we might not need them down the road. It depends on the project and how we are going to access the RPI 4.\nInstall an Operating System on the microSD Card The RPI operating system (previously called Raspbian) is the Foundation\u0026rsquo;s official OS for the Pi based on Debian Linux. The Pi Foundation also offers a Raspberry Pi Imager for a quick and easy way to install Raspberry PI OS and other operating systems to our microSD card. It\u0026rsquo;s available for Windows, MacOS and Ubuntu. Let\u0026rsquo;s download the Raspberry Pi Imager.\nFor our upcoming projects, I would suggest to install 64-bit Ubuntu Desktop 20.10 on the RPI instead of the 32-bit Raspbian operating system.\nAfter downloading the Raspberry Pi Imager, we will click on \u0026ldquo;CHOOSE OS\u0026rdquo; to select the Ubuntu Desktop 20.10 on our microSD card.\nIf the downloading process to the microSD is completed successfully, we shall see the below message.\nConclusion: Now, we have successfully created an image of Ubuntu Desktop 20.10. Let\u0026rsquo;s insert this MicroSD card into our RPI 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/spin-ubuntu-instance/",
	"title": "Part 1 - Ubuntu Instance",
	"tags": [],
	"description": "",
	"content": "Spinning up Ubuntu 20.04 LTS EC2 The following is a step by step tutorial on how to spin a Ubuntu 20.04 EC2 instance and how to SSH into our Ubuntu instance.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/portainer/",
	"title": "Part 2 - Portainer on Ubuntu",
	"tags": [],
	"description": "",
	"content": "Install Portainer on Ubuntu 20.04 LTS Instance This is a step by step tutorial on how to install Portainer Docker management tool on Ubuntu 20.04 on EC2 instance. Portainer is a great way to learn Docker and Containers for beginners.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/wordpress/",
	"title": "Part 3 - WordPress Docker Image",
	"tags": [],
	"description": "",
	"content": "Install WordPress from Portainer on Ubuntu 20.04 LTS EC2 Instance This is a step by step tutorial on how to install WordPress Docker image using Portainer.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/spin-ubuntu-instance/ec2-ubuntu/",
	"title": "Spin up Ubuntu Instance",
	"tags": [],
	"description": "",
	"content": "What is Ubuntu 20.04 LTS? Ubuntu is a complete Linux operating system. It is available with both community and professional support. The Ubuntu community should be available free of charge and people should have the freedom to customize it. To learn more about Ubuntu.\nPlease note that some parts of the images in this tutorial will be pixilated to hide all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rIn order to complete this tutorial, you will access to an AWS account. Therefore, if you don\u0026rsquo;t have an AWS account, you should sign up and create an account. AWS offers a 12-month free tier account. With this free-trier account, AWS gives you 750 hrs/monthly of EC2 computing service in a single Availability Zone and t2.micro instance. If we have one instance running 24hrs X 31 days = 744 hrs, which is still below the AWS free-tier account limit of 750 hrs/monthly. Basically, you would have one EC2 instance running for an entire year for free.\nOnce you\u0026rsquo;re logged in and on the home page, as shown below, make sure you select the region to where you want to spin your instance. Us East-1 (N. Virginia) is selected in my case. Then, click on EC2 under the Compute section.\nClick on Launch Instance On EC2 Dashboard\nChoose AMI (Amazon Machine Image) Ubuntu 20.04 LTS. You might need to scroll down to find Ubuntu 20.04 LTS.\nChoose Instance Type as General purpose t2.micro. This is the instance type identified by AWS for the Free-Tier account eligibility. Now click on Configure Instance Details\nOn the Configure Instance Details, we don\u0026rsquo;t need to make any changes. However, ensure that all selections are similar to what is on the below image.\nOn Add Storage screen is where we can increase the storage size of the instance. AWS permits up to 30GB for free-tier accounts. Let\u0026rsquo;s take advantage of it. I increased mine to 16 GB, you could use more if you\u0026rsquo;d like.\nIt is always a good practice and good a habit to have when it comes to adding tags. Since I have multiple Ubuntu instances, I would like to dedicate this to Portainer. You could tag your instance under any name you would like.\nConfigure Security Group is where we create security groups and specify Ports to open for the EC2 instance. Note that we will only ports that will allow us to install and run Portainer for now. We will open more ports down the road based on the ports required by the web application we will run.\nBe very careful when you open ports to the world.\nOn Create a new security group, name your security group and give it a description.\nThese are the two ports we need to open at this stage.\n   Type Port     SSH 22   Custom TCP 9000    The final stage of this process is Review Instance Launch. Once you have reviewed the instance configurations, click on Launch.\nWe will create a key pair to connect (SSH) to our EC2 instance from our local machine. Let us download the Key Pair and keep it in a secure location. Without this key, we will not be abe to connect to the EC2 instance.\nA key pair, consisting of a private key and a public key, is a set of security credentials that you use to prove your identity when connecting to an instance. Amazon EC2 stores the public key, and you store the private key. You use the private key, instead of a password, to securely access your instances. Anyone who possesses your private keys can connect to your instances, so it\u0026rsquo;s important that you store your private keys in a secure place. To read more about a key pair from AWS.\nClick on Launch Instance\nLet\u0026rsquo;s wait until the Instance State is updated to \u0026ldquo;running\u0026rdquo; status. The running status \u0026ldquo;Green\u0026rdquo; is indication that our EC2 instance is fully provisioned and ready to be connected to.\nConclusion: This concludes the first section of part one of the tutorial. We have successfully spun a Ubuntu 20.04 LTS EC2 instance. In the next section, we will learn how to SSH into the instance.\n"
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/",
	"title": "Cloudflare Tutorials",
	"tags": [],
	"description": "",
	"content": "Cloudflare Cloudflare Tunnel Cloudflare is a global network designed to make everything you connect to the Internet secure, private, fast, and reliable. Cloudflare offers a suite of services and Zero Trust Services are the services we will utilize in the following tutorials. Zero Trust Services consist of Teams, Access, Gateway and Browser Isolation.\nOur main goal is to obtain a free domain from Freenom and connect our hosted applications on a Ubuntu 20.04 LTS Raspberry Pi 4 within our local home network via a Cloudflare Tunnel to the world wide web securely without any port-forwarding complications or altering firewall.\n"
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/cloudflared/",
	"title": "Cloudflare Tutorials",
	"tags": [],
	"description": "",
	"content": "Cloudflare Tunnel In April, 2021, Cloudflare Tunnel is announced as a free service for everyone. This service creates a secure, outbound-only connection between applications hosted locally and Cloudflare by deploying a lightweight connector (Cloudflared daemon).\nBasically, with Cloudflare Tunnel, anyone can create a private link/tunnel from any locally hosted application or server to Cloudflare without a public IP address, port-forwarding or punching through a firewall. This secured Tunnel is established by running Cloudflared daemon, on the origin, which allows for a secure, outbound-only connection. All traffic, to the origin, funnels through Cloudflare network service.\n"
},
{
	"uri": "https://omar2cloud.github.io/cloudflare/cloudflared/cloudflare/",
	"title": "Create a Free Cloudflare Tunnel",
	"tags": [],
	"description": "",
	"content": "Tutorial Scenario:  Signup for a free Cloudflare for Teams. Install and authenticate cloudflared on a Raspberry Pi 4. Create a Cloudflare Tunnel. Configure the Tunnel details. Create DNS records to route traffic to the Tunnel. Run and manage the Tunnel. Add a Zero Trust policy. Run Tunnel as a service.  Step 1: Signup for a free Cloudflare for Teams: Navigate to Cloudflare for Teams and signup for a free account. Cloudflare has a well documented Get started site to walk you through the setup process. For this step, you don\u0026rsquo;t need to go beyond signing up.\nStep 2: Install and authenticate Cloudflared on a Raspberry Pi 4:  First of all, if you’d like to check your device’s architecture, run the following command:  uname -a\rNavigate to Install Cloudflared site to download the proper package for your architecture. In my case, I will install the Cloudflared daemon on my RPI-4, which is an arm64 architecture.  arm64 architecture (64-bit Raspberry Pi 4): sudo wget -O cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-arm64\rsudo mv cloudflared /usr/local/bin\rsudo chmod +x /usr/local/bin/cloudflared\rcloudflared -v\rAMD64 architecture (Debian/Ubuntu): sudo wget https://bin.equinox.io/c/VdrWdbjqyF/cloudflared-stable-linux-amd64.deb\rsudo apt-get install ./cloudflared-stable-linux-amd64.deb\rcloudflared -v\rarmhf architecture (32-bit Raspberry Pi): sudo wget https://bin.equinox.io/c/VdrWdbjqyF/cloudflared-stable-linux-arm.tgz\rtar -xvzf cloudflared-stable-linux-arm.tgz\rsudo cp ./cloudflared /usr/local/bin\rsudo chmod +x /usr/local/bin/cloudflared\rcloudflared -v\rOnce we have installed Cloudflared successfully, we will run the following command to authenticate the cloudflared to our Cloudflare account.  cloudflared login\rRunning the above command will launch the default browser window and prompt you to login to your Cloudflare account. Then, you will be prompted to select a hostname site, which we have create previously in Part 1: Step 2.\nAs soon as you have chosen your hostname, Cloudflare will download a certificate file to authenticate Cloudflared with Cloudflare\u0026rsquo;s network.\nThe cert.pem gives Cloudflared the capabilities to create tunnels and modify DNS records in the account. Once you have created a named Tunnel, you no longer need the cert.pem file to run that Tunnel and connect it to Cloudflare’s network. However, hte cert.pem file is still required to create additional Tunnels, list existing tunnels, manage DNS records, or delete Tunnels.\n\rOnce authorization is completed successfully, your cert.pem will be download to the default directory as shown below.\nIf you\u0026rsquo;re running a headless server (no monitor or keyboard), you could copy the authentication URL and paste it in a browser manually.\nThe credentials file contains a secret scoped to the specific Tunnel UUID which establishes a connection from cloudflared to Cloudflare’s network. cloudflared operates like a client and establishes a TLS connection from your infrastructure to Cloudflare’s edge.\n\rStep 3: Create a Cloudflare Tunnel:  Now, we are ready to create a Cloudflare Tunnel that will connect Cloudflared to Cloudflare\u0026rsquo;s edge. Utilizing the following command will create a Tunnel with tht name and generate an ID credentials file for it.  Prior to creating the Tunnel, you may need to exit the Command Line (CL). Next, let create the Tunnel.\nNote: replace \u0026lt;NAME\u0026gt; with any name of your choosing for the Tunnel.\ncloudflared tunnel create \u0026lt;NAME\u0026gt;\rOnce the Tunnel is created, a credential file is generated. It\u0026rsquo;s a JSON file that has the Universally Unique Identifier (UUID) assigned for the Tunnel.\nNote: although the Tunnel is created, the connection is not established yet.\nStep 4: Configure the Tunnel details: Although we can configure the Tunnel run in an add hoc mode, we will go over creating a configuring the Tunnel to automatically run it as a service.\nCloudflare utilizes a configuration file to determine how to route traffic. The configuration file contains keys and values, which is written in YAML syntax. You may need to modify the following keys and values to meet your configuration file requirements:\n   Keys Values     tunnel Tunnel name or Tunnel UUID   credentials-file location of credentials file (JSON)   hostname subdomain.hostname.xxx (example, test.example.com)   service url to local application - http://localhost:8000   service http_status:404   port of your app 80    By default, on Linux systems, Tunnel expects to find the configuration file in ~/.cloudflared, /etc/cloudflared and /usr/local/etc/cloudflared in that order. Let\u0026rsquo;s create our config file and save in the default expected directory for this tutorial.\nsudo nano ~/.cloudflared/config.yml\rOr,\nsudo nano home/\u0026lt;username\u0026gt;/.cloudflared/config.yml\rThen, we will paste our keys and values as shown below:\ntunnel: 1082b601-bce9-45e4-b6ae-f19020e7d071\rcredentials-file: /root/.cloudflared/1082b601-bce9-45e4-b6ae-f19020e7d071.json\ringress:\r- hostname: test.mytunnel.ml\rservice: http://localhost:80\r- service: http_status:404\rIf you don\u0026rsquo;t have any application ready to test the Tunnel, I\u0026rsquo;d suggest installing NGINX web server and port mapping it to port 80 as I\u0026rsquo;ve done in the configuration file.\n\r\r\rExpand me...\r\r\rHow to install NGINX web server on RPI-4:\nsudo apt install nginx Once the installation is completed, open a browser and type in: localhost:80. If the NGINX web server is installed properly, you shall see it running with its default index.html as shown below. \r Let\u0026rsquo;s make sure that we have all files in this directory:\nls -al\rNow, we have configured all required files to run the Tunnel in the default directory.\n\r\rExpand me...\r\r\rNote, if you\u0026rsquo;d like to save the config.yml file in a different location ( we will refrain from using this method for this tutorial), you will have to point to that directory during the run command by using the following: cloudflared tunnel \u0026ndash;config path/config.yml run UUID or Tunnel Name\nIt\u0026rsquo;s very import to specify \u0026ndash;config to change default directory for the config file. For more information about the configuration file.\n\r Step 5: Create DNS records to route traffic to the Tunnel: Cloudflare can route traffic to our Tunnel connection using a DNS record or a loud balancer. We will configure a DNS CNAME record to point to our Tunnel subdomain. There are two ways to acheive this mission:\nA. Manually: navigate to the DNS tab on Cloudflare Dashboard, create a new CNAME record and add your subdomain of your Tunnel as follows:\n Type: CNAME Name: any subdomain name of your choosing. Target: consists of two parts: \u0026lt;UUID\u0026gt; and \u0026lt;cfargotunnel.com\u0026gt; such as, \u0026lt;UUID.cfargotunnel.com\u0026gt;  B. Programmatically: run the following command from the command line. This command will generate a CNAME record that points to the subdomain of a specific Tunnel. The result is the same as creating a CNAME record from the dashboard as shown in step A.\ncloudflared tunnel route dns \u0026lt;UUID or NAME\u0026gt; test.example.com\rNote: unlike the previous Argo Tunnel architecture, this DNS record will not be deleted if the Tunnel disconnects.\nStep 6: Run and manage the Tunnel: The run command will connect cloudflared to Cloudflare\u0026rsquo;s edge network using the configuration created in step 4. We will not specify a configuration file location so Cloudflared retrieves it from the default location, which is ~/.cloudflared/config.yml\ncloudflared tunnel run \u0026lt;UUID\u0026gt; or \u0026lt;Tunnel Name\u0026gt;\rIf the config.yml file is not placed in the default directory, we need to pinpoint to its location to run the Tunnel:\ncloudflared tunnel --config path/config.yml run \u0026lt;NAME\u0026gt; or \u0026lt;UUID\u0026gt;\rWe can review the list of Tunnels we have created by running the following command:\nCloudflared Commands:    Functions Commands     Create a Tunnel cloudflared tunnel run \u0026lt;NAME\u0026gt;   List Tunnels cloudflared tunnel list   Stop Tunnel cloudflared tunnel stop \u0026lt;NAME\u0026gt;   Restart Tunnel cloudflared tunnel restart \u0026lt;NAME\u0026gt;   Delete Tunnel cloudflared tunnel delete \u0026lt;NAME\u0026gt;   Force Delete Tunnel cloudflared tunnel delete -f \u0026lt;NAME\u0026gt;   Show each Cloudflared info cloudflared tunnel info \u0026lt;NAME\u0026gt;    Stopping Cloudflared will not delete the Tunnel or the DNS record created. Although Tunnel deletes DNS records after 24-48 hours of a Tunnel being unregistered, it does not delete TLS certificates on your behalf once the Tunnel is shut down. If you want to clean up a Tunnel you’ve shut down, you can delete DNS records in the DNS editor and revoke TLS certificates in the Origin Certificates section of the SSL/TLS tab of the Cloudflare dashboard.\n\rTo update Cloudflared:\nsudo cloudflared update\rTo uninstall Cloudflared\nsudo cloudflared service uninstall Step 7: Add a Zero Trust policy: Now, we are ready to head back to Teams dashboard to configure our application and create a Zero Trust Policy.\n On Teams dashboard, navigate to the Application tab and click on Add an application.  Select Self-hosted.  Choose an application name, Session Duration, subdomain and Application domain. Then, click on Next.  Notice that the Tunnel duration ranges from 15 mins to 1 month.\nAdd a name to the rule and select Bypass as a Rule action. On Configure a rule, include Everyone. This rule allows everyone to view our NGINX site at test.mytunnel.ml  In the Advanced settings, enable automatic cloudflared authentication and browser rendering.  Finally, our application is now available in Cloudflare Access and is part of our Application list. We can navigate to a browser and type in our url test.MyTunnel.ml and if our Tunnel is established correctly, we shall see our NGINX web server running as shown below.\nStep 8: Run Tunnel as a service: By running the following command, the Tunnel can be installed as a system service which allows the Tunnel to run at boot automatically as launch daemon. By default, the Tunnel expects to find the configuration file in the default directory, ~/.cloudflared/config.yml but to run Tunnel as a service, we might need to move the config.yml file in ~/etc/cloudflared/.\nWe can employ the move mv command to do the job: mv \u0026lt;path/config.yml\u0026gt; to \u0026lt;/etc/cloudflared/\u0026gt;\nThe below command is in my case with my RPI-4 and how I moved the config file to /etc/cloudflared/\nsudo mv /home/p2/.cloudflared/config.yml /etc/cloudflared/\rNow, we are ready to run Tunnel as a service utilizing the command below:\nsudo cloudflared service install Conclusion: We have successfully established a secure Cloudflare Tunnel that links our locally hosted NGINX web server to Cloudflare\u0026rsquo;s network without requiring any public IP address, port-forwarding or punching through a firewall. We have also configured the Tunnel as a service to start at boot, and now we have our NGINX web server associated and accessible via our domain name, test.MyTunnel.ml\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/vino/",
	"title": "How to Remote into Ubuntu",
	"tags": [],
	"description": "",
	"content": "What is a Virtual Network Computing (VNC)? VNC is a graphical desktop sharing system that allows you to remotely control the desktop interface from another computer or mobile device (running VNC Viewer). VNC Viewer transmits the keyboard and the mouse events to VNC Server, and receives updates to the screen in return. Setting up a VNC Server on our RPI 4 is very beneficial to gain access to it from another device.\nVino VNC Server on Ubuntu Vino is the default VNC server in Ubuntu which allows us to share our graphical desktop form other devices. If you have multiple RPI 4, utilizing VNC server to remote into these Pis (nodes) from your main device is definitely a great way to gain access.\nHow to configure Vino VNC Server on Ubuntu 1- Click on settings and then navigate to \u0026ldquo;Sharing.\u0026rdquo;\n2- On Sharing, toggle sharing to on.\n3- To set Vino to request access each time, check \u0026ldquo;Allow connections to control the screen.\u0026rdquo;\n4- To add a layer of security, select \u0026ldquo;require a password,\u0026rdquo; and set a password hard-to-guess.\n5- Toggle the \u0026ldquo;Networks\u0026rdquo; which you will allow the sharing over. All local networks should show up under \u0026ldquo;Networks.\u0026rdquo;\n6- Now, we will need to locate our IP address. Let\u0026rsquo;s open Terminal and type the following command.\nip addr show\ror\nip a\ror\nhostname -I\rIf you prefer a graphical method to obtain the IP address, then we shall go back to settings, Wi-Fi and clock on the settings of the Wi-Fi network.\nIf you have you RPI 4 wire, then you could obtain the IP address from the settings of the wired network.\n7- The last step is to download a VNC viewer on your primary device to remote into our RPI 4. I would recommend to download RealVNC as VNC viewer. We will use the IP address of our RPI 4 and port 5900 or 5901 to access our RPI4 remotely.\nConclusion: This tutorial concludes the activation of Vino the default VNC server of Ubuntu on RPI 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/portainer/portainer/",
	"title": "Install Docker and Portainer on Ubuntu Instance",
	"tags": [],
	"description": "",
	"content": "What is Portainer? Portainer is the definitive open source container management tool for Kubernetes, Docker, Docker Swarm and Azure ACI. It allows anyone to deploy and manage containers without the need to write codes. To learn more about Portainer.\nUpdating the Operating System Now, we have a secured connection with our instance, let\u0026rsquo;s update and upgrade the operating system. Updating and upgrading the operating system of the instance is always a good habit to have. We will run the following update/upgrade commands. The -y in the end of the command line will automatically enters \u0026ldquo;yes\u0026rdquo; as a confirmation before installing the updates.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rCleaning up after an update If this is not the packges first time to run an update, there might have some unnecessary packages left for cleaning. Removing these packages will free space and prevent your system from cluttering. The following command shall do the job.\nsudo apt autoremove\rStep 1: Install Docker If always want to automatically get the latest version of Docker on Ubuntu, you must add its official repository to Ubuntu system. To do that, run the commands below to install prerequisite packages:\nsudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common\rNext, run the commands below to download and install Docker’s official GPG key. The key is used to validate packages installed from Docker’s repository making sure they’re trusted.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\rsudo apt-key fingerprint 0EBFCD88\rThe response would be like this:\nResponse:\rpub rsa4096 2017-02-22 [SCEA]\r9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88\ruid [ unknown] Docker Release (CE deb) \u0026lt;docker@docker.com\u0026gt;\rsub rsa4096 2017-02-22 [S]\rNow that the official GPG key is installed, run the commands below to add its stable repository to Ubuntu. To add the nightly or test repository, add the word nightly or test (or both) after the word stable in the commands below.\nsudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot;\rAfter this command, Docker’s official GPG and repository should be installed on Ubuntu. If you have older versions of Docker, run the commands below to remove them:\nsudo apt-get remove docker docker-engine docker.io containerd runc\rWhen you have removed all the previous versions of Docker, run the commands below to install the latest and current stable version of Docker: To install specific version of Docker, run the apt-cache command. Then select the version to install.\napt-cache madison docker-ce\rOutput:\rdocker-ce | 5:19.03.12~3-0~ubuntu-focal | https://download.docker.com/linux/ubuntu focal/stable amd64 Packages\rdocker-ce | 5:19.03.11~3-0~ubuntu-focal | https://download.docker.com/linux/ubuntu focal/stable amd64 Packages\rdocker-ce | 5:19.03.10~3-0~ubuntu-focal | https://download.docker.com/linux/ubuntu focal/stable amd64 Packages\rdocker-ce | 5:19.03.9~3-0~ubuntu-focal | https://download.docker.com/linux/ubuntu focal/stable amd64 Packages\r....\rNow to install a specific version, run the commands below with the version you wish to install:\nsudo apt-get install docker-ce=5:19.03.10~3-0~ubuntu-focal docker-ce-cli=5:19.03.10~3-0~ubuntu-focal containerd.io\rIf you just want to latest version without specifying above, run the commands below. The command below will always install the highest possible version:\nsudo apt-get install docker-ce docker-ce-cli containerd.io\rThis will install Docker software on Ubuntu. Add your account, for most cases it will be ubuntu, to Docker group and restart:\nsudo usermod -aG docker $USER\rReboot your instance:\nsudo reboot\rTo verify that Docker CE is installed correctly you can run the hello-world image:\nsudo docker run hello-world\rIf Docker is installed correctly you will see the following response:\nResponse:\rHello from Docker!\rThis message shows that your installation appears to be working correctly.\rTo generate this message, Docker took the following steps:\r1. The Docker client contacted the Docker daemon.\r2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub.\r(amd64)\r3. The Docker daemon created a new container from that image which runs the\rexecutable that produces the output you are currently reading.\r4. The Docker daemon streamed that output to the Docker client, which sent it\rto your terminal.\rStep 2: Install Docker Compose On Ubuntu Linux, you can download the Docker Compose binary from the Compose repository release page on GitHub.\nTo install it, run the commands below to download version 1.28.5 As of this writing, this was the current version.\nsudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose\rTo install a different version of Compose, substitute 1.28.5 with the version of Compose you want to use.\nAfter downloading it, run the commands below to apply executable permissions to the binary file and create a symbolic link to /usr/binary\nsudo chmod +x /usr/local/bin/docker-compose\rsudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\rNow, Docker Compose should work. To test it, we will run the command below:\ndocker-compose --version\rYou should see similar output as below:\nResponse:\rdocker-compose version 1.28.5, build 0aa59064\rStep 3: Setup Portainer Now that Docker and Docker Composer are installed, follow the steps below to get Portainer setup.\nYou can use Docker command to deploy the Portainer Server; note the agent is not needed on standalone hosts, however, it does provide additional functionality if used.\nTo get the server installed, run the commands below.\ncd ~/\rdocker volume create portainer_data\rdocker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:latest\rYou’ll just need to access the port 9000 of the Docker engine where Portainer is running using your browser.\nNote: the -v /var/run/docker.sock:/var/run/docker.sock option can be used in Linux environments only.\nAfter a successful pull, you should get a similar message as below:\nlatest: Pulling from portainer/portainer-ce\rd1e017099d17: Pull complete a7dca5b5a9e8: Pull complete Digest: sha256:4ae7f14330b56ffc8728e63d355bc4bc7381417fa45ba0597e5dd32682901080\rStatus: Downloaded newer image for portainer/portainer-ce:latest\r2fd5f4a0883a9d358ad424fd963699445be8839f3e6a2cf73d55778bcc268523\rAt this point, all you need to do is access Portainer portal to manage Docker. Open your web browser and browse to the server’s hostname or IP address followed by port #9000\nhttp://localhost:9000 or http://your_EC2_instance_public_ip_adress:9000\nYou should get Portainer login page to create an admin password.\nSubmit a new password.\nNow, you see some options to choose the environment you want to manage. Since we installed Docker on the same instance, select to connect and manage Docker locally.\nYou’ll be directed to Portainer dashboard where you can start managing Docker. If you see a notification for upgrade, click on it and proceed with the upgrade process.\nConclusion: By the end of this tutorial, we have successfully installed Portainer Docker management tool on our Ubuntu 20.04 EC2 instance. In part 3 of this tutorial, we will learn how to install and run a Wordpress web application.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/wordpress/_wordpress/",
	"title": "Install WordPress Docker",
	"tags": [],
	"description": "",
	"content": "What is WordPress? WordPress is an open source software, which is widely used to create a beautiful website, blog, or app. To learn more about Wordpress.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/",
	"title": "Portainer Tutorials",
	"tags": [],
	"description": "",
	"content": "PORTAINER Portainer Docker UI Manager Tutorials Portainer is one of the best way to learn Docker and Containers for beginners. Portainer is an open source management UI for Docker including DOcker Swarm environment. It makes managing Docker containers simpler and allows you to manage containers, images, networks and volumes from the web based Portainer dashboard. This tutorial consists of three parts. In the first part, we will spin a Ubuntu 20.04 LTS EC2 instance, and then in the second part, we will install Portainer on the Ubuntu 20.04 LTS EC2 instance. Finally, in the last part, we will run Portainer and install Wordpress site.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/portainer/spin-ubuntu-instance/ssh-into-ubuntu-ec2/",
	"title": "SSH into Ubuntu Instance",
	"tags": [],
	"description": "",
	"content": "What is Secure Shell (SSH)? Secure Shell is a network protocol uses encryption to secure connection between a client and a server. All user authentication, commands, output, and file transfers are encrypted to protect against attacks in the network. There are many tools available to establish SSH connection such as, PuTTy and WinSCP for Windows, CyberDuck for MacOS, OpenSSH for Unix and Linux and many other great tools.\nI would suggest we utilize AWS Command Line Interface (AWS CLI) version 2. The AWS CLI is a open source powerful tool that enables you to interact with AWS services using commands in your local machine command line shell. In my case, I\u0026rsquo;m using AWS CLI version 2 from within my Windows Command Prompt. Please, follow the installation process as documented by AWS based on your machine type (local device).\nWith this brief introduction about SSH, we will proceed with tutorial from where have stopped. Next, let’s get to actually logging into our Ubuntu EC2 instance. To do so, you’ll have to open Terminal of the SSH tool, which you have chosen to use. As I have previously stated, I have downloaded AWS CLI v2 ;therefore, I will run Command Line (Command Prompt) on Windows.\nOn CL, the first to do is change the directory using the command cd into the directory/folder where we have saved our key pair (pem file). This is the file we have downloaded in Part 1 tutorial.\nExample:\ncd C:\\Users\\Omar\\XXXXX\\Documents\\XXX\\AWS\\XXXX\rcd C:\\your full URL where you have saved the pem file for the instance\u0026gt;\rBack to the AWS console and on the Instances section, let select our Ubuntu instance and then click on Actions \u0026gt; Connect button.\nThen, let\u0026rsquo;s click on the third tab, SSH Client, and copy the SSH command line by clicking on the copy icon as shown below. This command has combined our pem file name, ssh command and our instance public domain name.\nLet\u0026rsquo;s paste this command on our CL or your chosen SSH tool and click enter. During the authentication process, you will prompted, \u0026ldquo;Are you sure you want to continue connecting (yes/no)?\u0026rdquo;, type in yes.\ncd C:\\Users\\Omar\\XX\\XX\u0026gt; ssh -i \u0026quot;Portainer.pem\u0026quot; ubuntu@ec2-xxxxxxx.compute-1.amazonaws.com\rCongratulations, now we have successfully SSH into our Ubuntu 20.04 LTS instance.\non Now, time to connect to the instance using terminal or we can say securely SSH into the instance. Select the EC2 instance into which you want to SSH.\nPlease note that some parts of the images in this tutorial will be pixilated or hidden to hide all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rConclusion: This concludes the second section of this tutorial. We have successfully established a secure connection (SSH) with our Ubuntu 20.04 LTS EC2 instance. Let\u0026rsquo;s move on to Part 2 of the tutorial.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/cloudwatch/",
	"title": "Install Unified CloudWatch Agent",
	"tags": [],
	"description": "",
	"content": "Install Unified CloudWatch Agent: The unified CloudWatch agent. It enables you to collect both logs and advanced metrics with one agent. It offers support across operating systems, including servers running Windows Server. This agent also provides better performance. The older CloudWatch Logs agent, which supports the collection of logs from only servers running Linux. AWS strongly recommends migrating to the unified CloudWatch agent.\nThe unified CloudWatch agent enables you to do the following: 1- Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in Metrics Collected by the CloudWatch Agent.\n2- Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.\n3- Retrieve custom metrics from your applications or services using the StatsD and collectd protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers.\n4- Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.\nCreate IAM Role to Use CloudWatch Agent: Access to AWS resources requires permissions. You create an IAM role, an IAM user, or both to grant permissions that the CloudWatch agent needs to write metrics to CloudWatch.\nIn the navigation pane on the left, choose Roles and then Create role.\nFor Choose a use case, choose EC2 under Common use cases.\nIn the list of policies, select the check box next to CloudWatchAgentServerPolicy.\nLet us confirm that CloudWatchAgentServerPolicy appears next to Policies, then choose Create role.\nNow, we will go back to our Ubuntu EC2 instance and attach this role to it.\nChoose our IAM role and save it.\nInstall the unified CloudWatch agent: The following dependencies need to be installed on the EC2 instance ( t2micro running Ubuntu 20.04 LTS) before the agent can be installed:\nsudo apt update \u0026amp;\u0026amp; sudo apt install collectd -y \u0026amp;\u0026amp; sudo apt install awscli -y\rThe following command will walk you through some configurations. The most important one is the region code to point to the region where the EC2 instance is configured. My region is N Virginia, so it should be us-east-1.\naws configure We will use a role for the CloudWatch Agent and will attach to the EC2 instance; therefore, we will leave the AWS Access and Secret keys empty. The result:\nAWS Access Key ID [None]:\rAWS Secret Access Key [None]:\rDefault region name [None]: us-east-1\rDefault output format [None]:\rInstall the agent using the following steps: sudo chown ubuntu:ubuntu -R /opt\rmkdir /opt/softwares\rwget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb\r\rIf you would like to download the agent for a different operating system, please refer to AWS instruction.\n\rLet\u0026rsquo;s install our Ubuntu version of the CloudWatch Agent.\nsudo dpkg -i -E ./amazon-cloudwatch-agent.deb\rTo start the installation wizard use the following command: Note: to enable the \u0026ldquo;usual metrics\u0026rdquo;, you might to select the defaults for all the questions during the wizard setup. Add the following path to collect logs from /var/log/syslog, and select Advanced for the following question: “Which default metrics config do you want?”.\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard\rAWS has a well detailed instruction about this installation wizard, for additional configuration of the CloudWatch Agent wizard.\nOnce the installation is completed, let us run the below commands to run and test the agent.\nTo start the agent:\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s\rTo restart the agent:\nsudo service amazon-cloudwatch-agent stop start status\rThe logs should be in the the below folder:\nls -al /opt/aws/amazon-cloudwatch-agent/logs/\rNotice, once you\u0026rsquo;re on CloudWatch \u0026ldquo;Metrics\u0026rdquo; \u0026ldquo;custom namespace\u0026rdquo;, scroll all the down until you find \u0026ldquo;CWAgent.\u0026quot;\nFrom the top select \u0026ldquo;Numbers\u0026rdquo; as the visualization. In the cloudwatch management console, select metrics under the \u0026ldquo;EC2\u0026rdquo; namespace as well to see the CPU and other numbers.\nA Stress Tool to Simulate a Stress for testing Sysbench is a command line app to run benchmarks on our system/instance. It is mainly intended for testing CPU, memory and file throughput as well. We can install this utility to simulate a stress on our EC2 to push CloudWatch Agent to present it on our metrics.\nTo install Sysbench in Ubuntu, run the command below:\nsudo apt install sysbench\rWe can increase or decrease the threads to simulate the stress.\nsysbench cpu --threads=5 run\rBelow, you will notice clearly the result of the stress on our EC2 instance by Sysbench.\nMany metrics are generated by our CloudWatch Agent:\nI would suggest you manuever CloudWatch dashboard and get familiar with how to make these metrics presentable in a fashion way by adjusting the x and y axises. Also, let\u0026rsquo;s try creating a dashboard and explore all other features that CloudWatch offers.\nConclusion: This concludes our tutorial of the successful installation of CloudWatch Agent.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/x11vnc/",
	"title": "Install x11vnc Server on Ubuntu 20.04 for Remote Access",
	"tags": [],
	"description": "",
	"content": "What is a X11VNC Server? The Virtual Network Computing (VNC) allows to view and interact with devices remotely with any VNC viewer. When working with multiple RPIs, having VNC servers give us access all our RPIs from one device. There are many VNC servers and X11VNC is one of them. On another tutorial, we will install RealVNC, which is another vnc server, and I will go over the differences between X11VNC and RealVNC.\nTo learn more about X11VNC.\nX11VNC is an alternative to Vino. If you\u0026rsquo;re happy with Vino, you don\u0026rsquo;t need to install x11vnc.\n\rLet\u0026rsquo;s Install X11VNC Server on our Ubuntu As a best practice, we shall update and upgrade our operating system.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rNow, we will need to change display manager to lightdm\nsudo apt-get install lightdm\rLet\u0026rsquo;s reboot the Raspberry Pi for these changes to take effect.\nsudo reboot\rThe following command to install x11vnc server\nsudo apt-get install x11vnc\rNow, we will create our service configuration file.\nsudo nano /lib/systemd/system/x11vnc.service\rLet\u0026rsquo;s copy and paste the following configuration into our newly create service file. These configurations will start the x11vnc server and make it run continuously. It will also force the server to restart automatically in case of any failure.\nNote: yourPassword is the password.\n[Unit]\rDescription=x11vnc service\rAfter=display-manager.service network.target syslog.target\r[Service]\rType=simple\rExecStart=/usr/bin/x11vnc -forever -display :0 -auth guess -passwd yourPassword\rExecStop=/usr/bin/killall x11vnc\rRestart=on-failure\r[Install]\rWantedBy=multi-user.target\rThe following commands to reload the systmd system and to enable and start the x11vnc service.\nsystemctl daemon-reload\rsystemctl enable x11vnc.service\rsystemctl start x11vnc.service\rsystemctl status x11vnc.service\rWe can check the status of our x11vnc server as shown.\nx11vnc status\rFinally, let\u0026rsquo;s reboot the RPI 4.\nsudo reboot\rNow, we are able to vnc into our RPI4 from any vnc viewer. We will need the Notice that x11vnc is warning us that we are running this server without a password. Well, let\u0026rsquo;s setup a password.\nIf you have not set your password in the previous step (yourPassword), you could use the following command to setup. You will be promoted to enter a password and to confirm it.\nx11vnc -storepasswd Make sure you have received a similar success message,and that you\u0026rsquo;re password has been written, if not try it again.\npassword written to: /home/\u0026lt;your-login-name\u0026gt;/.vnc/passwd\rConclusion: This concludes our x11vnc server installation on a Ubuntu 20.10 or 20.04 LTS installed on a RPI 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/",
	"title": "Raspberry Pi Tutorials",
	"tags": [],
	"description": "",
	"content": "Raspberry Pi Tutorials Multiple Raspberry Pi Tutorials and Projects\nIf you\u0026rsquo;re interested in Raspberry Pi project but are not sure where to start, let\u0026rsquo;s walk this journey together. In these tutorials, we will learn what a Raspberry Pi is, hardware and software, different models and more resources for projects and tutorials.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/mattermost/",
	"title": "install Mattermost and MySQL on EC2",
	"tags": [],
	"description": "",
	"content": "Mattermost Mattermost is an open-source, self-hostable online chat service with file sharing, search, and integrations. It is designed as an internal chat for organizations and companies, and mostly markets itself as an open-source alternative to Slack and Microsoft Teams. For information about Mattermost.\nAs part of this tutorial, we will install and configure MySQL on Ubuntu 20.04 LTS EC2 instance. The intension is implement two different subnets, Public and Private subnets. The public subnet will host the Mattermost web application, and the private subnet will host the MySQL database for the Mattermost web application. Since MySQL database will be isolated in a private subnet, we will need to associated it with a NAT instance, which will be hosted on the public subnet to allow the MySQL database in the private subnet to send traffic to the internet gateway. For more information about the functionality of NAT instance.\nThe below diagram is our design architecture for this tutorial.\nStep 1: VPC 1- Create a VPC.\n2- Name the VPC and choose the CIDR block for the VPC as 10.0.0.0/16 3- Select the VPC, edit DNS hostnames and enable DNS hostnames.\nWe have successfully created our VPC.\nStep: Create Public and Private Subnets 1- Navigate to Subnets and then click on \u0026ldquo;Create Subnet\u0026rdquo;.\n2- Make sure you select the newly created VNC and not the default VPC.\n3- Name this subnet as a Public_SN as in my case. It\u0026rsquo;s optional of you would like to select a specific Availability Zone. The CIDR block that does not conflict with any in the same VPC. Let choose 10.0.1.0/24 as our CIDR block for the public subnet and then click on create subnet.\n4- Since this is our designated Public Subnet, we will need to click on \u0026ldquo;Actions\u0026rdquo;, \u0026ldquo;Modify auto-assign IP setting\u0026rdquo; and enable it. If we don\u0026rsquo;t enable auto-assign public IPv4 address, our subnet will not be abe to allocate public IP addresses to instances within the subnet. To read more about IP Addressing in VPC.\n5- Now, we will create a Private_SN. It\u0026rsquo;s optional of you would like to select a specific Availability Zone. I will select a different AZ than the one I have my public subnet. Let choose a CIDR block that does not conflict or overlap with any other CIDR block in our VPC. I chose 10.0.2.0/24 as our CIDR block for the private subnet and then click on create subnet.\nNote: don’t enable auto-assign public IPv4 address for the private subnet\nStep 2: Internet Gateway In order for the VPC to communicate with the internet, we will need to create and attach an **Internet Gateway\u0026quot;\u0026quot; to our architecture.\n1- Let\u0026rsquo;s navigate to VPC, Internet Gateway, name it and then click on \u0026ldquo;Create Internet Gateway.\u0026rdquo;\n2- The Internet Gateway should be attached to our VPC by clicking on \u0026ldquo;Actions\u0026quot;and \u0026ldquo;Attach to VPC.\u0026rdquo;\n3- We will select our newly created VPC and click on \u0026ldquo;Attach internet Gateway.\u0026rdquo;\nStep 3: Route tables The Route Tables contain a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. Each subnet in our VPC must be associated with a route table, which controls the routing for the subnet. We can explicitly associate a subnet with a particular route table. Otherwise, the subnet is implicitly associated with the main route table. A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same subnet route table.\n1- Let\u0026rsquo;s create a public route table by navigating to \u0026ldquo;Route Tables\u0026rdquo; and then clicking on \u0026ldquo;Create route table.\u0026rdquo;\n2- After naming the public route table, make sure you are selecting our newly created VPC, and then click on \u0026ldquo;Create.\u0026rdquo;\n3- Now, we will add a route to out Internet Gateway. Let\u0026rsquo;s click on \u0026ldquo;Routes\u0026rdquo; and then \u0026ldquo;Edit routes.\u0026rdquo;\n4- Click on \u0026ldquo;add route\u0026rdquo;, choose Destination as 0.0.0.0/0, select the Target as our Internet Gateway and click on Save routes.\n5- Now, we will need to associate our public subnet with this public route table. Let\u0026rsquo;s click on Subnet Associations, Edit subnet associations, check our public subnet and click Save.\nAs a best practice, let\u0026rsquo;s inspect Routes and Subnet Associations to ensure they reflect the association of public route and public subnet as well as thr attachment of Internet Gateway to this public subnet.\nNote, we will also create a private route table but after we spin our NAT instance.\nStep 4: NAT instance Our MySQL database will be hosted in a private subnet;therefore, we will utilize a NAT instance to send traffic to the internet gateway. AWS recommends using NAT Gateway instead a NAT instance, but for this tutorial, we will use a community available NAT instance.\n1- Let\u0026rsquo;s navigate to EC2, select Instances and Launch Instances. From the lest menu, click on Community AMIs and then type in NAT in the search bar. My NAT AMI selection is shown on the image below.\n2- To keep the tutorial within the Free Tier eligible, I will select t2.micro for the Instance Type. On the Configuration Instance Details, select the VPC(MyVPC) and Public Subnet(Public_SN).\n3- No need to add more than 8 GiB storage size. It\u0026rsquo;s best practice to add tags.\n4- We shall create a security group for the NAT instance. We will open ports 80 and 443 only. These are the two port, which we need to allow traffic from our MySQL database instance. Then, let\u0026rsquo;s review and launch our NAT instance.\n5- let\u0026rsquo;s review, launch our NAT instance and download our Key pair name.\n6- In any NAT instance, we must stop source/destination checking. This allows the NAT instance to send and receive traffic when the source or rhe destination is not itself.\nStep 5: Private Route Table 1- Now, we will create a private route table using the steps for the public route table.\n2- Click on \u0026ldquo;add route\u0026rdquo;, choose Destination as 0.0.0.0/0, select the Target as our NAT instance and click on Save routes.\n3- Now, we will need to associate our private subnet with this private route table. Let\u0026rsquo;s click on Subnet Associations, Edit subnet associations, check our private subnet and click Save.\nLet\u0026rsquo;s inspect our final associations to ensure all is in place. Step 6: MySQL Database Instance 1- Navigate to EC2, select Launch instance and select AMI Ubuntu Server 18.04 LTS.\n2- We will keep the Instance Type as t2.micro. On the Configure Instance Details, select our VPC and place the instance in the private subnet.\n3- No changes to the storage size but let\u0026rsquo;s add a tag. We will configure a security group to open ports 22 and 8065. We will need to open port 22 so we can SSH from the Mattermost web app into the MySQL database instance. Port 8065 is for the MySQL instance to communicate with the Mattermost web app.\n4- Let\u0026rsquo;s create a new key pair and launch the instance.\nStep 7: Mattermost Instance 1- We will choose the same AMI we used previously for the MySQL instance, which is Ubuntu Server 20.04 LTS.\n2- On Configure Instance Details, make sure that you select the correct VPC and Public Subnet.\n3- We will configure a security group to open ports 22 and 3306. We will need to open port 22 so we can SSH to the Mattermost web app and port 3306 to run the Mattermost web app.\n4- We will not generate new key pair instead we will use MySQL key pair. We will this key pair to SSH into the Mattermost instance as well as SSH into MySQL database instance. I have refrained from generating a different key pair to avoid confusion; however, in a production scenario, I will use different key pairs.\nStep 8: MySQL Installation 1- We will use the below scp command to copy MySQL database pem file into the Mattermost instance. Replace the x.xxx.xxx.xx with your Mattermost instance\u0026rsquo;s public IP address. We could use this command even if we are not SSH into the instance.\nExample:\nscp -i\u0026lt;application server pem file\u0026gt; \u0026lt;database server pem file\u0026gt; ubuntu@\u0026lt;application public IP address\u0026gt;:/home/ubuntu\rActual:\nscp -i MySQL.pem MySQL.pem ubuntu@3.235.137.90:/home/ubuntu\rThe result:\n2- We will SSH into the Mattermost instance. There are many methods to SSH into an instance depending on the device. To learn about how to SSH into our Ubuntu instance, please refer to my previous tutorial.\n3- From the Mattermost instance, we will SSH into the MySQL database instance. We will use the pem file copied in step 1. Also, since our MySQL instance is in a private subnet, it does not have a public IP address; therefore, we will use its private IP address to SSH into it from the Mattermost most instance. With the pem file and the private IP address to our MySQL instance, we can SSH from our Mattermost instance.\nFirstly, let\u0026rsquo;s secure the pem file in the Mattermost instance but restricting access to it using the command chmod 600.\nsudo chmod 600 MySQL.pem\r4- Now, we will attempt to SSH into MySQL database instance.\nExample:\nssh -i \u0026lt;database server pem file\u0026gt; ubuntu@\u0026lt;private IP of database server\u0026gt;\rActual:\nssh -i MySQL.pem ubuntu@10.0.2.60\r5- Now, we have successfully SSH into the MySQL database instance, let\u0026rsquo;s install MySQL database.\nAs a best practice, let\u0026rsquo;s update the instance first.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rwget https://raw.githubusercontent.com/OmarCloud20/aws-tutorials/main/install_mysql.sh\r6- To configure MySQL, let\u0026rsquo;s run the following commands.\nchmod 700 install_mysql.sh\rsudo ./install_mysql.sh\rWe have installed and configured MySQL database successfully. Let\u0026rsquo;s exit the instance by typing exit.\nStep 9: Mattermost Installation 1- We will use the following bash file to install and configure Mattermost app.\nwget https://raw.githubusercontent.com/OmarCloud20/aws-tutorials/main/mattermost_install.sh\rchmod 700 mattermost_install.sh\r2- We will use MySQL instance private IP address to connect Mattermost to the database.\nExample:\nsudo ./mattermost_install.sh \u0026lt;private IP of MySQL server\u0026gt;\rActual:\nsudo ./mattermost_install.sh 10.0.2.60\r3- Final configuration:\nsudo chown -R mattermost:mattermost /opt/mattermost\rsudo chmod -R g+w /opt/mattermost\rcd /opt/mattermost\rsudo -u mattermost ./bin/mattermost\rStep 10: Running Mattermost Web Application To inspect the deployment of the Mattermost web application, let\u0026rsquo;s navigate to the browser URL bar and type in, :8065\nCongratulation, we have installed and configured Mattermost application and connect it with its MySQL database on Ubuntu 20.04 and Ubuntu 18.04 LTS respectively.\nConclusion: By the end of this tutorial, we have successfully created Mattermost web app, MySQL database and NAT instances on AWS EC2.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/realvnc/",
	"title": "Install RealVNC Server on Ubuntu 20.04 for Remote Access",
	"tags": [],
	"description": "",
	"content": "What is a RealVNC Server? RealVNC allows us to interact with our Raspberry Pis graphically via Virtual Network Computing (VNC). The RealVNC server comes preinstalled with the Raspberry Pi OS. It\u0026rsquo;s extremely secure, convenient and reliable. Until the moment of writing this tutorial, 5/15/2021, RealVNC allows us to have up to 5 subscribed devices to remote into via cloud connectivity for FREE. By using this feature, we can remote into our devices from anywhere in the world without a VPN, a port-forwarding or a firewall configuration. If you\u0026rsquo;d like to subscribe more devices, it won\u0026rsquo;t be covered under the free subscription plan. However, you could access more devices from within the local network using direct feature.\nSince we have installed Ubuntu, which is a 64bit-OS, on our RPI-4, and RealVNC only provides a 32bit server for Raspberry Pi OS, we will need to go through some hoops to get RealVNC installed and running. During this tutorial, I will go over how to install RealVNC on Ubuntu 20.04 LTS.\nI have successfully installed and tested RealVNC based on multiple resources such, resource 1 and resource 2.\n\rStep 1: Install RealVNC Server on our Ubuntu 20.04 LTS Let\u0026rsquo;s navigate to our terminal. As a best practice, we shall update and upgrade our operating system.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rLet\u0026rsquo;s download the arm64 package from the Raspberry Pi Foundation\u0026rsquo;s site using the following command.\nwget https://archive.raspberrypi.org/debian/pool/main/r/realvnc-vnc/realvnc-vnc-server_6.7.2.43081_arm64.deb\rNow, we will install the package using dpkg command:\nsudo dpkg -i realvnc-vnc-server_6.7.2.43081_arm64.deb\rHere, we will add specific files to /user/lib/ folder, so let cd into it:\ncd /usr/lib/aarch64-linux-gnu\rLet\u0026rsquo;s add the following 10 files to the folder:\nsudo ln libvcos.so /usr/lib/libvcos.so.0\rsudo ln libvchiq_arm.so /usr/lib/libvchiq_arm.so.0\rsudo ln libbcm_host.so /usr/lib/libbcm_host.so.0\rsudo ln libmmal.so /usr/lib/libmmal.so.0\rsudo ln libmmal_core.so /usr/lib/libmmal_core.so.0\rsudo ln libmmal_components.so /usr/lib/libmmal_components.so.0\rsudo ln libmmal_util.so /usr/lib/libmmal_util.so.0\rsudo ln libmmal_vc_client.so /usr/lib/libmmal_vc_client.so.0\rsudo ln libvcsm.so /usr/lib/libvcsm.so.0\rsudo ln libcontainers.so /usr/lib/libcontainers.so.0\rFinally, let\u0026rsquo;s enable and start the following services:\nsudo systemctl enable vncserver-virtuald.service\rsudo systemctl enable vncserver-x11-serviced.service\rsudo systemctl start vncserver-virtuald.service\rsudo systemctl start vncserver-x11-serviced.service\rLet\u0026rsquo;s reboot our Raspberry Pi for these changes to take effect.\nsudo reboot\rNote: if RealVNC server GUI has not started automatically, follow the following steps. On the default display manager, select lightdm.\nsudo apt-get install lightdm\rsudo reboot\r\rThis installation will also work on Ubuntu 20.10 as well.\n\rStep 2: Signup for a Free RealVNC Team\u0026rsquo;s Account   Signup for a free RealVNC Team\u0026rsquo;s Account.\n  On your RPI-4, signin with your credentials to add this RPI-4 to your Computers' account.\n  Notice, the Connectivity is Direct and Cloud. Now, your RPI-4 is ready to receive a secured VNC connection whether it\u0026rsquo;s a direct or cloud connection.\nTo manage your devices, navigate to Computers under TEAM on your profile page and you shall see all subscribed devices to Cloud. Remember, you may only add up to 5 devices using the free subscription.  Step 3: Install RealVNC Viewer Now, we have successfully installed RealVNC server, signup for an account and subscribed our device, we will need a method to remote into our RPI-4. Navigate to RealVNC to download a VNC Viewer to your viewer device. This could be your iPhone, Android phone, Mac Pro or your Windows machine.\n After installing the VNC Viewer, signin with your credentials and you shall see all subscribed devices for cloud connection automatically. You may also add direct connection device on the Address book section.  Double click on your RPI-4, in my case it\u0026rsquo;s p4. Click on Continue.  Enter your RPI-4\u0026rsquo;s username and password to connect via VNC. These are the credentials you\u0026rsquo;re using to signin into your RPI-4.  Conclusion: This concludes our RealVNC server installation on a Ubuntu 20.04 LTS (or 20.10) on a RPI 4. We have successfully installed and configured RealVNC server on our RPI-4 arm64. Moreover, we have installed RealVNC Viewer and established a cloud VNC connection securely to our RPI-4. Cheers!\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/lambda/",
	"title": "AWS Lambda Custom Layers and a Lambda Function in Python",
	"tags": [],
	"description": "",
	"content": "What is AWS Lambda? AWS Lambda is a serverless compute service, which allow the user to run codes without provisioning or managing servers. With Lambda, the user does not manage runtimes nor admin the server. Utilizing Lambda is as simple as uploading a code in a ZIP file or a container image, and Lambda automatically allocates compute execution power and runs the code based on the incoming request or event. Lambda functions can be written in many flavors such as, Node.js, Python, Go, Java, and more. For more information about Lambda.\nIn this tutorial, we will explore two important aspects of AWS Lambda, which are Custom Layers and Lambda Function in Python using layers.\nTutorial Scenario:  A user uploads a PDF to S3 bucket. The bucket PUT event triggers a lambda function. The lambda function inspects the JSON and extract the bucket and the file names from it. The file is read using boto3 - AWS Python library. Apache Tika library is to parse the PDF and to extract metadata and content. The outcome text is saved in a different/destination S3 bucket.  Then, We shall create a common layer containing the 3rd part library dependency of Apache Tika. Finally, code a lambda function in python.  Step 1: Spin a Ubuntu 18.04 LTS instance and Configure App File:  Spin a Ubuntu 18.04 LTS instance. Ensure port 22 is open to SSH. SSh into the instance and run the following commands:  As a best practice, let\u0026rsquo;s update our Ubuntu operating system.\nsudo apt update\rLet\u0026rsquo;s install the zip utility.\nsudo apt install zip -y\rpip is the package installer for Python. Let\u0026rsquo;s install pip.\nsudo apt install python3-pip\rThe chown command allows us to change the user and/or group ownership of a given file or directory.\nsudo chown ubuntu:ubuntu -R /opt\rWe will cd into opt directory. cd is the command used to move between directories/folders.\ncd /opt\rmkdir is the command to create directories.\nmkdir -p appzip/python\rLet\u0026rsquo;s cd into app directory.\ncd appzip/\rNow, we will use the zip utility to zip the appzip.\nzip -r appzip.zip python\rWe will download the zip file to our device. We will use Secure Copy scp, which is command line utility to allow us to securely copy files and directories between devices.\nFrom your device\u0026rsquo;s terminal, use the following command to download the zip file to your local device. For more information, visit AWS.\nscp -i \u0026lt;Name\u0026gt;.pem ubuntu@\u0026lt;EC2-Public-IP OR DNS\u0026gt;:/opt/appfolder/appfolder.zip ./appfolder.zip\rI\u0026rsquo;m using AWS CLIv2 via my Windows Command Line:\nC:\\Users\\Omar-PC\\Desktop\u0026gt;scp -i Lambda.pem ubuntu@XX.XXX.XX.XXX:/opt/appfolder/appfolder.zip C:\\Users\\Omar-PC\\Desktop\rStep 2: AWS Lambda Layer Configuration On AWS lambda, click on \u0026ldquo;Layers\u0026rdquo;, give it a name, description and upload the zip file (appzip). So far, we created a lambda layer as simple as that.\nStep 3: Lambda Role and S3 configuration  Create an IAM role which allows the lambda function to access other needed resources.   Add the following policies to the role, \u0026ldquo;AmazonS3Full Access and CloudWatchFull Access.\n  Name the role, lambda-pdf-extractor.\n   On S3, create two buckets:\nA. First bucket name: source[5 random numbers]\nB. Second bucket name: destination[5 random numbers]\n  Note: S3 bucket names are global names, which means they are unique. Two identical bucket names don\u0026rsquo;t exist.\nUpload a random PDF to the sourcexxxxxx bucket (under 200kb of size). Save the name of the PDF, we will need it down the road.  Step 4: Python Lambda Function Now, it\u0026rsquo;s time to create the lambda function using the layer and the role we have previously created.\n On the Lambda console, click on \u0026ldquo;Create function.\u0026rdquo; Function name: PDFExtractor Select \u0026ldquo;Author from scratch.\u0026rdquo; Runtime: Python 3.6. Permissions: \u0026ldquo;Use an existing role\u0026rdquo; and select the role we have previously created, lambda-pdf-extractor. Click on \u0026ldquo;Create function.\u0026rdquo;   On the lambda function code editor, remove the existing code and replace it with the code form my GitHub repo.\n  On Layers section, add a Custom layers by selecting our previously created Layer (TikaLayer) version 1.\n  On Configuration, Environment variables, add the following:     key value     TARGET_BUCKET destinationxxxxx    Note: replace destinationxxxxx with the name of your destination S3 bucket.\n On Configuration, General configuration, edit to the following:\nA. Memory: 256 MB\nB. Timeout: 2 min\nC. On the Existing role, select our previously created role, lambda=pdf-extractor\n   Next, select Test, and on the Template dropdown menu, select \u0026ldquo;Amazon S3 Put\u0026rdquo;\n  Name the event: s3put\n  On the JSON section, replace the JSON with this one from my Github. Let\u0026rsquo;s ensure that you update the following:\nA. The AWS region. In my case, it is us-east-1\nB. The bucket name. This should match the name of your S3 source bucket.\nC. The object key. The object name should match the name of your PDF.\n  Click on Save changes.\n  Step 5: Testing the Python Lambda Function Now, we are ready to test our function by selecting \u0026ldquo;Code, our \u0026ldquo;s3put\u0026rdquo; and click \u0026ldquo;test\u0026rdquo; button. If you have received status code 200, it means our code executed successfully.\nIf the function fails, it will state error \u0026ldquo;Unable to start the Tika server.\u0026rdquo; If this occurs, run the test one more time.\nWhen the lambda function executed successfully, the extracted PDF is saved in our S3 destination bucket. It includes the result of the PDF text extraction, as shown below.\nConclusion: By the end of this tutorial, we have successfully created our AWS Lambda Function which extracts texts from uploaded PDFs in a S3 source bucket. The extracted texts are saved in a designated S3 destination bucket. We have achieved this task by utilizing a Custom Layer and a Lambda Function in Python using layers.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/headless/",
	"title": "Setting UP RPI to Run Headless",
	"tags": [],
	"description": "",
	"content": "What is Headless? Headless is the term used when we run our RPI without a monitor or a keyboard. I like to setup and configure my RPI using a monitor at least for the first time only, but if we are hosting a server with no desktop GUI on a RPI, setting up headless is the way to go.\nPlease, refer to Raspberry Pi Foundation for information on how to setup RPI headless\n\rMy RPI does not boot without HDMI connected! I have been there. We would like to remote into our RPIs headless via vnc or ssh..etc. However, we will face some hiccups along the way.\nModify Config.txt to resolves headless and vnc resolution: We need to make minor modifications to the configuration file on our RPI to boot up without monitor. Moreover, we will stumble into resolution hiccups after making this modifications. Here is the resolution:\nRPI running Raspberry Pi OS : The following command to access the configuration text file.\nsudo nano /boot/config.txt\rLet\u0026rsquo;s make the necessary modifications according to the below image.\nTo save the file:\n hold and click: CTRL-x type: y click: enter.  RPI running Ubuntu: The following command to access the configuration text file.\nsudo nano /boot/firmware/config.txt\rThe above process should allow the RPI to boot up without a monitor or keyboard. The resolution distortion should be resolved as well.\nUpdate (6/8/2021): I recently provisioned a new RPI-4 to run Ubuntu Desktop 21.04. I also followed the same steps above to run the RPI-4 without a monitor; however, the RPI-4 had not worked without a monitor. Therefore, I have done my research and the work around relies on the Config.txt file.\nLet\u0026rsquo;s start by accessing the configuration text file using the following command on our terminal:\nsudo nano /boot/firmware/config.txt\rThen, we will perform the following:\n1- Comment out dtoverlay=vc4-kms-v3d\n2- Add the following:\n A. hdmi_force_hotplug=1\rB. framebuffer_width=1920\rC. framebuffer_height=1080\r The Config.txt should like the image below:\nFinally, to save the file in the nano text editor:\n hold and click: CTRL-x type: y click: enter  Conclusion: Now, we are able to run Ubuntu 20.04 on RPI-4 without a need for a monitor. To access the RPI-4 remotely, refer to my tutorial about RealVNC.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/lambda_web/",
	"title": "Application Load Balancer with Lambda Backend",
	"tags": [],
	"description": "",
	"content": "Application Load Balancer An Application Load Balancer has the ability to send traffic to a specific target group (TG) based on path. The default path is to send all the traffic to a given Target Group.. Usually a TG comprises of infrastructure components either on the cloud (EC2 instances) or from on-prem data-center (via IP).\nHowever, there is an exciting feature by which the target group can contain a lambda function as the backend. By the end of this tutorial, we should successfully utilize an ALB to direct traffic to a Lambda function backend. The Lambda function is written in Python and is target group for the ALB.\nPlease note that some parts of the images in this tutorial will be pixilated or hidden to hide all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rArchitecture Implementation - Tutorial Scenario:  Create the lambda function using the console, name it \u0026ldquo;web-backend.\u0026rdquo; Create a lambda IAM role. Create an instance of ALB. When creating the TG, select lambda. Select \u0026ldquo;web-backend\u0026rdquo; lambda from the dropdown. Enable health check. Create the ALB and TG and the wait for status to change from provisioning to active before attempting to access the DNS.  Step 1: Lambda Role  Create an IAM role which allows the lambda function to access other needed resources.  Add the following policy to the role CloudWatchFullAccess.  Step 2: Python Lambda Function Lets' create a lambda function and attach the role we have previously created.\n On the Lambda console, click on \u0026ldquo;Create function.\u0026rdquo; Function name: web_backend Select \u0026ldquo;Author from scratch.\u0026rdquo; Runtime: Python 3.7. Permissions: \u0026ldquo;Use an existing role\u0026rdquo; and select the role we have previously created, Lambda_web_Backend. Click on \u0026ldquo;Create function.\u0026rdquo;  On the lambda function code editor, remove the existing code and replace it with the code from my GitHub repo. Then, click Deploy.  Step 3: Create an Instance of Application Load Balancer  Navigate to EC2 console, select Load Balancers and Create Load Balancer.  Select an Application Load Balancer as the type of a load balancer.  Configure the ALB as shown below. At least, select two AVailability Zones.  On the configuration of the security group, create a new security group and make sure you have port 80 open to receive http traffic.  Add a new target group as a Lambda function and enable Health check. Also, add /lambda/ to the health check path as shown below.  We will select our lambda function from the drop down menu to register it as a target. If you have multiple versions of lambda function, select the version you\u0026rsquo;d like to register as a target.  Review the configuration and then click on Create.  We will need to wait until our ALB status is updated from provisioning to active. Then, we get the DNS name of the ALB and paste it on the browser.  Conclusion: Congratulations, we have successfully employed an Application Load Balancer to direct traffic to our Lambda function backend. This Lambda function is written in Python and is target group for the ALB.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/rpidock/",
	"title": "Docker on Ubuntu 20.04 Raspberry Pi 4",
	"tags": [],
	"description": "",
	"content": "\rAs of 11/25/21, if you were unable to install Docker on RPI-4 running Ubuntu 21.10, you may need to install extra kernel modules by running the following command $ sudo apt install linux-modules-extra-raspi\n\rUpdating the Operating System Now, we have a secured connection with our instance, let\u0026rsquo;s update and upgrade the operating system. Updating and upgrading the operating system of the instance is always a good habit to have. We will run the following update/upgrade commands. The -y in the end of the command line will automatically enters \u0026ldquo;yes\u0026rdquo; as a confirmation before installing the updates.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y\rCleaning up after an update If this is not the packages first time to run an update, there might have some unnecessary packages left for cleaning. Removing these packages will free space and prevent your system from cluttering. The following command shall do the job.\nsudo apt autoremove\rStep 1: Install Docker There are different methods to install Docker Engine. The following approach is the method from the official Docker site.\nWe will setup Docker\u0026rsquo;s repository and then install from them for ease of installation and upgrade tasks as it\u0026rsquo;s the recommended approach.\n1 - Setup the Repository:\n sudo apt-get update\r sudo apt-get install \\\rapt-transport-https \\\rca-certificates \\\rcurl \\\rgnupg \\\rlsb-release\r2- Add Docker\u0026rsquo;s Official GPG Key:\nThe commands below is to download and install Docker’s official GPG key, which is used to validate packages installed from Docker’s repository making sure they’re trusted.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\r\rIf you were unable to download the GPG key, check if you have the CURL command-line utility installed on your Ubuntu 20.04 by typing $ curl \u0026ndash;version. If you need to install the CURL, type in $ sudo apt-get install curl -y\n\rcurl --version\rsudo apt-get install curl -y\r3 - Setup the Stable Repository:\nDocker Engine is supported on x86_64 (or amd64), armhf, and arm64 architectures. We should know that RPI 4 is an arm64 architecture, but the Raspberry Pi OS is a 32-bit operating system. We have decided to utilize Ubuntu 20.04 LTS on our RPI 4 because it\u0026rsquo;s a 64-bit operating system. Therefore, we will need to install Docker Engine that supports 64-bit operating system on an arm64 architecture such as our current Ubuntu 20.04 LTS on RPI 4.\nI would like to suggest studying the differences between amd64, 64 and armhf.\nNow, let\u0026rsquo;s use the following command to setup the stable repository for arm64.\n echo \\\r\u0026quot;deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\r$(lsb_release -cs) stable\u0026quot; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null\r4 - Install Docker Engine:\nFirstly, we will update the apt package index and then install the latest version of Docker Engine and containerd.\nsudo apt-get update\rsudo apt-get install docker-ce docker-ce-cli containerd.io\rLet\u0026rsquo;s verify that we have installed Docker Engine by running the hello-world image.\nsudo docker run hello-world\rThe response should be similar to the below:\nHello from Docker!\rThis message shows that your installation appears to be working correctly.\rTo generate this message, Docker took the following steps:\r1. The Docker client contacted the Docker daemon.\r2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub.\r(arm64v8)\r3. The Docker daemon created a new container from that image which runs the\rexecutable that produces the output you are currently reading.\r4. The Docker daemon streamed that output to the Docker client, which sent it\rto your terminal.\rTo try something more ambitious, you can run an Ubuntu container with:\r$ docker run -it ubuntu bash\rShare images, automate workflows, and more with a free Docker ID:\rhttps://hub.docker.com/\rFor more examples and ideas, visit:\rhttps://docs.docker.com/get-started/\rLet\u0026rsquo;s run the following command to find out the Docker version we have just installed.\ndocker version\rYou might notice that we have received a permission denied message because we have not used sudo in front of the command and we are not the root user. But, what if we don\u0026rsquo;t want to use sudo all the times; in this case, we will assign the non-root user to docker group as follows:\nsudo usermod -aG docker $USER\rNote: exit out of the terminal for the change to take effect, and check the docker verion once more.\nNow, we have installed Docker successfully, let\u0026rsquo;s move on to installing Docker-Compose.\nStep 2: Install Docker-Compose Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration. To learn more about all the features of Compose, see the list of features.\nIf you try to install Docker-Compose from Docker\u0026rsquo;s official website, you will face some hiccups as shown below.\nsudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose\rDocker does not have a release for Docker-Compose for arm64 yet; however, I have a shell script which will allow us to download a compatible Docker-Compose container with arm64 from my GitHub.\nTo install it, run the commands below to download version 1.28.5 As of this writing, this was the current version.\nsudo curl -L --fail https://github.com/OmarCloud20/docker-compose/releases/download/1.28.5/run.sh -o /usr/local/bin/docker-compose\rif 1.28.5 is not th latest Docker-Compose version, you could use the below command to download the latest version.\nsudo curl -L --fail https://github.com/AppTower/docker-compose/releases/download/latest/run.sh -o /usr/local/bin/docker-compose\rLet\u0026rsquo;s apply executable permissions to the binary:\nsudo chmod +x /usr/local/bin/docker-compose\rLet\u0026rsquo;s test our installation.\ndocker-compose --version\rWe should see a result similar to the below.\ndocker-compose version 1.28.5, build c4eb3a1f\rReboot your RPI 4.\nsudo reboot\rSo far, we have installed Docker and Docker-Compose successfully. We will move on to installing Portainer.\nSome useful Docker commands: To list all Docker images:\ndocker images\rTo list all Docker image port information:\ndocker ps\rPrior to removing an image, make sure it\u0026rsquo;s container is not running.\ndocker stop [container ID]\rTo force a removal of a Docker image (portainer/portainer:tag is an example of the image name and tag):\ndocker image rm -f portainer/portainer:latest\rTo force removal of a Docker image by the Image ID.\ndocker rmi -f [image ID]\rStep 3: Install Portainer Now that Docker and Docker Composer are installed, follow the steps below to get Portainer setup.\nYou can use Docker command to deploy the Portainer Server; note the agent is not needed on standalone hosts, however, it does provide additional functionality if used.\nTo get the server installed, run the commands below. Firstly, let\u0026rsquo;s get to home directory.\ncd ~/\rLet\u0026rsquo;s run the following command to download the Portainer image that is compatible with our RPI 4 64arm processor.\ndocker pull portainer/portainer-ce:latest\rIf you download a Docker image that is incompatible with the RPI 4 arm64 architecture, you will get a warning once you try to create a container as shown below.\nWARNING: The requested image's platform (linux/arm/v7) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\rWe will create Portainer data volume first.\ndocker volume create portainer_data\rNow, we will create a container which will run Portainer. We will dedicate port 9000 to Portainer; however, you could allocate another port if you\u0026rsquo;re using port 9000 with another application.\nsudo docker run -d -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce\rYou’ll just need to access the port 9000 of the Docker engine where Portainer is running using your browser.\nNote: the -v /var/run/docker.sock:/var/run/docker.sock option can be used in Linux environments only.\nAfter a successful pull, you should get a similar message as below:\nlatest: Pulling from portainer/portainer\rd1e017099d17: Pull complete a7dca5b5a9e8: Pull complete Digest: sha256:4ae7f14330b56ffc8728e63d355bc4bc7381417fa45ba0597e5dd32682901080\rStatus: Downloaded newer image for portainer/portainer:latest\r2fd5f4a0883a9d358ad424fd963699445be8839f3e6a2cf73d55778bcc268523\rAt this point, all you need to do is access Portainer portal to manage Docker. Open your web browser and browse to the server’s hostname or IP address followed by port #9000\nhttp://localhost:9000 or http://your_EC2_instance_public_ip_adress:9000\nYou should get Portainer login page to create an admin password.\nSubmit a new password.\nNow, you see some options to choose the environment you want to manage. Since we installed Docker on the same instance, select to connect and manage Docker locally.\nYou’ll be directed to Portainer dashboard where you can start managing Docker. If you see a notification for upgrade, click on it and proceed with the upgrade process.\nConclusion: By the end of this tutorial, we have successfully installed Portainer Docker management UI on our Ubuntu 20.01 RPI 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/psswd/",
	"title": "How To Reset My Portainer Password",
	"tags": [],
	"description": "",
	"content": "Resetting Admin password in Portainer running as container Portainer does not have a password reset feature. We will have to you use a password container helper.\n1- Firstly, we will stop the Portainer container.\ndocker container stop portainer\r2- We will run the helper using the same bind-mount/volume for the data volume\ndocker run --rm -v portainer_data:/data portainer/helper-reset-password\rThe response should be similar to the below.\n2020/06/04 00:13:58 Password succesfully updated for user: admin\r2020/06/04 00:13:58 Use the following password to login: \u0026amp;_4#\\3^5V8vLTd)E\u0026quot;NWiJBs26G*9HPl1\r3- Finally, we will restart the Portainer container.\ndocker container start portainer\rNow, we have our username and password to log in into Portainer portal. "
},
{
	"uri": "https://omar2cloud.github.io/rasp/docker-on-ec2/",
	"title": "Basic Docker Commands",
	"tags": [],
	"description": "",
	"content": "Basic Commands of Docker The heart of the Docker echo system consists of two parameters: the images and the containers.\nTo list out the docker processors:\nps -ef | grep [d]ocker\rTo list out all images on the local machine:\ndocker images\rTo check for any running containers/processors:\ndocker ps -a\rDocker run reference - Foreground vs. Detached Foreground In foreground mode, docker run can start the process in the container and attach the console to the process’s standard input, output, and standard error.\nTo create a new container from the docker image and remove it once its executed. If the image busybox does not exist locally, it will be pulled from the Docker Hub, create a container and execute a command \u0026ldquo;Hello Docker\u0026rdquo;:\ndocker run --rm busybox:latest /bin/echo \u0026quot;Hello Docker\u0026quot;\rNote: busybox is a tiny utility image used for testing\nLet\u0026rsquo;s check the docker images:\ndocker images\rNow, let\u0026rsquo;s check whether there is a container running.\ndocker ps -a\rYou might notice the docker container busybox was not running because it was removed as per our previous command. Also, note that the image itself won\u0026rsquo;t be removed, just the container. However, if we run the same command without the \u0026ndash;rm, the container will be persistent.\nTo remove a container, you could use the container\u0026rsquo;s name:\ndocker rm modest_heisenberg\rOr the container\u0026rsquo;s ID:\ndocker rm 61d53262896d If you decided to use the container\u0026rsquo;s ID, you don\u0026rsquo;t have to type the entire ID; only the unique first few numbers should be sufficient. However, using the name is more convenient\nThe preferred storage driver, for all currently supported Linux distributions, and requires no extra configuration is overlays. This is where Docker saves the images.\nsudo ls -al /var/lib/docker/overlay2\rCreating multiple containers from the same image scenario sudo run -it --rm busybox:latest\rNote that the (-it) is for interactive terminal\nNow, we have logged in into the container. If we run a ps -ef command, we shall see the processors. We could also run the command ls -al to see all directories in this container.\nWe can also interact with the container and create a directory using mkdir command and create a text file named test.txt. Then, we will check with the text file is created successfully and exit the container.\nmkdir test\rcd test/\recho \u0026quot;This is just a sample text file for testing\u0026quot; \u0026gt; test.txt\rls -al\rNow, if you go back and run docker ps -a, you won\u0026rsquo;t find busybox container running. Notice that containers are ephemeral; however, there are ways to make the file system durable such as using volume mapping.\nDetached To start a container in detached mode, we will use -d=true or just -d option. By design, containers started in detached mode exit when the root process used to run the container exits, unless you also specify the \u0026ndash;rm option. If you use -d with \u0026ndash;rm, the container is removed when it exits or when the daemon exits, whichever happens first.\nLet\u0026rsquo;s run a Centos image in detached mode:\ndocker run -d centos tail -f /dev/null\rLet\u0026rsquo;s check if the image was downloaded successfully:\ndocker images\rdocker ps -a\rLet\u0026rsquo;s check on the processor:\nps -ef | grep [d]ocker\rWe can stop the processor using the process ID:\nNote 1139 is the process ID\nExample:\nsudo kill -9 [process ID]\rActual:\nsudo kill -9 1139\rThe status of the container has changed to Existed. Now, let\u0026rsquo;s run another container from the same Docker Centos image. We can run multiple containers from the same image independently.\nTo get into the container:\nExample:\ndocker exec -it [Container Name] bash\rActual:\ndocker exec -it heuristic_wilbur bash\rWe will attempt to stop our Centos container and then remove the two Centos containers.\nTo stop a container:\ndocker stop heuristic_wilbur To remove containers:\ndocker rm heuristic_wilbur funny_galileo\rTo remove the images:\ndocker images\rExample:\ndocker rmi [Image ID]\rActual:\ndocker rmi a0477e85b8ae\r"
},
{
	"uri": "https://omar2cloud.github.io/rasp/dynamodb/",
	"title": "AWS DynamoDB on Raspberry Pi 4 Running Ubuntu 21.04",
	"tags": [],
	"description": "",
	"content": "What Is Amazon DynamoDB? Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. With DynamoDB, we can create database tables that can store and retrieve any amount of data and serve any level of request traffic. In addition to the Amazon DynamoDB web service, AWS provides a downloadable version of DynamoDB that we can run on our computers and is perfect for development and testing of our code.\nThis tutorial will walk you through how to deploy DynamoDB locally on a Raspberry Pi 4 running Ubuntu 21.04 using Docker-Compose version 3. We will also deploy DynamoDB-Admin, which is a GUI (Graphical User Interface) for DynamoDB. It\u0026rsquo;s a great tool to interact with DynamoDB visually and is built by Aaron Shafovaloff. For more information about Dynamodb-Admin, here is the link to the GitHub repo. And, for more information about Amazon DynamoDB-local Docker image, here is the link to Amazon Documentation.\nIf you don\u0026rsquo;t have Docker, Docker-Compose and Portainer installed on your RPI-4, I\u0026rsquo;d suggest you install them first. Please, refer to this Tutorial.\n\rInstalling DynamoDB and DynamoDB-Admin locally using Docker-Compose v3: There is more than one way to skin a cat (I\u0026rsquo;ve always wondered why does it have to be a poor cat). I have tried to spin a docker container for DynamoDB on my RPI-4, and then installed the DynamoDB-Admin locally on my RPI-4 (not a container). I have also tried to run a Docker-Compose for DynamoDB and DynamoDB-Admin images using Portainer. However, I had faced some hiccups because the community version of Portainer currently does not support Docker-Compose 3 (as of writing this tutorial 11/14/21).\nAlthough there are multiple ways to install the DynamoDB locally on our RPI-4, I have found the best way that fits my needs is by using Docker-Compose version 3 (by creating a yml file). The following steps will walk you through how to run DynamoDB and DynamoDB-Admin locally using Docker-Compose version 3. This method works great for me. Then, we will interact with DynamoDB using AWS Python SDK - Boto3 and create a table.\nStep 1: Creating Folders  Create a DynamoDB folder on your RPI-4 Desktop or on any other directory of your choosing. This would be our project folder. Create a folder inside the DynamoDB folder and name it dynamodb-volume. We will use this folder to host our DynamoDB database down the road. Create a file named docker-compose.yml inside the DynamoDB (not inside the dynamodb-volume). We will run this file to spin our containers later.  You have the option to create these folders and docker-compose file programmatically using the CLI (Command Line Interface) or using the GUI (Graphical User Interface) and any text editor of your choosing. The hierarchy should look like the below image.\nStep 2: Composing Docker-Compose.yml File  Add this text to the Docker-Compose.yml file and read the below discussion prior to saving the file.  version: '3.8'\rservices:\rdynamodb-local:\rcommand: \u0026quot;-jar DynamoDBLocal.jar -sharedDb -port 8000 -dbPath ./data\u0026quot;\rimage: amazon/dynamodb-local:latest\rcontainer_name: dynamodb-local\rports:\r- \u0026quot;8000:8000\u0026quot;\rvolumes:\r- \u0026quot;/home/pi/Desktop/DynamoDB/dynamodb-volume:/home/dynamodblocal/data\u0026quot;\rworking_dir: /home/dynamodblocal\rdynamodb-admin:\rimage: omarcloud20/dynamodb-admin-arm64\rcontainer_name: dynamodb-admin\rports:\r- \u0026quot;8001:8001\u0026quot;\renvironment:\rDYNAMO_ENDPOINT: \u0026quot;http://dynamodb-local:8000\u0026quot;\rAWS_REGION: \u0026quot;us-east-1\u0026quot;\rAWS_ACCESS_KEY_ID: local\rAWS_SECRET_ACCESS_KEY: local\rdepends_on:\r- dynamodb-local\r\rIf you have created the dynamodb-volume folder in a different directory, you may need to point to the directory location in the volumes line: \u0026ldquo;/home/pi/Desktop/Dynamodb/dynamodb-volume\u0026rdquo;\n\rPrior to moving to the next step, let\u0026rsquo;s have a brief discussion about the parameters of this file.\n Although we don\u0026rsquo;t have to add the command variables, We have added them just in case we need to modify them in the future such as changing the port number. To have data persistency and database accessability, we have added a persistent volume (dynamodb-volume). By doing so, we have ensured that our DynamoDB tables won\u0026rsquo;t get deleted if we delete the DynamoDB container. Moreover, we have access to the database in case we decide to move it to a different host. For my case, it\u0026rsquo;s ideal and better than creating a Docker volume for it. The current DynamoDB-Admin image is amd64 architecture which is incompatible with our RPI-4 arm64 architecture. Therefore, I had to build a RPI-4 compatible image (omarcloud20/dynamodb-admin-arm64) for the DynamoDB-Admin application. The environment variables AWS_REGION, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are required dummy variables to run the Docker-Compose but are not required to be valid AWS keys to access DynamoDB Local. By default, the DynamoDB local is listening on port 8000 and the DynamoDB-Admin is on port 8001.  Step 3: Run Docker-Compose  Let\u0026rsquo;s cd into the directory where we have saved docker-compose.yml file. In my case, it\u0026rsquo;s /home/p4/Desktop/DynamoDB. Now, let\u0026rsquo;s run the below command to execute docker-compose. Note, the flag -d is used to keep docker-compose running in the background.  docker-compose up -d\rStep 4: Access DynamoDB-Admin UI  Let\u0026rsquo;s open a browser on the RPI-4 and type in http://localhost:8001, we should be able to see the DynamoDB-Admin UI. We could also access the DynamoDB-Admin UI from any other device on the same network by typing in the browser the IP address of the RPI-4 and port 8001, http://rpi4-ip-address:8001. For example, http://192.168.1.131:8000  We could also verify the status of the DynamoDB and DynamoDB-Admin from Portainer as shown below.  Step 5: Install and Configure AWS CLI and Boto3  Let\u0026rsquo;s install AWS CLI on our RPI-4:  sudo apt install awscli -y\r\rIf you would like to install AWS CLI version 2 instead of version 1, here is the link to Amazon installation document.\n\rNow, we need to to configure AWS CLI by running the below command:  aws configure\rAs part of the configuration process, we will be prompted for the following:\n AWS Access Key ID: type local AWS Secret Access Key: type local Default region name: type us-east-1 Default output format: \u0026ldquo;leave empty and hit enter\u0026rdquo;  The AWS SDK for Python is called Boto3. We will use it to write a Python code to interact with our locally hosted DynamoDB. For more information about Boto3. Now, let\u0026rsquo;s run the following command to install boto3 on our RPI-4.  pip install boto3\ror\npip3 install boto3\r\rIf you encountered any issues, run this command to find out whether or not you have pip installed: pip --version or pip3 --version. If you don\u0026rsquo;t have pip installed, please refer to pip installation instructions.\n\rStep 6: Getting Started Developing with Python and DynamoDB  Let\u0026rsquo;s write a simple Python code to create a table called Movies. Once again, you have the option to create the Python file using the CLI or the GUI. Let\u0026rsquo;s use the CLI and cd into our DynamoDB folder. Then, let\u0026rsquo;s use nano text editor to create a Python file name MoviesCreateTable.py.  nano MoviesCreateTable.py\rWe will copy the below code and past it onto nano and save the file by:   Ctrl + x (control and x) type in y Finally hit enter  def create_movie_table(dynamodb=None):\rif not dynamodb:\rdynamodb = boto3.resource('dynamodb', endpoint_url=\u0026quot;http://localhost:8000\u0026quot;)\rtable = dynamodb.create_table(\rTableName='Movies',\rKeySchema=[\r{\r'AttributeName': 'year',\r'KeyType': 'HASH' # Partition key\r},\r{\r'AttributeName': 'title',\r'KeyType': 'RANGE' # Sort key\r}\r],\rAttributeDefinitions=[\r{\r'AttributeName': 'year',\r'AttributeType': 'N'\r},\r{\r'AttributeName': 'title',\r'AttributeType': 'S'\r},\r],\rProvisionedThroughput={\r'ReadCapacityUnits': 10,\r'WriteCapacityUnits': 10\r}\r)\rreturn table\rif __name__ == '__main__':\rmovie_table = create_movie_table()\rprint(\u0026quot;Table status:\u0026quot;, movie_table.table_status)\rNow, we are ready to run the Python code using the following the command:  python3 MoviesCreateTable.py\r If we refresh the browser, you shall see our newly created table on the DynamoDB-Admin UI as shown below. Hooryah.   We can also create a DynamoDB table from the DynamoDB-Admin UI. Let\u0026rsquo;s create one.\n  Let\u0026rsquo;s verify that all tables are stored in our dynamodb-volume. By running the following AWS command, we will get a list of all of our local DynamoDB tables:  aws dynamodb list-tables --endpoint-url http://localhost:8000 --region us-east-1\rNote that the Movies table was created by running the Python code using boto3 and the Omar_Table was created on the DynamoDB-Admin UI.\nFor more about developing with Python and DynamoDB such as loading data, query and scan tables, please refer to Amazon documentation.\n\rConclusion: By the end of this tutorial, we have successfully configured Docker containers for AWS DynamoDB-local and DynamoDB-Admin application using Docker-Compose version 3 on our RPI 4 running Ubuntu 21.04 OS. We have also created DynamoDB tables using AWS Python SDK (boto3) and DynamoDB-Admin UI. This concludes our tutorial and remember learning never stops, cheers.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/prj2/",
	"title": "Invoice Processing Application",
	"tags": [],
	"description": "",
	"content": "Invoice Processing Application The Invoice Processing Application is to parse the content of the uploaded text format invoices to S3 bucket using a Python custom code running on a Ubuntu EC2 to convert them into CSV records. Once a record is processed, it will be saved in DynamoDB for retention and the converted CSV record is saved in S3 destination bucket. AWS Athena is to query the CSV records to aggregate expenses grouped by date.\nPlease note that some parts of the images in this tutorial will be pixilated or hidden to hide all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rArchitecture Implementation - Tutorial Scenario:  The customer uploads the invoice data to S3 bucket in a text format as per their guidelines and policies. This bucket will have a policy to auto delete any content that is more than 1 day old (24 hours). An event will trigger in the bucket that will place a message in SNS topic A custom program running in EC2 will subscribe to the SNS topic and get the message placed by S3 event The program will use S3 API to read from the bucket, parse the content of the file and create a CSV record along with saving the original record in DynamoDB The program will use S3 API to write CSV record to destination S3 bucket as new S3 object. Athena is used to query the CSV file (query to show aggregated expenses grouped by date).  Step 1: S3 Buckets  Navigate to S3 using the Services button at the top of the screen. Select \u0026ldquo;Create Bucket\u0026rdquo; Enter a source bucket name and use the default options for the rest of the fields. Repeat 1 to 3 steps to create a target bucket  Step 2: SNS Subscription Lets' create a lambda function and attach the role we have previously created.\n Navigate to SNS -\u0026gt; Topics -\u0026gt; Standard Click on \u0026ldquo;Create Topic\u0026rdquo; Enter the following fields Name: S3toEC2Topic Click on Create Topic  Let\u0026rsquo;s modification of SNS Access Policy as follows:  {\r\u0026quot;Version\u0026quot;: \u0026quot;2008-10-17\u0026quot;,\r\u0026quot;Id\u0026quot;: \u0026quot;__default_policy_ID\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;__default_statement_ID\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Principal\u0026quot;: {\r\u0026quot;AWS\u0026quot;: \u0026quot;*\u0026quot;\r},\r\u0026quot;Action\u0026quot;: \u0026quot;SNS:Publish\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:sns:us-east-1:Your AWS Account Number:S3toEC2Topic\u0026quot;,\r\u0026quot;Condition\u0026quot;: {\r\u0026quot;StringEquals\u0026quot;: {\r\u0026quot;aws:SourceAccount\u0026quot;: \u0026quot;Your AWS Account Number\u0026quot;\r},\r\u0026quot;ArnLike\u0026quot;: {\r\u0026quot;aws:SourceArn\u0026quot;: \u0026quot;arn:aws:s3:*:*:\u0026quot;S3 Source Bucket Name\u0026quot;\r}\r}\r}\r]\r}\rNote: A. Replace SNS-topic-ARN as shown below.\nB. Replace bucket-name with your source bucker name as shown below.\nC. Replace bucket-owner-account-id with yours as shown below.\nStep 3: Configuring SNS Notification for S3   Navigate to S3 and select the source bucket, which we have previously created.\n  Select Properties and scroll down to Event Notifications and select it.\n  Follow the following details:\nA. Name: S3PutEvent B. Select PUT from the list C. Destination: SNS Topic D. SNS: S3toEC2Topic\n  Save changes.\n  Step 4: Spin an EC2 Instance to Run the Custom Code   Navigate to EC2 -\u0026gt; Instances\n  Create an EC2 instance with the following parameters:\nA. AMI: Ubuntu 18.04 LTS B. VPC: Default C. Security Group: Open ports 22 and 8080\n  Note: to enhance the security, you could select opening port 22 ONLY to your home public IP address. This will limit the SSH access to request coming from your home IP address.\nStep 5: IAM role for the EC2 instance   Navigate to IAM -\u0026gt; Roles -\u0026gt; Create role\n  Attach the following two policies:\nA. AmazonS3FullAccess\nB. AmazonDynamoDBFullAccess\n  Name the role, EC2S3Access\n  Let\u0026rsquo;s go back to the EC2 and attach this role to it as shown below.  Step 6: Uploading the Python Code   Download the application zip file from my GitHub repo and save it to your local device.\n  Open the code using any code editor of your choice. I\u0026rsquo;m using Visual Studio Code as a code/text editor.\n  Unzip the file and navigate to docproc-new -\u0026gt; api -\u0026gt; views.py.\n  Open views.py with your choice of code editor and ensure that:\nA. The name of the S3 target bucket must match the name of our target bucket\u0026rsquo;s name (line 19).\nB. The region should match your region. Mine is us-east-1.\n  Then, let\u0026rsquo;s save the python file.\nNow, we need to copy folder docproc-new from our local device to the home folder of our Ubuntu EC2 instance using scp command as shown below.  Note: first you may need to navigate (cd) to where you have saved the pem file.\nscp -i \u0026lt;Ubuntu pem file\u0026gt; ./docproc-new ubuntu@\u0026lt;Ubuntu public ip\u0026gt;:/home/ubuntu\rIf your transfer is successful, you shall see a message similar to the below.\nStep 7: Source Code Installation   Let\u0026rsquo;s SSH into our Ubuntu EC2 instance. If you need help SSH\u0026rsquo;ing into the instance, please refer to my previous tutorial about SSH.\n  Now, we will run the following commands on our Ubuntu instance to update the operating system, install Python, Django, Boto3 and a virtual environment:\n  sudo apt update\rsudo apt install python-pip -y\rpython -m pip install --upgrade pip setuptools\rsudo apt install virtualenv -y\rvirtualenv ~/.virtualenvs/djangodev\rsource ~/.virtualenvs/djangodev/bin/activate\rpip install django\rpip install boto3\rLet\u0026rsquo;s create a directory and copy the source code.  sudo cp -r docproc-new /opt\rsudo chown ubuntu:ubuntu -R /opt\rcd /opt/docproc-new\rNow, let\u0026rsquo;s run the python code:  python manage.py runserver 0:8080\rNote: make sure the development server is running on the CLI/Terminal\nStep 8: SNS Subscription   Let\u0026rsquo;s navigate to SNS in the AWS Console and select the topic S3toEC2Topic\n  Click on Create Subscription\n  Enter the following details:\nProtocol : HTTP\nEndpoint : http://IP-address:8080/sns, where the IP Address is the public IP of the EC2 instance.\n  In the EC2 terminal window, look for the field \u0026ldquo;SubscribeURL\u0026rdquo; and copy the entire link given without the prentices. This should be the token used to confirm the subscription.  Note: If a message is seen \u0026ldquo;ValueError: No JSON object could be decoded\u0026rdquo;, it can be safely ignored.\nPaste that URL into a browser window, http://\u0026lt;instance public IP address\u0026gt;:8080/sns*, to verify the SNS subscription (Ignore any messages received in the web browser).  When EC2 instance is stopped, its public IPv4 is released, and a new IPv4 is assigned to the instance once it starts back up. To read more about AWS Stop and start your instance.\n\rStep 9: Generation of CSV File In the project folder, we have docproc-invoice.txt which we will upload to our S3 source bucket.\n Navigate to S3 in the AWS Console. Upload the sample invoice file to the source S3 bucket using the default options.  *Note: you should be able to track the processing of the file on command line/terminal as shown below.\nVerify that a CSV file is generated in the target S3 bucket.  Step 10: Create a Table in DynamoDB  Navigate to DynamoDB using the Services Menu. Click on tables on the left side. Select the table \u0026ldquo;invoice\u0026rdquo; Click on the \u0026ldquo;Items\u0026rdquo; tab and verify that a record has been created in the table with the contents of the invoice file.  Step 11: AWS Athena Querying CSV File  Navigate to AWS Athena in the AWS Console. Paste the following command in the query editor and Run query to create a database: create database proj2db;  Run the following query to create the table based on the generated CSV file:  Note: add your S3 target bucket name instead of \u0026lt;target S3 bucket\u0026gt;.\nCREATE EXTERNAL TABLE IF NOT EXISTS proj2db.invoice (\r`customer-id` string,\r`inv-id` string,\r`date` string,\r`from` string,\r`to` string,\r`amount` float,\r`sgst` float,\r`total` float,\r`inwords` string\r)\rROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\rWITH SERDEPROPERTIES (\r'serialization.format' = ',',\r'field.delim' = ','\r) LOCATION 's3://\u0026lt;target S3 bucket\u0026gt;/'\rTBLPROPERTIES ('has_encrypted_data'='false');\rPaste the following query and run it to show aggregated expenses by date:  SELECT sum(\u0026quot;amount\u0026quot;), \u0026quot;date\u0026quot; FROM \u0026quot;proj2db\u0026quot;.\u0026quot;invoice\u0026quot; GROUP BY \u0026quot;date\u0026quot;;\rShown below is the output of the Athena query to show aggregated expenses by date. Since we have only uploaded one invoice, we don\u0026rsquo;t have lots of the invoices to query.\nConclusion: By the end of this tutorial, we have successfully created an Invoice Processing Application web. The application has utilized S3, SNS, EC2, DynamoDB and Athena.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/prj3/",
	"title": "Docker Image on ECS Fargate",
	"tags": [],
	"description": "",
	"content": "A Sample Web Application Containerized as a Docker image Deployed on AWS ECS Fargate In this tutorial, we will package and containerize a sample web application as a Docker image running on Apache Tomcat having JRE-8 as a runtime. Then, we will push this new Docker image to our public repository at Dockerhub. The web application, which is packaged into a Docker image will be deployed on AWS ECS Fargate cluster. Finally, to view and access our web application cluster, we will utilize a DNS of a load balancer.\nThe below diagram is our design architecture for this tutorial.\nIt\u0026rsquo;s optional to either utilize Ubuntu EC2 instance or your own device to package and push the Docker image to Dockerhub. I expect that you have Docker installed on your instance or device, if not, please refer to my previous tutorial install Docker.\n\rArchitecture Implementation - Tutorial Scenario:  Download the WAR file from my GitHub Web application needs to be packaged as a Docker image running on Tomcat having JRE8 - you will have to write a Dockerfile Once the image is created, run and verify image by accessing web application using ec2 instance public-ip Sign up for docker hub and create public repository. Tag the image appropriately and push to Docker Hub Repository. Using AWS ECS Fargate create a cluster, task and service(s).  Step 1: Create a Docker image  Navigate to folder /opt and create helloworld folder.  sudo chown ubuntu:ubuntu -R /opt\rcd /opt\rmkdir helloworld\rcd helloworld/\rDownload the War file from my GitHub to /opt/helloworld  wget https://github.com/OmarCloud20/aws-tutorials/raw/main/prj3/HelloWorld.war\rCreate a Docker file in the helloworld folder.  cd /opt/helloworld/\rsudo nano Dockerfile\rThen, add or type the following sentences into the Dockerfile. Note, you could add your name and email address as a maintainer.\nFROM tomcat:jre8\rMAINTAINER [your name]\rCOPY HelloWorld.war /usr/local/tomcat/webapps\rNote: to save the file in nano text editor, hold ^x, type yes and click enter.\n Verify that the file is created.  ls -al\rBuild the Docker image using the Docker build command:  docker build -t helloworld .\rNote: if you receive any permission error during the build, add sudo to the command line\nsudo docker build -t helloworld .\rRun the image created above using the command given below. We will use -p to port mapping port 8080 of the container to port 80 of the host:  docker run -d -p 80:8080 helloworld\r Let\u0026rsquo;s check whether or not our container is running:  docker ps -a\r Now, let\u0026rsquo;s verify that our application can be accessed from the device using the URL:\n\u0026lt;public IP address of instance\u0026gt;/HelloWorld\u0026gt;\n  Step 2: Push the Packaged Web Application Docker Image to Docker Hub  Create a new free account at Docker Hub. Create a new public repository in your Dockerhub account.  Login to the Dockerhub account from the CLI using the command below:  Example:\ndocker login --username=\u0026lt;DockerHub username\u0026gt;\rActual:\ndocker login --username=omarcloud20\rNote: you will be prompted to enter your password.\nWe will need to tag the image which we will upload to Docker hub using the Image ID, then we will use the tag command as shown below:  To find the Image ID:\ndocker images\rThen, we use the tag command:\nExample:\ndocker tag \u0026lt;image id\u0026gt; \u0026lt;dockerhub username\u0026gt;/\u0026lt;repository name\u0026gt;:latest\rActual:\ndocker tag c0bd35d35ced omarcloud20/project3:latest\rFinally, we will push the image to Dockerhub:  Example:\ndocker push \u0026lt;image id\u0026gt; \u0026lt;dockerhub username\u0026gt;/\u0026lt;repository name\u0026gt;:latest\rActual:\ndocker push c0bd35d35ced omarcloud20/project3:latest\r Verfiy that the image is uploaded to your public dockerhub account.  Step 3: Create an AWS ECS cluster to run the Docker image   Navigate to the ECS service in your AWS account and click Get Started.\n  Under Container and Task Definitions, select Custom-\u0026gt;Configure, and enter the following values:\nA. Container name. Mine is project3.\nB. Image location: docker.io/dockerhub username/dockerhub repository:latest\nC. Memory Limits: Soft limit - 256MB\nD. Port mappings: 8080 - tcp\nE. Click update and hen Next\n  On the defining the Service, select Application Load Balancer and click Next.  On the defining the Cluster, enter a Cluster name and click Next.  On the final review, click Create.  AWS ECS Fargate will create all necessary services and the cluster on your behalf. If you have encounter any errors, you may need to repeat the process one more time.  Step 4: Verification of Running Container on ECS Fargate   Navigate to EC2 using the Services Menu.\n  Navigate to Load balancer.\n  Select the Load Balancer that has been created by the ECS Cluster.\n  Take note of the DNS of the load balancer and visit the below URL to verify that the ECS cluster is running the container:\n\u0026lt;DNS of load balancer\u0026gt;:8080/HelloWorld\n  Conclusion: Congratulations, we have successfully packaged a sample web application into a Docker image utilizing Apache Tomcat and JRE-8 runtime. From our newly created Dockerhub public account, we pushed the new image to AWS ECS Fargate cluster.\n"
},
{
	"uri": "https://omar2cloud.github.io/rasp/gitlab_runner/",
	"title": "GitLab Runner on RPI-4 (Build, Push Docker images to Docker Hub using GitLab Runner on GitLab)",
	"tags": [],
	"description": "",
	"content": "What is GitLab Runner GitLab Runner is an agent that runs GitLab CI/CD (Continues Integration/Continuous Deployment) jobs in a pipeline. It\u0026rsquo;s heavily utilized in the world of DevOps to provision and configure infrastructure. The GitLab Runner can be installed as a binary on Linux, MacOS or Windows. It can also be installed as a container.\nOn this tutorial, I will walk through installing and configuring GitLab Runner as a container using a Docker image on a RPI-4..yaay. I will make it very swift to get you started and won\u0026rsquo;t feel a thing. I will not bore you with details, but if there are useful links for further study, I will definitely throw it in. The goal is to get you started with GitLab Runners and the rest is on you.\nAs a bonus, we will run our first job of building docker images and push them to Docker hub. Enough reading, let\u0026rsquo;s get our hands dirty, I mean our keyboards 😉\nTo lean more about GitLab Runners, refer to GitLab official documentation.\nSensitive information will be pixilated or erased. This should not alter the quality of the tutorial.\n\rRun a GitLab Runner Container on a RPI-4 If you don\u0026rsquo;t have Docker installed on your RPI-4, you may refer to my Docker on Ubuntu 20.04 Raspberry Pi 4 tutorial.\n Let\u0026rsquo;s create a persistent Docker volume for the runner. On your RPI-4 terminal, run the following command:  docker volume create gitlab-runner-volume\rNote: you can change the volume name gitlab-runner-volume to any name of your chosen, but you should be consistent as we will use the volume name to bind the container to the RPI-4 local host.\nRun the below commands to start GitLab Runner container:  docker run -d --name gitlab-runner --restart always --env TZ=US \\\r-v gitlab-runner-volume:/etc/gitlab-runner \\\r-v /var/run/docker.sock:/var/run/docker.sock \\\rgitlab/gitlab-runner:alpine\rThe only parameters you have the options to change are:\n the name flag, gitlab-runner the volume name, gitlab-runner-volume the gitlab runner image, gitlab/gitlab-runner:alpine the env flag, TZ=US  For more information about the installation process, here is the link GitLab\u0026rsquo;s official documentation.\nCapture the GitLab Runner token:  We do need to head to our GitLab account and grab the runner\u0026rsquo;s token. If you don\u0026rsquo;t have a GitLab account, you can create one for free. Here is the link.\nNow, we need to create a repository to host our project. From GitLab, let\u0026rsquo;s click on create New project. On the Create new project, select create blank project. Then, give the project a name and click Create project. In my case, I named my repo, GitLab-Runner-RPI-4.\nFrom the repository, let\u0026rsquo;s click on settings, CI/CD and then Expand on the Runners section as shown below.\n Capture the token and save it on a note pad. We will need the token for the next step. Disable Enable shared runners for this project.  Register the GitLab Runner:  Replace the token place holder after registration-token with the one from our note pad, and then run the following commands on your RPI-4 terminal:\ndocker run --rm -it -v gitlab-runner-volume:/etc/gitlab-runner gitlab/gitlab-runner:alpine register -n \\\r--url https://gitlab.com/ \\\r--registration-token GR1348941EDhyNWqfxPttukrGVKJd \\\r--executor docker \\\r--description \u0026quot;My Docker Runner\u0026quot; \\\r--docker-image \u0026quot;docker:20.10.12-dind-alpine3.15\u0026quot; \\\r--docker-privileged \\\r--docker-volumes \u0026quot;/certs/client\u0026quot;\rIf everything went smooth, you should not exhibit no errors as shown below.\nTo confirm that the GitLab Runner container is running, run the below Docker command:\ndocker ps -a\rAlright, this is a great indication that we have successfully configured GitLab Runner on RPI-4. Now, we are ready to make some actions 💥\nBuild and Push Docker images to Docker Hub using GitLab Runner on GitLab  First of all, we need get a token from our Docker hub account to avoid using account password. The token is to allow GitLab Runner to authenticate and push Docker images to our Docker hub repository. If you don\u0026rsquo;t know what Docker hub is, it is the world\u0026rsquo;s largest library for container images.   On Docker hub account settings, click on Security, New Access Token and generate a Read, Write, Delete token. Docker hub free account allows for one active token. Capture the token and save to a note pad.  From GitLab repo, which have we have created previously:   Click on Clone and copy the Clone with HTTPS link. On your RPI-4 terminal and in a folder of your chosen, run the below command. This is where the repo will be saved locally:  git clone `your-Clone-with-HTTPS-link`\rNote: the git clone command will prompt you to enter your GitLab credentials for authentication.\ncd into the repo and either use a text editor or the terminal to edit the .gitlab-ci.yml file. The file should only contain the following code. Be very careful with the indentation. Lastly, save the file:  image: docker:19.03.12\rvariables:\rIMAGE_NAME: \u0026quot;test:1.0.0\u0026quot;\rDOCKER_TLS_CERTDIR: \u0026quot;/certs\u0026quot;\rservices:\r- docker:19.03.12-dind\rbefore_script:\r- docker info\rbuild image:\rstage: build script: - docker build -t $REGISTRY_USER/$IMAGE_NAME .\r- docker login -u $REGISTRY_USER -p $DOCKER_HUB_TOKEN\r- docker push $REGISTRY_USER/$IMAGE_NAME\rWe will create a simple Dockerfile. The runner will use this Dockerfile to build a Docker image and push it to Docker hub. For right now, we will keep the Dockerfile VERY simple. Once the image is built from the Dockerfile, it will echo Hello World. With your preferred text editor or terminal, create and name a file Dockerfile without any extension. Then save it in the same local repo.  FROM alpine\rRUN echo \u0026quot;Hello World\u0026quot;\rNote: In the local repo, we should have two files, Dockerfile and .gitlab-ci.yml. A README file might be there as well.\nWe will need to head back to GitLab repo to create two variables. On Settings, click on CI/CD, then expand Variables. We will click on Add variable to create two variables:    Key: REGISTRY_USER Value: your Docker hub username NOT the email\n  Key: DOCKER_HUB_TOKEN Value: the token which we generated from Docker hub\n  Note: the flags should be unchecked for simplicity for the two variables.\nNow, we push the local repo to GitLab repo (remote repo) to start the GitLab Runner pipeline process. Yes, once the remote repo is updated, the runner will be triggered. Let\u0026rsquo;s get back to our terminal on the RPI-4 and from within the local repo, run the following git commands:  git add -A\rgit commit -am \u0026quot;first test\u0026quot;\rgit push\rIf you head back to our GitLab repo and click on CI/CD under Pipelines, you would notice that the pipeline is running.\nMoreover, if you click on the running status and you would be directed to the current stage, which is Build.\nHere, click on build image, you shall see more details about the current build status. The goal is to see Job succeeded. Once you see Job succeeded and passed status in green, you can start doing your victory dance 👯‍♀️ 👯‍♀️\nIf we head back to Docker hub account, we should see that our Docker image test:1.0.0 has been successfully pushed to the repo.\nDocker hub allows Personal (free account), one private repository and unlimited number of public repositories. Therefore, if you get denied message to access the resource on the pushing images to Docker hub step, ensure your repository is public.\n\rConclusion: By the end of this tutorial, we have successfully configured a GitLab Runner on a RPI-4, created a GitLab repo and registered GitLab Runner to it. Finally, we created a Hello World Docker image from a Dockerfile and had the runner building the Docker image and pushing it to our Docker hub account. Now, off you go and the sky is the limit.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/rekognition/",
	"title": "Image Analysis with AWS Rekognition",
	"tags": [],
	"description": "",
	"content": "AWS Rekognition Amazon Rekognition is part of AWS cognitive services, which requires no machine learning expertise to use. It\u0026rsquo;s a simple way to analysis images and videos for any application using proven record of high scalability and deep learning technology. Rekognition technology is utilized to identify and detect objects, shapes, people, texts, and activities in media contents. For more about Amazon Rekognition.\nIn this tutorial, we will explore the important aspect of AWS Rekognition and practically detect people and text on sample picture and video utilizing boto3 - AWS Python library..\nTutorial Scenario:   Create an IAM user to access AWS Rekognition service.\n  Install and configure AWS CLI-v2 on Raspberry Pi 4.\nA. Install Python, pip and zip utility.\nB. Install boto3 - AWS Python library.\n  Run aws-rekognition sample application to identify objects, people and texts.\n  Please note that some parts of the images in this tutorial will be pixilated or hidden to hide all sensitive account information..\n\rStep 1: Create an IAM User:  Navigate to Identity and Access Management (IAM) and click Add user. Then, add a user name and select Access type as Programmatic access as shown below.  On the Set permissions screen, select Attach existing policies directly. On the Filter policies, type in the term rekognition and check AmazonRekognitionFullAccess.   As a best practice and a good habit to acquire, add a tag and then click on Next.   Download and save the csv file in a secure location. This file contains the new user credentials needed to sign programmatic requests that you make to AWS.\n  Step 2: Install and configure AWS CLI-v2 on Raspberry Pi 4: Prior to installing AWS CLI-v2, we have to make sure Python and some utilities are installed. As a best practice, let\u0026rsquo;s update our Ubuntu operating system.\nsudo apt update\rTo find more information about our hardware architecture:\nuname -a\rLet\u0026rsquo;s check out whether or not we have Python and pip installed and their versions if we do:\npython3 ---version\rpip --version\rIf we don\u0026rsquo;t have python3 installed, we need to install it. In my case, pip was not installed on my device, so I installed it. pip is the package installer for Python. For more information pip.\nsudo apt install python3-pip\rLet\u0026rsquo;s install the zip utility to unzip files, if you don\u0026rsquo;t have it installed.\nsudo apt install zip -y\rLet\u0026rsquo;s install Python (Boto3) library.\npip install boto3\rInstall the latest AWS Command Line Interface - CLI verizon 2 for Linux ARM architecture. Raspberry Pi is an arm64 architecture.\ncurl \u0026quot;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026quot; -o \u0026quot;awscliv2.zip\u0026quot;\runzip awscliv2.zip\rsudo ./aws/install\rNow, we are ready to configure aws on our RPI-4, which is the fastest way to set up our AWS CLI installation. When you enter this aws configure command, the AWS CLI prompts you for four pieces of information:\n Access key ID Secret access key AWS Region Output format  The access keys consist of an access key ID and secret access key, which are used to sign programmatic requests that you make to AWS.If you don\u0026rsquo;t have access keys, you can create them from the AWS Management Console. As a best practice, do not use the AWS account root user access keys for any task where it\u0026rsquo;s not required.\nLet\u0026rsquo;s start with aws configuration by using the following command in our CLI.\naws configure\rNow, let\u0026rsquo;s setup our file system and add a folder for our sample app. The chown command allows us to change the user and/or group ownership of a given file or directory.\nNote: replace ubuntu by the name of you RPI 4. For example, raspberry:raspberry\nsudo chown ubuntu:ubuntu -R /opt\rWe will cd into opt directory. cd is the command used to move between directories/folders.\ncd /opt\rmkdir is the command to create directories.\nmkdir aws-rekognition\rLet\u0026rsquo;s cd into our sample app folder and download it.\ncd aws-rekognition\rNow, we will download the aws-rekognition sample application in the aws-rekognition folder.\nwget https://github.com/OmarCloud20/aws-tutorials/raw/main/Rekognition/aws-rekognition.zip\runzip aws-rekognition.zip\rIf you don\u0026rsquo;t have the unzip utility, run the following command to install it.\nsudo apt install zip -y\rAs we have downloaded our sample application zip file and unzipped it, we have also confirmed that all files are downloaded successfully in the proper directory, which is aws-rekognition.\nStep 3: Run aws-rekognition sample application: Finally, we completed all necessary configuration and it\u0026rsquo;s time to run our application. We need to cd into our directory as shown below.\ncd /opt/aws-rekognition/\rls Once we run the python code, rekognition.py, we will be prompted to enter the picture name including its file extension. For the below example, the picture is Truck.jpg.\npython3 rekognition.py\rFor rhe second example, the picture is jobs.jpg.\nConclusion: By the end of this tutorial, we have successfully utilized AWS Rekognition service to analyze images. Rekognition is one of AWS cognitive services, and we have made several API calls to it via boto3 - AWS Python library from within a Raspberry Pi 4.\nWithout any machine learning expertise, we have identified and detected objects, shapes, people, texts, and activities in several images successfully.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/codepipeline/",
	"title": "Create a Simple AWS CodePipeline from S3",
	"tags": [],
	"description": "",
	"content": "AWS CodePipeline AWS CodePipeline is a continuous delivery service offered by AWS to model, visualize, and automate the release of applications. This service allows for rapid modeling and configuring of different steps of an application release process. It automates the necessary process of change of application deployment. For information about AWS CodePipeline.\nLearning Outcomes and Tutorial Scenario: In this tutorial, we will create a two-stage pipeline that uses a versioned S3 bucket and CodeDeploy to release a sample application.\n Creating an S3 bucket for the sample application. Spinning Two Amazon Linux EC2 instances and install the CodeDeploy agent. Creating an application in CodeDeploy. Creating our first pipeline in CodePipeline. Upload a second version of our sample application to our S3 bucket and wait for CodePipeline to automate the release.  When AWS S3 is the source provider for a pipeline, zip your source file or files into a single .zip and upload it to the source bucket. Using an unzipped file, may fail downstream.\n\rStep 1: Creating an S3 bucket for a sample application  Navigate to S3 and create a source bucket and enable versioning. Make sure you create a bucket in the region where you intend to create your pipeline.  Next, download a sample application from AWS site and save it into a folder or directory on your local device. Do not unzip the file. Navigate to the newly created S3 source bucker and upload the sample application zip file.  Step 2: Spinning Two Amazon Linux-2 EC2 instance and install the CodeDeploy agent  Navigate to IAM to create a role. Choose Create role, Select type of trusted entity as EC2 and select AmazonEC2RoleforAWSCodeDeploy as a policy.  Enter a name for the role and click on Create role.  3. Next, navigate to create an EC2 and choose Amazon Linux 2 AMI.\nEnter in Number of instances, 2. And, Auto-assign Public IP and choose Enable. In the IAM role, choose the IAM role which we have created in steps 1 and 2. Expand Advanced Details, and in User data, enter the following:  *Note: you will need to replace the region to match your region on the wget command. Mine is us-east-1\n#!/bin/bash\ryum -y update\ryum install -y ruby\ryum install -y aws-cli\rcd /home/ec2-user\rwget https://aws-codedeploy-us-east-1.s3.us-east-1.amazonaws.com/latest/install\rchmod +x ./install\r./install auto\rOn the Add Tags, add a key name to a Value of EC2_Pipeline or any other name of your choice as shown below. This is very important for next steps. Also, on Configure Security Group page, allow ports 22 and 80 communications so we can SSH (if needed) and access the public instance endpoint.  Note: to enhance the security of the SSH access, you may need to limit the source to your own local IP address by selecting My IP on the Source.\nChoose Review and Launch. On the Review Instance Launch page, choose Launch. When prompted for a key pair, name and download the key pair.  It may take a few minutes for the instance to be ready for you to connect to it. Check that your instance has passed its status checks. You can view this information in the Status Checks column.  Step 3: Creating an application in CodeDeploy  Navigate to CodeDeploy menu, choose Applications and Create application. Enter a name for the application. In Compute Platform, choose EC2/On-premises and choose Create application.  To create a deployment group in CodeDeploy  Navigate to IAM to create a role. Choose Create role, Select type of trusted entity as CodeDeploy. The policy suggested would be AWSCodeDeployRole.  Enter a name for the role and click on Create role.   On the page that displays your application, choose Create deployment group.  Enter a name for the Deployment group name. In Service Role, choose a service role that trusts AWS CodeDeploy with. Under Deployment type, choose In-place. Under Environment configuration, choose Amazon EC2 Instances. Choose Name in the Key field, and in the Value field. Under Deployment configuration, choose CodeDeployDefault.OneAtaTime. Under Load Balancer, clear Enable load balancing. You do not need to set up a load balancer or choose a target group for this tutorial. Leave the defaults for the Advanced section and Create deployment group.  Step 4: Creating our first pipeline in CodePipeline To create a CodePipeline automated release process:\n  Navigate to Pipeline menu and Create pipeline.\n  Enter a name for the pipeline.\n  In Service role, choose new service role to allow CodePipeline to create a new service role in IAM. In Role name, the role and policy name both default to this format: AWSCodePipelineServiceRole-region-pipeline_name.\n  Leave the defaults for the Advanced section and then choose Next.\n  In Step 2: Add source stage, in Source provider, choose Amazon S3. In Bucket, enter the name of the S3 bucket we have previously created. In S3 object key, enter the object key with or without a file path, and remember to include the file extension. For example, for SampleApp_Windows.zip, enter the sample file name as shown in this example:  SampleApp_Linux.zip\rChoose Next step and leave defaults for Amazon CloudWatch Events as recommended. Choose Next.  In Step 3: Add build stage, choose Skip build stage, and then accept the warning message by choosing Skip again. Choose Next. In Step 4: Add deploy stage, in Deploy provider, choose AWS CodeDeploy. The Region field defaults to the same AWS Region as your pipeline. In Application name, enter the name your chose previously for the application, or choose the Refresh button, and then choose the application name from the list. In Deployment group, enter the name your chose previously for the deployment group, or choose it from the list, and then choose Next.  In Step 5: Review, review the information, and then choose Create pipeline. The pipeline starts to run. You can view progress and success and failure messages as the CodePipeline sample deploys a webpage to each of the Ubuntu EC2 instances in the CodeDeploy deployment.  To verify your pipeline ran successfully On the Description tab, in Public DNS, copy the address, and then paste it into the address bar of your web browser. View the index page for the sample application you uploaded to your S3 bucket.\nNotice, there are two different public IP addresses for the sample application running on two different EC2 instances.\nStep 5: Upload Version 2 of Sample Application to S3 bucket  We will unzip SampleApp_linux.zip to modify the html file with your choice of code editor. I\u0026rsquo;m using V.S. Code to change the background color as well as adding version 2 to the text as shown below.  Let\u0026rsquo;s re-zip the SampleApp_Linux and upload the zip file to our S3 bucket. Navigate to CodePipeline to witness the deployment.  4.\tOnce the deployment is successful, let\u0026rsquo;s refresh the browser to inspect the version 2 of our sample application. Conclusion: By the end of this tutorial, we have successfully created a simple pipeline in CodePipeline. The pipeline has two stages:\n  A source stage named Source, which detects changes in the versioned sample application stored in the S3 bucket and pulls those changes into the pipeline.\n  A Deploy stage that deploys those changes to two EC2 instances with CodeDeploy.\n  Congratulations!\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/s3/",
	"title": "Create a Static Sample Website on S3 Utilizing AWS CloudFormation",
	"tags": [],
	"description": "",
	"content": "AWS S3 Hosted Website: The objective of tutorial is to host a sample static website on AWS S3, obtain a free domain name from Freenom and assign it to the S3 website and utilize AWS Route 53 as DNS hosting provider. Moreover, an AWS CloudFront distribution is employed to serve the website over AWS\u0026rsquo;s fast content network service with low latency and Lambda@Edge is to add security headers to all web server responses.\nFinally, we will deploy the architecture using AWS CloudFormation to automate the deployment of our website.\nSome parts of the images in this tutorial will be pixilated or hidden to protect all personal information such as account numbers, ip addresses and any other personal or sensitive information.\n\rTutorial Scenario: Part 1: We should complete this portion of the scenario:\n Register a free domain name at Freenom. Please, refer to my previous tutorial - Step 1 to complete this task. Create a hosted zone in Route 53. Let\u0026rsquo;s navigate to Route 53 on AWS console and select Create hosted zone. This service will cost us ONLY $0.50/month. We could use Freenom DNS service;however, we won\u0026rsquo;t be able to use AWS Certificate Manager to obtain the SSL/TLS certificate.  3. Add the domain name and select Public hosted zone. As a best practice, add a tag to this service.\nOnce we have the hosted zone created, AWS provides us with nameservers to route our traffic to. We will replace our assigned nameservers by the ones we have at Freenom for our domain name as shown below.  Part 2: This portion of the tutorial is completed by our CloudFormation template to automate deployment of our architecture as follows:\n Create S3 buckets and upload a sample website. Create an AWS CloudFront distribution to serve website\u0026rsquo;s traffic through Amazon Content Delivery Network (CDN). Utilize AWS Certificate Manager (ACM) and Obtain SSL/TLS certificate to serve the domain\u0026rsquo;s website securely via HTTPS protocol. Utilize Lambda@Edge to add security headers to every server response. Add a CNAME record for the S3 website endpoint in the hosted zone.  CloudFormation Template Configuration:  Navigate to CloudFormation on AWS console and click on Create stack. Download CloudFormation template to your local device. Then, select Template is ready, Upload a template file and click on Choose file to upload. Click Next.  On Specify stack details, add a stack name, your domain name. My registered domain name is omartesting2021.tk. Click Next.  As a best practice, add a tag to track down the CloudFormation stack. Then, create a new SNS topic by entering a name for your topic and email address to receive updates via email. You will have to confirm the SNS topic subscription once you receive the confirmation email; otherwise, the SNS topic may not work. The SNS topics will update you on the progress of the stack building.  Note: don\u0026rsquo;t forget to select the SNS topic you have created from the dropdown menu.\nOn Capabilities section, select the two options as shown below. These two options are to allow the stack to create an IAM role and CAPABILITY_AUTO_EXPAND to name the resources dynamically. Finally, click on Create stack.  Now, let\u0026rsquo;s grab a cup of coffee (don\u0026rsquo;t forget to add some of that hazelnut creamer 😄 ) and wait on the SNS updates via emails. The CloudFront propagation may take some time. We will definitely appreciate the fact that we have subscribed to an SNS topic to get live updates.  We can also monitor the progress of resource provisioning from within CloudFormation, and once all resources are created successfully, we can access our sample static website from our domain name, www.omartesting2021.tk.\nWe can go back to our created S3 buckets and upload our site files to replace the sample website in the S3 bucket that has bucketroot.\nConclusion: Congratulations. We have successfully hosted a sample website on S3, registered a free domain name from Freenom and assign it to the S3 website via Route 53 hosted zone DNS management. In addition, we have distributed our website access via AWS\u0026rsquo;s low latency content network utilizing CloudFront service and Lambda@Edge to enhance security of the website. Enjoy it.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/terraform/",
	"title": "Create a Terraform Template to Spin EC2 Instance from a RaspberryPi 4",
	"tags": [],
	"description": "",
	"content": "What is Terraform? Terraform is an Infrastructure as a Code - IaaC- service. It is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. For more information about Terraform.\nDuring this tutorial, we will create a sample Terraform template to spin an AWS EC2 instance from the Command Line Interface (CLI) on a Raspberry Pi 4.\nLearning Outcomes:  Download Terraform on a Raspberry Pi 4 arm64 architecture. Create AWS Identity and Access Management (IAM) user to run the Terraform sample template from the RPI-4. Create a Terraform sample template to spin an AWS EC2. Learn Terraform commands.  What you do you need to complete the tutorial:  AWS Account Credentials. A Raspberry Pi 4. Shell script environment (any text editor of your choice).  Step 1: Download Terraform on Raspberry Pi 4  First of all, if you\u0026rsquo;d like to check your device\u0026rsquo;s architecture, run the following command:  uname -a\rNavigate to Terraform download page and download the proper package for your operating system and architecture. In my case, I chose to download the Linux package arm64 to my RPI-4.  Now, let navigate to the folder where the package is downloaded and unzip it using the unzip utility. If you don\u0026rsquo;t have the unzip utility, use the following command to install it.\nsudo apt install zip -y\rNow, let\u0026rsquo;s unzip the package as shown below:\nWe will run the following commands to move the Terraform zip file to our PATH. Firstly, we need to find the PATH.  echo $PATH\rThen, we will move the Terraform zip file to our main PATH, which is /usr/local/bin/. This will allow us to run Terraform commands from any location without any restrictions.\nsudo mv terraform /usr/local/bin/\rThen, we will cd into the the bin folder as shown below:\nNow, let\u0026rsquo;s run the command terraform. If the results as shown below, we have installed Terraform successfully.\nterraform\rThe results should be as shown below:\nStep 2: Create AWS Identity and Access Management (IAM) user Part 1:  Navigate to AWS console. Go to IAM and click on Users on the left hand side menu. Click on Add user and then add a user name and select access type as Programmatic access as shown below. Then, click on Next.  On Set permissions, click on Attach existing policies directly. search for XX policy and check it. Then, click Next.  On Add tags, it\u0026rsquo;s a best practice to use tags; therefore, adda key and value as shown below. Then, click Next.  Finally, review the add user details and permissions summary and then click on Create user.  Now, we have obtained the Access Key ID and Secret Access Key, let\u0026rsquo;s download the csv file and save it in a secure location for future reference.  Part 2: Install the latest AWS Command Line Interface - CLI version 2 for Linux ARM architecture since my Raspberry Pi 4 is an arm64 architecture. However, you shall Choose the appropriate package for your operating system and device architecture.\ncurl \u0026quot;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026quot; -o \u0026quot;awscliv2.zip\u0026quot;\runzip awscliv2.zip\rsudo ./aws/install\rWe will configure the aws on our device (RPI-4 in my case), which is the fastest way to set up our AWS CLI installation. When you enter this aws configure command, the AWS CLI prompts you for four pieces of information:\n Access key ID Secret access key AWS Region Output format  The access keys consist of an access key ID and a secret access key, which are used to sign programmatic requests that you make to AWS. If you don\u0026rsquo;t have access keys, you can create them from the AWS Management Console. As a best practice, do not use the AWS account root user access keys for any task where it\u0026rsquo;s not required.\n\rLet\u0026rsquo;s start with aws configuration by using the following command in our CLI.\nExample:\naws configure --profile [your profile name]\rActual:\naws configure --profile Terraform\rTo Check if the CLI and profile have been configured properly, run the following command which will lists all of your AWS EC2 instance resources using credentials and settings, which we have defined in the [your profile name] profile\naws ec2 describe-instances --profile [your profile name]\rStep 3: Create a Terraform Sample Template to Spin an AWS EC2 Terraform Commands    Commands Usage     terraform init Download any plugins required to run templates   terraform plan Will give you a list of resources that will be created/deleted   terraform apply WIll create/delete resources   terraform destroy Will delete all the resources created by Terraform   terraform fmt Will format the file with proper indentation    Note: you could always reference Terraform Providers for more commands per provider.\nCreating a Terraform Sample Template  Utilize any Shell script environment or text/code editor of your choice to start the Terraform language code for Terraform sample template as shown below and save the file with a .tf extension:  provider \u0026quot;aws\u0026quot; {\rprofile = \u0026quot;Terraform\u0026quot;\rregion = \u0026quot;us-east-1\u0026quot;\r}\rresource \u0026quot;aws_instance\u0026quot; \u0026quot;demo_instance\u0026quot; {\rami = \u0026quot;ami-042e8287309f5df03\u0026quot;\rinstance_type = \u0026quot;t2.micro\u0026quot;\rtags = {\rName = \u0026quot;Terraform_Demo\u0026quot;\r}\r}\rNote: every AMI has a unique ID and the Ubuntu 20.04 LTS we have chosen has the following AMI ID: ami-042e8287309f5df03. If you would like to spin a different AMI, replace the AMI ID. Also, don\u0026rsquo;t forget to replace the profile name with the name of AWS profile which we have created earlier (in my case is Terraform).\nNow, we are ready to run Terraform command. Let\u0026rsquo;s start with running terraform init, where we have the Template file saved.  terraform init\rThe terraform plan command will inspect the template for resources.  terraform plan\rFinally, we are ready to apply changes. Let\u0026rsquo;s run terraform apply. You will be prompted to enter yes to apply the changes requested by the template. Then, the resources requested will be created as shown below.  terraform apply\rWe will head to AWS console to verify the status of our newly spun EC2. Let\u0026rsquo;s pay close attention to the name of the instance and the AMI ID which should match the names we chose in our Terraform sample template.  Once we have completed the tutorial, we can utilize terraform destroy command to terminate the resources we have created and spun by our sample template. This command will only destroy the resources built by Terraform.  terraform destroy\rConclusion: Congratulations!! We have created a Terraform sample template and ran it to spin an AWS EC2 instance successfully from the Command Line Interface (CLI) on a Raspberry Pi 4.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/codecommit/",
	"title": "Create a Simple AWS CodePipeline from CodeCommit and Elastic Beanstalk",
	"tags": [],
	"description": "",
	"content": "AWS CodeCommit, Elastic Beanstalk and CodePipeline AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. CodeCommit eliminates the need to operate your own source control system or worry about scaling its infrastructure. You can use CodeCommit to securely store anything from source code to binaries, and it works seamlessly with your existing Git tools. For more information about AWS CodeCommit.\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. For more information about AWS Elastic Beanstalk.\nAWS CodePipeline is a continuous delivery service offered by AWS to model, visualize, and automate the release of applications. This service allows for rapid modeling and configuring of different steps of an application release process. It automates the necessary process of change of application deployment. For information about AWS CodePipeline.\nLearning Outcomes and Tutorial Scenario: In this tutorial, we will create a two-stage pipeline that uses a AWS CodeCommit as repository and AWS Elastic Beanstalk to release a sample application.\n Creating a CodeCommit repository and upload the sample application zip file. Creating an application in Elastic Beanstalk. Creating a pipeline in CodePipeline.  Step 1: Creating a CodeCommit repository for the sample application.  Download the sample application from AWS GitHub and save it into a folder or directory on your local device. Do not unzip the file.  Navigate to CodeCommit and create a repo by giving it a name and an optional description.  Next, upload the sample application zip file to the repo. You may need to give the Author name, Email address and description. Then, click on COmmit change as shown below.  Step 2: Creating an application in Elastic Beanstalk  Navigate to Elastic Beanstalk and click on Create Application. On the left hand menu, click on Environments and select Web server environment. Then, enter a name for the application, Application tag, choose PHP for a platform as shown below and then click on Create application.  Note: it will take a few minutes to setup the environment. Once the environment is setup, you should see the screen below\nstopped here.\nClick on the Application to create a new environment and then click on Create a new environment. Select Web server enviornment and click Select.  Step 4: Creating our first pipeline in CodePipeline To create a CodePipeline automated release process:\n  Navigate to Pipeline menu and Create pipeline.\n  Enter a name for the pipeline.\n  In Service role, choose new service role to allow CodePipeline to create a new service role in IAM. In Role name, the role and policy name both default to this format: AWSCodePipelineServiceRole-region-pipeline_name.\n  Leave the defaults for the Advanced section and then choose Next.\n  In Step 2: Add source stage, in Source provider, choose Amazon S3. In Bucket, enter the name of the S3 bucket we have previously created. In S3 object key, enter the object key with or without a file path, and remember to include the file extension. For example, for SampleApp_Windows.zip, enter the sample file name as shown in this example:  aws-codepipeline-s3-aws-codedeploy_linux.zip\rChoose Next step and leave defaults for Amazon CloudWatch Events as recommended. Choose Next.  In Step 3: Add build stage, choose Skip build stage, and then accept the warning message by choosing Skip again. Choose Next. In Step 4: Add deploy stage, in Deploy provider, choose AWS CodeDeploy. The Region field defaults to the same AWS Region as your pipeline. In Application name, enter the name your chose previously for the application, or choose the Refresh button, and then choose the application name from the list. In Deployment group, enter the name your chose previously for the deployment group, or choose it from the list, and then choose Next.  In Step 5: Review, review the information, and then choose Create pipeline. The pipeline starts to run. You can view progress and success and failure messages as the CodePipeline sample deploys a webpage to each of the Ubuntu EC2 instances in the CodeDeploy deployment.  To verify your pipeline ran successfully On the Description tab, in Public DNS, copy the address, and then paste it into the address bar of your web browser. View the index page for the sample application you uploaded to your S3 bucket.\nConclusion: By the end of this tutorial, we have successfully created a simple pipeline in CodePipeline. The pipeline has two stages:\n  A source stage named Source, which detects changes in the versioned sample application stored in the S3 bucket and pulls those changes into the pipeline.\n  A Deploy stage that deploys those changes to EC2 instances with CodeDeploy.\n  Congratulations!\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/patchmanager/",
	"title": "Patching Model using AWS Systems Manager - Patch Manager",
	"tags": [],
	"description": "",
	"content": "Table of Contents  Patching Model using AWS Systems Manager - Patch Manager  AWS Systems Manager Patch Manager  What is the Patch Baseline? How Patch Baseline Rules Work on Amazon Linux 2   Patching Model Solution Architecture   Architecture Diagram   How to Create a Patching Model for a Single Managed Amazon Linux 2 EC2 Instance - Step by Step  Prerequisites  How to Create SSM Documents Create IAM Service Role for Maintenance Window  Create a policy Create IAM resource role   Create and add inline policies to the EC2 IAM instance profile   Step 1: Create a Custom Patch Baseline Step 2: Tagging the EC2 Instance with the Patch Group Key-Value Step 3: Assigning a Patch Group to the Patch Baseline Step 4: Creating a Maintenance Window Step 5: Registering Targets to the Maintenance Window Step 6: Assigning Tasks to the Maintenance Window   Conclusion     July 8th, 2022 AWS Systems Manager Patch Manager Patch Manager is a capability of AWS Systems Manager. It applies and automates the patching process of managed nodes for both security related and other types of updates, which makes it a useful tool for mutable infrastructure model. It\u0026rsquo;s capable of patching operating systems as well as applications. With the use of patch baseline, Patch Manager includes rules for auto-approving patches and for creating a list of approved or rejected patches. Patches can be installed individually or to a large groups of managed instances using tags. To schedule patching, Patch Manager runs tasks of Maintenance Window, which is another capability of Systems Manager. Although using Patch Manager is not the only patching option, it is one of the most straightforward and practical approaches. This patching model tutorial is to provide a proof of concept for patching a single managed Amazon Linux 2 instance. The tutorial\u0026rsquo;s goal is to equip cloud teams the know-how they need to start using the Patch Manager. As a result, the tutorial\u0026rsquo;s expertise and knowledge are to be applied to a fleet of managed instances or on-premises servers.\n Note: Patch Manager doesn\u0026rsquo;t support upgrading major versions of operating systems.\n What is the Patch Baseline? Patch Manager provides predefined baselines for each supported operating systems. The service uses the native package manager to drive the installation of patches approved by the patch baseline. For Amazon Linux 2, the predefined baseline is to approve all operating systems patches classified as Security and have a severity level of Critical or Important. The patches are auto-approved 7 days after release. Moreover, all patches classified Bugfix are also auto-approved 7 days after release. The predefined patch baseline are not customizable. However, it\u0026rsquo;s feasible to create a custom patch baseline to control patch classifications, approval/rejection and auto-approve days after release.\nHow Patch Baseline Rules Work on Amazon Linux 2 Below is the guidelines for the packages selected for update:\n   Security option Equivalent Yum Command     Pre-defined default patch baselines provided by Amazon (non-security updates option is not selected) sudo yum update-minimal --sec-severity=critical,important --bugfix -y   User Custom patch baselines (non-security updates option is selected) sudo yum update --security --bugfix -y    Please, refer to AWS documentation for further information on how security patches are selected.\n Patching Model Solution Architecture The plan is to utilize Patch Manager to develop a patching model for a single managed Amazon Linux 2 EC2 instance by using tags. Based on a scheduled maintenance window, an SSM Agent, installed on the EC2 instance, receives command issued to commence a patching process. The agent validates the instance\u0026rsquo;s Patch Group tag-value and queries Patch Manager for an associated patch baseline. Once Patch Manager confirms the patch baseline for the Patch Group tag-value, it notifies the SSM Agent to retrieve the patch baseline snapshot. Finally, the SSM Agent begins scanning and installing patches based on the rules defined in the patch baseline snapshot provided by Patch Manager.\nTheAWS-RunPatchBaselineWithHooks SSM document will be used to orchestrate multi-step installing patches. It offers three optional hooks which allows running SSM documents at three points during the patching cycle (pre-install, post-patch and post-reboot). We will create three simple SSM documents, as a proof of concept, to be used during the patching cycle. To read more about AWS-RunPatchBaselineWithHooks SSM document , please refer to AWS blog.\nKernel Live Patching is another feature available for Amazon Linux 2. It allows applying patches without the need for an immediate reboot or any disruption to running applications. We will not be using this feature for our patching model but I would like to recommend considering it. For more information about Kernel Live Patching, please refer to AWS documentation. Please, bear in mind in case there is a need to run patching outside of the maintenance window for a fleet of instances, it\u0026rsquo;s recommend by AWS, as best practice, to provide a Snapshot-ID. It ensures consistency among the targeted instances. Please, refer to AWS documentation for more information. In this patching model, we have allocated a Patch Group tag per instance; therefore, no snapshot id is needed for patching outside the maintenance window.\n Note: it\u0026rsquo;s highly recommended to test the patching model in a development environment prior to deploying to production. Also, for multi-account and multi-region patching, please refer to AW blog for more information.\n Architecture Diagram - Patching Model using Patch Manager This patching model should work as a proof of concept. A similar concept to this architecture can be applied to fleets of managed EC2 instances and on-premises servers.\n Note: the lambda functions to Slack channel and to PagerDuty are not included in this tutorial.\n  How to Create Patching Model for Single Managed Amazon Linux 2 EC2 Instance - Step by Step Prerequisites  SSM Agent version 3.0.502 or later (requirement for the AWS-RunPatchBaselineWithHooks SSM document) Internet connectivity. The managed instance must have access to the source patch repositories. Minimum operating system - Amazon Linux 2 2 - 2.0 IAM service role for Systems Manager to run Maintenance Window tasks Optionally, adding policies to the IAM service role for SNS and CloudWatch logs Optionally, a preconfigured S3 bucket to receive the patching command logs Optionally, a preconfigured SNS topic for patching event notifications Optionally, adding S3 and CloudWatch logs policies to the EC2 instance profile role   Step 1: Create a Custom Patch Baseline  Navigate to AWS Systems Manager console. Under Node Management, select Patch Manager.   Select View predefined patch baselines and click on Create patch baseline. The, fill out the requirements as following:\n  Under Patch baseline details:\n Name: give the custom baseline a name such as, AmazonLinux2AllPatchesBaseline Description: optionally, add a description for the baseline such as, Amazon Linux2 All Patches Baseline Operating system: from the dropdown menu, select Amazon Linux 2    Under Approval rule for operating systems:\n Product: select All Classification: All Severity: All Auto-approval: leave the default selected option, Approve patches after a specified number of days Specify the number of days: leave the default 0 days Compliance reporting - optional: leave the default selected option, Unspecified Include nonsecurity updates: check the box to install nonsecurity patches    Under Patch exceptions:\n Rejected patches - optional: add system-release.* as shown below. Rejected patches action - optional: select Block   Note: the purpose of this block is to reject patches to new Amazon Linux releases beyond the Patch Manager supported operating systems.\n   Under Patch sources: we will not add other source. Please, refer to AWS documentation on how to define alternative patch source repository\n  Under Manage tags: optionally define tags for the patch baseline\n    Step 2: Assigning a Patch Group to the Patch Baseline  Navigate to AWS Systems Manager console. Under Node Management, select Patch Manager. Click on Patch baseline tab and select the previously created custom baseline name AmazonLinux2AllPatchesBaseline. Click on the Baseline ID which is leading to the baseline details page. On Actions button, select Modify patch groups. In the Patch groups textbox, type in the value we defined in step 2, which is WebServer-Prd. Then, click on Add button and Close.  Step 3: Creating a Maintenance Window   Navigate to AWS Systems Manager console. Under Change Management, select Maintenance Windows.\n  Click on Create maintenance window and fill out the requirements as following:\n  Under Provide maintenance window details:\n Name: give the maintenance window a name such as, Patch-WebServer-Prd Description: optionally, add a description for the maintenance window such as, Patching Maintenance Window for Patching WebServer in production Unregistered targets: uncheck this option. If the option is selected, it allows to register instances that are not part of the targets.    Under Schedule:\n Specify with: select CRON/Rate expression. For our scenario, we will run patching on the first Wednesday of each month at 9:00pm CDT. Therefore, the CRON job is cron(0 21 ? * WED#1 *). We will define the timezone shortly.   Note: for testing, you can use Rate schedule to the run the patching job every hour or so. Also, refer to AWS documentation for more information about cron and rate expressions for Systems Manager.\n Duration: the number of hours the maintenance window will run. Type in 2. Stop initiating tasks: the number of hours before the end of the maintenance window that the systems should stop scheduling new tasks to run before the window closes. Type in 1. Window start date: select the starting date and time of this maintenance window such as, 7/6/2022 20:00pm GMT-05:00 (the GMT-05:00 is a conversion to US/Cental time). Window end date: you may define an end date for the maintenance window. For our scenario, leave it empty. Schedule timezone: select US/Central timezone.   Note: the Central Daylight Time (CDT) is -5 hours from GMT;therefore, the maintenance window is in affect starting from the 6th of July at 8:00pm CDT.\n Schedule offset: leave it empty.    Under Manage tags: optionally define tags for the maintenance window\n    Finally, click on Create maintenance window to complete the process.\n  Step 4: Registering Targets to the Maintenance Window  Navigate to AWS Systems Manager console. Under Change Management, select Maintenance Windows. Then, click View details. Press the Actions button and select Register targets from the dropdown menu. On the Register target screen under maintenance window target details, fill out the requirements as following:  Name: give the targets a name such as, WebServer-Prd Description: optionally, add a description for the maintenance window such as, The target is the Web Server in production Owner information: optionally, you may specify a name of the owner such as, The A-Engineering Team   Under the Targets section, for Target selection, select Specify instance tags. Then, enter the patch group key-value tag that we created previously and click Add as shown below.     Key Value     Patch Group WebServer-Prd    Finally, click on Register target to complete the process.  Step 5: Assigning Tasks to the Maintenance Window  Navigate to AWS Systems Manager console. Under Change Management, select Maintenance Windows. Then, click View details. Press the Actions button and select Register run command task from the dropdown menu. On Register Run command task and Under Maintenance window task details section, fill out the requirements as following:  Name: give the task a name such as, PatchWebServerPrd Description: optionally, add a description for the maintenance window such as, Run Command Task for patching Patching Web Server in production New task invocation cutoff: check Enabled to prevent new task invocations when the maintenance window cutoff time is reached   Under Command document section, search for AWS-RunPatchBaselineWithHooks and then select it. Leave Document version at the default selection Default Version at runtime and leave the Task priority at default value of 1.   Note: AWS-RunPatchBaselineWithHooks is a wrapper document for AWS-RunPatchBaseline document. It divides the patching into two events, before reboot and after reboot for a total of three hooks to support custom functionality. Refer to About the AWS-RunPatchBaselineWithHooks SSM document for more information.\n For the Targets section, select Selecting registered target groups which is the default selection. Then, from the list select the target group that we assigned to the maintenance window. Under Rate control,  For Concurrency, leave it at default selection, targets, and type in 1 For Error threshold, leave it at default selection, errors, and type in 1   On the IAM service role, select maintenance-window-role role from the dropdown menu.   Note: if the IAM service role is not created yet, refer to Create IAM Service Role for Maintenance Window section.\n  Under Output options,\n Optionally, check Enable writing to S3 and type the name of the preconfigured S3 bucket. In my case, it\u0026rsquo;s patching-webserver.   Note: to capture the complete terminal output logs, configure an S3 bucket because only the last 2500 characters of a command output are displayed in the console.\n Optionally, add an S3 key prefix such as, patching/webserver/prd Check CloudWatch output and, optionally, type in a name for the Log group such as, patching/webserver/prd   Note: we will have to give the required permissions to the EC instance IAM profile role to put objects to S3 and put logs to CloudWatch.\n   For the SNS notifications section\n Check Enable SNS notifications Select the preconfigured IAM role for the SNS service. In our case, it\u0026rsquo;s maintenance-window-role because we have added an SNS policy to it. Paste the SNS topic ARN For Event type, leave it at default selection which is All events For Notification type, select Per instance basis notification when the command status on each instance changes    Under the Parameters section:\n For Operation, select Install from the dropdown menu Snapshot Id, leave it empty Reboot Option: leave it at default value, RebootIfNeeded Pre Install Hook Doc Name: type in the name of the pre install document Post Install Hook Doc Name: : type in the name of the post install document On Exit Hook Doc Name: type in the name of the on-exit document Comment: optionally, add comments about the command Timeout (seconds): leave it at default value, 600 Finally, click Register Run command task   Note: for details on how to create documents, refer to How to Create SSM Document section. If you don\u0026rsquo;t want to use any of the three hooks, then leave it at default value, which is AWS-Noop.\n   Step 6: Assigning Patch Group Tag to the Targeted Amazon Linux 2 EC2 Instance  Navigate to AWS Systems Manager console. Under Node Management, select Fleet Manager. From the Managed nodes list tab, select the EC3 that is targeted for patching. Then, click on Node actions and select View details from dropdown menu. From the Tags tab, click Edit to add the below tag key-value and then press the Save button:     Key Value     Patch Group WebServer-Prd     How to Create SSM Documents We will create three simple documents as a proof of concept. Please, refer to AWS documentation for more details about creating SSM documents.\n Navigate to AWS Systems Manager console. Under Shared Resources, select Documents. Click Create document button and select Command or Session from the dropdown menu. On the Create document screen:  Name: give the document a name such as, Pre-patch-WebServer-Document Target type: leave it empty Document type: leave it at default which is Command document Content: replace the default content with the below into the YAML editor Document tags: optionally add key-value tag to the document Finally, click Create document    ---\rschemaVersion: '2.2'\rdescription: Pre-patch Web Server Document\rparameters: {}\rmainSteps:\r- action: aws:runShellScript\rname: configureServer\rinputs:\rrunCommand:\r- sudo systemctl status httpd\rRepeat the process from bullet #3 for Post-install-WebServer-Document and Post-reboot-WebServer-Document. The YAML contents are below.  ---\rschemaVersion: '2.2'\rdescription: Post-install Web Server Document\rparameters: {}\rmainSteps:\r- action: aws:runShellScript\rname: configureServer\rinputs:\rrunCommand:\r- sudo systemctl stop httpd\r---\rschemaVersion: '2.2'\rdescription: Post-reboot Web Server Document\rparameters: {}\rmainSteps:\r- action: aws:runShellScript\rname: configureServer\rinputs:\rrunCommand:\r- sudo systemctl start httpd\r- sudo systemctl status httpd\r Create IAM Service Role for Maintenance Window As Systems Manager needs permissions to run maintenance window tasks, below are the steps to create the required IAM role:\nCreate a policy:  From the IAM console, navigate to Policies and click Create policy button. Click on the JSON tab and after clearing the default content, paste the below policy and click Next:Tags.  {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;ssm:SendCommand\u0026quot;,\r\u0026quot;ssm:CancelCommand\u0026quot;,\r\u0026quot;ssm:ListCommands\u0026quot;,\r\u0026quot;ssm:ListCommandInvocations\u0026quot;,\r\u0026quot;ssm:GetCommandInvocation\u0026quot;,\r\u0026quot;ssm:ListTagsForResource\u0026quot;,\r\u0026quot;ssm:GetParameters\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;resource-groups:ListGroups\u0026quot;,\r\u0026quot;resource-groups:ListGroupResources\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;*\u0026quot;\r]\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;tag:GetResources\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;*\u0026quot;\r]\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;sns:Publish\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:sns:*:*:*\u0026quot;\r]\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: \u0026quot;iam:PassRole\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;,\r\u0026quot;Condition\u0026quot;: {\r\u0026quot;StringEquals\u0026quot;: {\r\u0026quot;iam:PassedToService\u0026quot;: [\r\u0026quot;ssm.amazonaws.com\u0026quot;\r]\r}\r}\r}\r]\r}\rOptionally, add tag-key value pairs and click Next:Review. Give the policy a name and description such as: a. Name: maintenance-window-policy b. Description: The policy allows Systems manager to run maintenance window tasks Finally, click Create policy  Create IAM resource role:  From the IAM console, navigate to Roles and click Create role button. For Select trusted entity page:  Trusted entity type: leave at default, AWS service Use case - Use cases for other AWS services: search for Systems Manager and select it (NOT Systems Manager - Inventory and Maintenance Windows) Click Next   On Add permissions screen, search for the previously created policy, maintenance-window-policy and select it. Click Next Give the role a name such as, maintenance-window-role and click Create role.  Create and add inline policies to the EC2 IAM instance profile: The EC2 instance profile requires necessary permissions to write data to an S3 bucket and to put log events to CloudWatch. We will create inline policies to assign the required permissions to the EC2 instance profile.\n  From the IAM console, navigate to Roles and search for your EC2 instance profile role. Then, click on the Role name.\n  Under *Permissions tab:\n From Add permissions button, select Create inline policy from the dropdown menu Select the JSON tab and replace the default content with the below policy:  {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: \u0026quot;s3:PutObject\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r}\r]\r}\rClick Review policy On the Review policy page, give the policy a name such as, S3-Put-Instance-Profile Finally, click Create policy    Repeat the process from bullet 2 to create an inline policy to allow the EC2 instance to create log stream and to put log events to CloudWatch. Use the below inline policy:\n  {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;logs:CreateLogStream\u0026quot;,\r\u0026quot;logs:PutLogEvents\u0026quot;,\r\u0026quot;logs:CreateLogGroup\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r}\r]\r}\r Conclusion Patch Manager is an effective and powerful tool. It assists cloud teams in managing security risks and addressing vulnerabilities by automating the patching process. By the end of the tutorial, we have learned how to schedule patches for a single managed Amazon Linux 2 instance using Patch Manager and its features. Now that we have this knowledge, we can use it to create a patching model that is more intricate for a fleet of managed instances or on-premises servers.\nI hope this tutorial helps you learn more about Patch Manager and how to use it for your patching models.\nOmar A Omar\n "
},
{
	"uri": "https://omar2cloud.github.io/aws/cdktf/",
	"title": "CDK for Terraform (CDKTF) on AWS",
	"tags": [],
	"description": "",
	"content": "Table of Contents  Introduction Step 1: Required Prerequisites Step 2: Initializing First CDKTF Project using Python Template Step 3: Configuring an S3 Remote Backend  Option 1: Utilize an existing S3 bucket and DynamoDB to configure the S3 Remote Backend Option 2: Create an S3 Bucket and DynamoDB Table for the S3 Remote Backend using CDKTF   Step 4: Learn How to Use Construct Hub and AWS Provider Submodules  Scenario 1: S3 Bucket Scenario 2: ECS Cluster CDKTF Commands   Step 5: Deploying a Lambda Function URL using CDKTF Conclusion   Sept 27th, 2022 How to Configure an S3 Remote Backend and Deploy a Lambda Function URL using Python on Ubuntu 20.04 LTS Introduction After two years of collaboration between AWS and HashiCorp and on the 9th of August, 2022, AWS announced the general availability of CDK for Terraform on AWS. As the CDKTF framework finally saw the light of day, the news triggered lots excitement among the community. The CDKTF framework is a new open-source project that enables developers to use their favorite programming languages to define and provision cloud infrastructure resources on AWS. Under the hood, it converts the programming language definitions into Terraform configuration files and uses Terraform to provision the resources.\n\u0026ldquo;Learning is no longer a stage of life; it’s a lifelong journey\u0026rdquo; - Andy Bird\nIt\u0026rsquo;s often said that the best way to learn something new is to do it, and the first milestone in learning is often the hardest. However, with the right guidance, you can overcome the challenges and achieve your goals. My objective in this article is to guide you through the process of installing, configuring, and deploying your first AWS resource using CDKTF on AWS. In addition, I will also show you how to use the Construct Hub documentations to deploy your own AWS resources.\nThe tutorial should be easy to follow and understand for beginners and intermediate users. The tutorial is devoted to developers with adequate AWS, Terraform and Python knowledge but who are unsure of how and where to begin their CDKTF learning.\nThe main topics covered in this tutorial are:\n Proper installation and configuration of all required prerequisites. Installing and configuring CDKTF. Initializing first CDKTF project using Python template and local backend. Deploying a S3 bucket and DynamoDB table and configuring an S3 remote backend. Learning how to read/use AWS Provider Submodules and Construct Hub documentations Provisioning an S3 Bucket using CDKTF Provisioning an IAM role and Lambda Function URL using CDKTF  This tutorial is also available on my GitHub - CDKTF-Tutorial repo. By the end of this tutorial, you shall be comfortable deploying AWS resources using the CDKTF framework. I will pass you the learning baton and you can take it from there. Enough said, let\u0026rsquo;s get started.\n Step 1: Required Prerequisites To complete this tutorial successfully, we should install and configure the following prerequisites properly. To set you up for success, I have to ensure you have the following prerequisites installed and configured properly:\n AWS CLI version 2:   Follow AWS Prerequisites to use the AWS CLI version 2 documentation for all required prerequisites. Follow Installing or updating the latest version of the AWS CLI document to install the AWS CLI v2 as per your local device architecture. In my case, it\u0026rsquo;s an Ubuntu 20.04 LTS OS running on a Linux x86 (64-bit) architecture. Lastly, follow the Configuration basics - Profiles document to configure the AWS CLI v2 and create a named profile (name it CDKFT as shown below). We will use the named profile to configure AWS Provider credentials later on.  aws configure --profile CDKTF\r Note: the profile name does not have to be CDKTF, you can name it however you like. However, I will be using CDKTF in this tutorial.\n Terraform (version v1.0+ as per Terraform\u0026rsquo;s recommendation). Node.js and npm v16+:   Follow NodeSource Node.js Binary Distributions to install Node.js v16.x for Ubuntu (the installation includes npm). For other operating systems and architectures, refer to Node.js official page. Once you have Node.js installed, make sure it\u0026rsquo;s version 16.x by running the below command:  node -v\rPython 3.7+: Ubuntu 20.04 LTS distribution comes with Python 3.8.2 pre-installed by default. Run the below command to confirm:  python3 --version\rAdditionally, we will need to make sure we have pip installed and available.\npip3 --version\r Note: if pip is unavailable, run sudo apt install python3-pip to install it as per Python documentation.\n pipenv v2021.5.29+: as of Sept 22, 2022, the latest version of pipenv is 2022.9.20. We will use pip to install pipenv.  pip3 install pipenv\r Note: if you receive a WARNING stating pipenv is not installed on PATH as shown on the below image, run the below command to add it to the path:\n export PATH=\u0026quot;the-path-mentioned-in-the-warning:$PATH\u0026quot;\rActual example:\nexport PATH=\u0026quot;/home/CDKTF/.local/bin:$PATH\u0026quot;\rWe have to confirm that pipenv is installed and available before we proceed. Run the below command to confirm:\npipenv --version\r Note: it\u0026rsquo;s tempting to install pipenv by using the package manager sudo apt install pipenv, but be aware that the system repository version of pipenv is outdated and will not work with the CDKTF framework.\n CDKFT: now, we are ready to install the latest stable release of CDKTF using npm:  npm install --global cdktf-cli@latest\rIf you receive a permission denied error, use sudo as shown below:\nsudo npm install --global cdktf-cli@latest\rLet\u0026rsquo;s confirm the version:\ncdktf --version\rMake sure you have all required prerequisite versions installed and configured successfully as shown on the below image:\nReaching this point means you have all required prerequisites installed and configured properly. We are ready to move on to the next step. This is a milestone, so take a moment to celebrate it. You deserve it.\n Step 2: Initializing the First CDKTF Project using Python Template In this section, we will learn how to use CDKTF commands to create our first AWS CDKTF Python project. We will use the cdktf init command to create a new project in the current directory. The command also creates a cdktf.json file that contains the project configuration. The cdktf.json file contains the project name, the programming language, and the Terraform provider. The cdktf.json file is used by the cdktf command to determine the project configuration. Follow the below steps to initialize the first CDKTF project using Python template:\n Create a new directory for the project and cd into the directory. I will create a directory on my Desktop and name it first_project:  mkdir first_project \u0026amp;\u0026amp; cd first_project\rRun the following command to initialize our first CDKTF project. We will be promoted to enter the following information:  A. Name of the project: leave it as first_project and hit Enter.\nB. Project description: leave it as My first CDKTF project and hit Enter.\nC. Send crash reports to CDKTF team. I would highly recommend you say yes to send crash reports to the CDKTF team. This will help improve the product and fix bugs.\ncdktf init --template=\u0026quot;python\u0026quot; --local\r Note 1: we are using the --local flag to store our Terraform state file locally. We will reconfigure the backend to S3 Remote backend in the next section.\n  Note 2: if you receive error [ModuleNotFoundError: No module named 'virtualenv.seed.via_app_data'], you would need to remove virtualenv by running sudo apt remove python3-virtualenv. We should still have virtualenv as part of the pip packages. Run pip3 show virtualenv to confirm. If you don\u0026rsquo;t see virtualenv in the list, run pip3 install virtualenv to install it.\n Activate the project\u0026rsquo;s virtual environment (optional but recommended):  pipenv shell\r Note: the purpose of creating a virtual environment is to isolate the project and and all its packages and dependencies from the host or local device. It\u0026rsquo;s a self contained environment within the host to prevent polluting the system. It\u0026rsquo;s highly recommended to activate it to keep your host healthy and clean.\n Install AWS provider. There are multiple ways to install AWS Provider. We will use pipenv to install the AWS provider. Run the below command to install the AWS provider:  pipenv install cdktf-cdktf-provider-aws\r Note: as of the 26th of Sept, 2022, if you decide to install the AWS Provider using cdktf provider add \u0026quot;aws@~\u0026gt;4.0\u0026quot;, the installation will fail due to no matching distribution found for version v9.0.36. There are other methods of importing a provider but this tutorial won\u0026rsquo;t discuss to focus on simplicity.\n  \nCongratulations, you have successfully initialized your first CDKTF Python project. This is another milestone to celebrate. Get a cup of coffee ☕️ and let\u0026rsquo;s move on to the next section.\n Step 3: Configuring an S3 Remote Backend Terraform stores all managed infrastructure and configuration by default in a file named terraform.tfstate. If a local backend is configured for the project, the state file is stored in the current working directly. However, when working in a team environment to collaborate with other team members, it is important to configure a remote backend. There are several remote backend options such as, consul, etcd, gcs, http and s3. For a full list of remote backends, refer to Terraform documentation.\nFor this tutorial, we will configure an S3 Remote Backend which includes an S3 bucket for storing the state file and a DynamoDB table for state locking and consistency checking.\nSelect one of the following options to configure the S3 Remote Backend:\nOption 1: Utilize an existing S3 bucket and DynamoDB to configure the S3 Remote Backend.   From Step 2, while we still have the virtual environment activated, let\u0026rsquo;s open the project directory using our choice of an Integrated Development Environment (IDE). In my case, I\u0026rsquo;m using Visual Studios Code which I downloaded from the Ubuntu Software Store. Run code . on the terminal to open the project directory via VS Code.\n  Navigate to main.py file and add the AWS provider construct to the imports section:\n  from cdktf_cdktf_provider_aws import AwsProvider\r Note: the final main.py code will be provided at the end of this section.\n Configure the AWS provider by adding the following code to MyStack class:  AwsProvider(self, \u0026quot;AWS\u0026quot;, region=\u0026quot;us-east-1\u0026quot;, profile=\u0026quot;CDKTF\u0026quot;)\r Note: we are using the profile attribute to specify the AWS profile to use. This is the AWS CLI profile we have previously discussed and created in the Required Prerequisites section. If you don\u0026rsquo;t have a profile created, you can remove the profile attribute and the AWS provider will use the default profile. Or, you can use a different authentication method as per the AWS Provider documentation.\n Add S3Backend class to the other imported classes. The S3Backend class is employing the S3 bucket and DynamoDB table as an S3 remote backend.  from cdktf import App, TerraformStack, S3Backend\rAdd the S3 Backend construct to the main.py file. We will add the following S3 Backend configurations to the MyStack class:    bucket - the name of the S3 bucket to store the state file. The bucket must exist and be in the same region as the stack. If the bucket doesn\u0026rsquo;t exist, the stack will fail to deploy.\n  key - the name of the state file and its path. The default value is terraform.tfstate.\n  encrypt - whether to encrypt the state file using server-side encryption with AWS KMS. The default value is true.\n  region - the region of the S3 bucket and DynamoDB table. The default value is us-east-1.\n  dynamodb_table - the name of the DynamoDB table to use for state locking and consistency checking. The table must exist and be in the same region as the stack. If the table doesn\u0026rsquo;t exist, the stack will fail to deploy.\n  profile - the AWS CLI profile to use. The default value is default. But, we have already configured the AWS provider to use the CDKTF profile.\n  Here is how the S3 Backend construct will look like:\n S3Backend(self,\rbucket=\u0026quot;cdktf-remote-backend\u0026quot;,\rkey=\u0026quot;first_project/terraform.tfstate\u0026quot;,\rencrypt=True,\rregion=\u0026quot;us-east-1\u0026quot;,\rdynamodb_table=\u0026quot;cdktf-remote-backend-lock\u0026quot;,\rprofile=\u0026quot;CDKTF\u0026quot;,\r)\rThe final main.py file should look like this:\nfrom constructs import Construct\rfrom cdktf import App, TerraformStack, S3Backend\rfrom cdktf_cdktf_provider_aws import AwsProvider\rclass MyStack(TerraformStack):\rdef __init__(self, scope: Construct, ns: str):\rsuper().__init__(scope, ns)\rAwsProvider(self, \u0026quot;AWS\u0026quot;, region=\u0026quot;us-east-1\u0026quot;, profile=\u0026quot;CDKTF\u0026quot;)\rS3Backend(self,\rbucket=\u0026quot;cdktf-remote-backend\u0026quot;,\rkey=\u0026quot;first_project/terraform.tfstate\u0026quot;,\rencrypt=True,\rregion=\u0026quot;us-east-1\u0026quot;,\rdynamodb_table=\u0026quot;cdktf-remote-backend-lock\u0026quot;,\rprofile=\u0026quot;CDKTF\u0026quot;,\r)\r# define resources here\rapp = App()\rMyStack(app, \u0026quot;first_project\u0026quot;)\rapp.synth()\rRun cdktf synth to generate the Terraform configuration files. The Terraform configuration files will be generated in the cdktf.out directory. The synth command will fail if the S3 bucket and DynamoDB table don\u0026rsquo;t exist.  cdktf synth\rYou have successfully configured the S3 Remote Backend. Let\u0026rsquo;s move on to the next section.\nTo test the configuration of the S3 remote backend, follow the below steps to create an S3 bucket resource and deploy the stack.   Import the S3 bucket library to the main.py file:  from cdktf_cdktf_provider_aws import AwsProvider, s3\r Add the S3 bucket resource to the MyStack class:   my_bucket = s3.S3Bucket(self, \u0026quot;my_bucket\u0026quot;,\rbucket=\u0026quot;Name-of-the-bucket\u0026quot;,\r)\r  Replace Name-of-the-bucket with the name of the bucket you want to create. Note, S3 bucket names must be unique across all existing bucket names in Amazon S3.\n  Run cdktf deploy to deploy the stack and create the S3 bucket.\n  cdktf deploy\r\n Note: if you get Incomplete lock file information for providers warning, you can either ignore it or you can run terraform providers lock -platform=linux_amd64 from the project root directory to validate the lock file. For more information, refer to Terraform documentation.\n Congratulations, you have successfully configured an S3 remote backend and created an S3 bucket using CDKTF. It\u0026rsquo;s time to take a break and stretch your legs.\n Option 2: Create an S3 Bucket and DynamoDB Table for the S3 Remote Backend using CDKTF For this option, we will take a different approach. We will create the S3 bucket and DynamoDB table using CDKTF and then configure the S3 remote backend to use the newly created resources. Follow the below steps to create the S3 bucket and DynamoDB table:\n  Open the project directory using your preferred IDE. If you are using VS Code, cd into the project folder and run code . on the terminal to open the project directory using VS Code.\n  Navigate to the main.py file and replace the default code with the following. This code creates an S3 bucket and DynamoDB table for the S3 remote backend.\n  from constructs import Construct\rfrom cdktf import App, TerraformStack, S3Backend\rfrom cdktf_cdktf_provider_aws import AwsProvider, s3, dynamodb\rclass MyStack(TerraformStack):\rdef __init__(self, scope: Construct, ns: str):\rsuper().__init__(scope, ns)\rAwsProvider(self, \u0026quot;AWS\u0026quot;, region=\u0026quot;us-east-1\u0026quot;, profile=\u0026quot;CDKTF\u0026quot;)\r# define resources here\rs3_backend_bucket = s3.S3Bucket(self,\r\u0026quot;s3_backend_bucket\u0026quot;,\rbucket=\u0026quot;cdktf-remote-backend-2\u0026quot;,\r)\rdynamodb_lock_table = dynamodb.DynamodbTable(self, \u0026quot;dynamodb_lock_table\u0026quot;,\rname=\u0026quot;cdktf-remote-backend-lock-2\u0026quot;,\rbilling_mode=\u0026quot;PAY_PER_REQUEST\u0026quot;,\rattribute=[\r{\r\u0026quot;name\u0026quot;: \u0026quot;LockID\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;S\u0026quot;\r}\r],\rhash_key=\u0026quot;LockID\u0026quot;,\r)\rapp = App()\rMyStack(app, \u0026quot;first_project\u0026quot;)\rapp.synth()\r Note, if the S3 bucket and DynamoDB table already exist, an error will be thrown. The S3 bucket names must be globally unique across all existing bucket names in Amazon S3 and DynamoDB table names must be unique within an AWS account. If you get an error, you can change the bucket and table names to unique names.\n Run cdktf deploy to deploy the stack and create the S3 bucket and DynamoDB table.  cdktf deploy\r\n Note: if you get Incomplete lock file information for providers warning, you can either ignore it or you can run terraform providers lock -platform=linux_amd64 from the project root directory to validate the lock file.\n Now, we will configure the S3 remote backend to use the newly created S3 bucket and DynamoDB table. Open the main.py file and replace the code with the following. This code configures the S3 remote backend to use the newly created S3 bucket and DynamoDB table. Make sure to replace the bucket and dynamodb_table values with the names of the S3 bucket and DynamoDB table you created in the previous step.  from constructs import Construct\rfrom cdktf import App, TerraformStack, S3Backend\rfrom cdktf_cdktf_provider_aws import AwsProvider, s3, dynamodb\rclass MyStack(TerraformStack):\rdef __init__(self, scope: Construct, ns: str):\rsuper().__init__(scope, ns)\rAwsProvider(self, \u0026quot;AWS\u0026quot;, region=\u0026quot;us-east-1\u0026quot;, profile=\u0026quot;CDKTF\u0026quot;)\r#S3 Remote Backend\rS3Backend(self,\rbucket=\u0026quot;cdktf-remote-backend-2\u0026quot;,\rkey=\u0026quot;first_project/terraform.tfstate\u0026quot;,\rencrypt=True,\rregion=\u0026quot;us-east-1\u0026quot;,\rdynamodb_table=\u0026quot;cdktf-remote-backend-lock-2\u0026quot;,\rprofile=\u0026quot;CDKTF\u0026quot;,\r)\r# Resources\rs3_backend_bucket = s3.S3Bucket(self, \u0026quot;s3_backend_bucket\u0026quot;,\rbucket=\u0026quot;cdktf-remote-backend-2\u0026quot;,\r)\rdynamodb_lock_table = dynamodb.DynamodbTable(self, \u0026quot;dynamodb_lock_table\u0026quot;,\rname=\u0026quot;cdktf-remote-backend-lock-2\u0026quot;,\rbilling_mode=\u0026quot;PAY_PER_REQUEST\u0026quot;,\rattribute=[\r{\r\u0026quot;name\u0026quot;: \u0026quot;LockID\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;S\u0026quot;\r}\r],\rhash_key=\u0026quot;LockID\u0026quot;,\r)\rapp = App()\rMyStack(app, \u0026quot;first_project\u0026quot;)\rapp.synth()\rRun cdktf synth to generate the Terraform configuration files. The Terraform configuration files will be generated in the cdktf.out directory.  cdktf synth\rTo migrate the local state backend to an S3 remote backend, navigate to the cdktf.out/stacks/first_project directory and run the following command to start the migration process. The first_project is the name of the project. If you have named your project differently, navigate to the cdktf.out/stacks/\u0026lt;project_name\u0026gt; directory and run the command.  Note: before running this command, read Important Notes below\ncd cdktf.out/stacks/first_project\rterraform init --migrate-state\rImportant Notes:\nWhen you run terraform init --migrate-state, Terraform prompts you to answer the following question:\nDo you want to copy existing state to the new backend?\nA. If you enter, yes to migrate the state file to the S3 backend, CDKTF will manage the S3 remote backend (S3 bucket and DynamoDB table) for you. Therefore, if you delete the stack, the S3 bucket and DynamoDB table will be vurnerable to deletion. Note, we can\u0026rsquo;t delete a non-empty S3 unless we add force_destroy=True to the S3 bucket configuration. This option is not recommended if you want to keep the S3 bucket and DynamoDB table, especially if you are using the S3 bucket and DynamoDB table as a remote backend for other Terraform projects. But, if you are just experimenting with CDKTF, this option is fine.\nB. If you enter no, to migrate the state file to the S3 backend, CDKTF will not manage the S3 bucket and DynamoDB table. If you delete the stack, the S3 bucket and DynamoDB table will not be deleted and you will have to manually delete the S3 bucket and DynamoDB table. Moreover, you will also need to remove the S3 bucket and DynamoDB table constructs from the main.py file.\nTo read more about initializing remote backend manually, refer to the Terraform documentation.\nThe below terminal recording demonstrates the steps above. In the recording, I have shown the error message that you may get if you attempt to run cdktf deploy prior to reconfiguring from local backend to an S3 remote backend. I have entered yes to migrate the state file to the S3 remote backend and let the CDKTF manage the S3 bucket and DynamoDB table.\n\nRun cdktf diff from the project root directory to compare the current state of the stack with the desired state of the stack. The output should be empty, which means there are no changes to be made and the state file is up to date.  cdktf diff\r Note: you have to be in the project root directory to run cdktf diff.\n Great! You have successfully migrated the local state backend to an S3 remote backend. Way to go, you have achieved another milestone! 🎉🎉🎉\n Step 4: Learn How to Use Construct Hub and AWS Provider Submodules Prior to digging into the AWS provider, let\u0026rsquo;s first understand most commonly used terms in the CDKTF documentation:\n  Submodules is a collection of related resources. For example, the s3.S3Bucket construct is part of the s3 submodule. The s3.S3Bucket construct creates an S3 bucket. The s3 submodule contains other constructs such as s3.S3BucketPolicy, s3.S3BucketAcl, s3.S3BucketObject, etc.\n  Construct is another important term to understand. A construct is a class that represents a Terraform resource, data source, or provider. The s3 submodule contains constructs that represent S3 constructs, classes and struts.\n  Stack is a collection of constructs. The MyStack class in the main.py file is a stack. The MyStack class contains constructs that represent S3 constructs, classes and struts.\n   Scenario 1: S3 Bucket Let\u0026rsquo;s say we would like to create an S3 but we don\u0026rsquo;t know which construct to use. Let\u0026rsquo;s head to the Python Construct hub for the AWS provider and follow the steps below:\n From the left hand side and under Documentation, click on Choose Submodule. In the search box, type in s3 and then click on the result, which is s3. Under Submodule:s3, you will see a list of Constructs and Struts. Click on S3Bucket To create an S3 bucket, you will import the s3 class as shown under Initializers. The construct to use is s3.S3Bucket. We need to find the required configurations for the s3.S3Bucket construct. Scan the page and look for configurations marked Required. In this case, S3 bucket does not have any required configuration, not even a name. If you leave the name argument empty, the S3 bucket will be created with a random name. We can specify a name for the S3 bucket and other configurations, but this is not required.  We have to distinguish between required and optional configurations. Required configurations must be specified when creating a resource. Optional configurations can be specified when creating a resource.\nThis is the code snippet for creating an S3 bucket with minimal configurations:\nmy_3bucket= s3.S3Bucket(self, \u0026quot;s3_bucket\u0026quot;)\r Note: if you leave the name argument empty, the S3 bucket will be created with a random name.\n  Scenario 2: ECS Cluster This time let\u0026rsquo;s say we would like to create an ECS cluster. Let\u0026rsquo;s head to the Python Construct hub for the AWS provider and follow the steps below:\n From the left hand side and under Documentation, click on Choose Submodule. In the search box, type in ecs and then click on the result, which is ecs. Under Submodule:ecs, you will see a list of Constructs and Struts. Click on EcsCluster To create an ECS cluster, you will import ecs class as shown under Initializers. The construct to use is ecs.EcsCluster as shown below. We need to find the required configurations for the ecs.EcsCluster construct. The minimum required configurations to create an ECS cluster is just the name of the cluster. But, we can also specify other configurations such as, capacity_providers, default_capacity_provider_strategy, configuration, etc.  my_ecs_cluster = ecs.EcsCluster(self, \u0026quot;my_ecs_cluster\u0026quot;,\rname = \u0026quot;My_Cluster\u0026quot;\r)\r CDKTF Commands: There are several CDKTF commands that we need to be familiar with. The below table shows the commands, their descriptions and the corresponding Terraform commands.\n   Commands Description Aliases     cdktf init Create a new cdktf project from a template.    cdktf get Generate CDK Constructs for Terraform providers and modules.    cdktf convert Converts a single file of HCL configuration to CDK for Terraform. Takes the file to be converted on stdin.    cdktf deploy Deploy the given stacks [aliases: apply]   cdktf destroy Destroy the given stacks    cdktf diff Perform a diff (terraform plan) for the given stack [aliases: plan]   cdktf list List stacks in app.    cdktf login Retrieves an API token to connect to Terraform Cloud or Terraform Enterprise.    cdktf synth Synthesizes Terraform code for the given app in a directory. [aliases: synthesize]   cdktf watch [experimental] Watch for file changes and automatically trigger a deploy    cdktf output Prints the output of stacks [aliases: outputs]   cdktf debug Get debug information about the current project and environment    cdktf provider A set of subcommands that facilitates provider management    cdktf completion generate completion script     To find out more about the cdktf commands, run cdktf [command] --help and replace [command] with the command you want to learn more about.\nFor example, to learn more about the cdktf deploy command, run cdktf deploy --help.\n Step 5: Deploying a Lambda Function URL using CDKTF CDKTF is a great tool to provision AWS resources. We have already created an S3 bucket and DynamoDB table in the previous section. In this section, I will show you how to create a Lambda function using CDKTF with function url enabled. The lambda will host a simple static web page, and well configure the function url as an output. The process requires creating an IAM role and attaching a policy to the role. I will also introduce you to multiple concepts in CDKTF.\nIn this section, I will cover the following topics:\n How to create an IAM role for Lambda function How to attach a policy to the IAM role How to create a Lambda function How to enable function url for the Lambda function How to package a Lambda function from a local directory and python file How to create an output for the Lambda function url  Buckle up, we are going to learn a lot in this section! 🚀🚀🚀\nFollow the steps below to deploy a Lambda function URL using CDKTF:\n  Firstly, let\u0026rsquo;s keep out project organized and create a new directory called lambda in the root directory of the project. This is where we will store our Lambda function code.\n  Create a new file called lambda_function.py in the lambda directory.\n  Copy the lambda_function.py code from my GitHub repository and paste it into the lambda_function.py file.\n  I will go over the main.py code and the final main.py file will provided at the end of the section.\n   In the main.py file, import the TerraformOutput, TerraformAsset and AssetType classes from the cdktf module. Assets construct is introduced in CDK for Terraform v0.4+ and is used to package a local directory and python file into a zip file. The TerraformOutput construct is used to create an output for the Lambda function url. The AssetType is used to specify the type of asset.  The final import statements should look like this:\nfrom constructs import Construct\rfrom cdktf import App, TerraformStack, S3Backend, TerraformOutput, TerraformAsset, AssetType\rfrom cdktf_cdktf_provider_aws import AwsProvider, s3, dynamodb, iam, lambdafunction\rimport os\rimport os.path as Path\r Note, we have imported os and os.path as Path modules. We will use these modules to get the current working directory and to join the path to the lambda directory.\n  The TerraformAsset construct requires a path argument. The path argument is the path to the directory or file that you want to package. In this case, we will use the os module to get the current working directory and then join the path to the lambda directory. The AssetType.ARCHIVE is to specify that the asset is an archive. The final path argument should look like this:  asset = TerraformAsset(self, \u0026quot;lambda_file\u0026quot;,\rpath = Path.join(os.getcwd(), \u0026quot;lambda\u0026quot;),\rtype = AssetType.ARCHIVE,\r)\r Creating a Lambda function requires creating an IAM role. Therefore, we will create an IAM role first. We will also attach the AWSLambdaBasicExecutionRole AWS managed policy to the IAM role. This policy allows the Lambda function to write logs to CloudWatch. Refer to AWS documentation for more information about the AWSLambdaBasicExecutionRole policy.  The assume_role_policy argument is the policy that grants permission to assume the IAM role, which requires a JSON string. The JSON string is the policy document that grants permission to assume the IAM role. The final snippet of code should look like this:\n lambda_role = iam.IamRole(self, \u0026quot;lambda_role\u0026quot;,\rname=\u0026quot;my-lambda-url-role\u0026quot;,\rmanaged_policy_arns=[\r\u0026quot;arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\u0026quot;\r],\rassume_role_policy=\u0026quot;\u0026quot;\u0026quot;{\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;,\r\u0026quot;Principal\u0026quot;: {\r\u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot;\r},\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Sid\u0026quot;: \u0026quot;\u0026quot;\r}\r]\r}\u0026quot;\u0026quot;\u0026quot;,\r)\r Now, we are ready to create a lambda function. The handler configuration is the name of the python file that contains the lambda function. The runtime provides a language-specific environment that runs in an execution environment. The source_code_hash argument is the hash of the file that contains the lambda function. The source_code_hash argument is required to trigger a new deployment when the lambda function code changes. The filename argument is the path to the file that contains the lambda function.  The final snippet of code should look like this:\n my_lambda = lambdafunction.LambdaFunction(self, \u0026quot;my_lambda\u0026quot;,\rfunction_name=\u0026quot;my-lambda-url\u0026quot;,\rhandler=\u0026quot;lambda_function.lambda_handler\u0026quot;,\rrole=lambda_role.arn,\rruntime=\u0026quot;python3.9\u0026quot;,\rsource_code_hash = asset.asset_hash,\rfilename=asset.path,\r)\r We need to enable function url for the lambda function and define an authorization_type for the function url. The authorization_type argument is the type of authorization that is used to invoke the function url. The authorization_type argument can be set to NONE or AWS_IAM. The final snippet of code should look like this:   my_lambda_url = lambdafunction.LambdaFunctionUrl(self, \u0026quot;my_lambda_url\u0026quot;,\rfunction_name=my_lambda.function_name,\rauthorization_type=\u0026quot;NONE\u0026quot;,\r)\r Finally, we need to create an output for the Lambda function url. The value argument is the value of the output. The value argument can be a string, number, boolean, or a list. The final snippet of code should look like this:   TerraformOutput(self, \u0026quot;lambda_url\u0026quot;,\rvalue=my_lambda_url.invoke_url,\r)\rThe final main.py code should look like this:\nfrom constructs import Construct\rfrom cdktf import App, TerraformStack, S3Backend, TerraformOutput, TerraformAsset, AssetType\rfrom cdktf_cdktf_provider_aws import AwsProvider, s3, dynamodb, iam, lambdafunction\rimport os\rimport os.path as Path\rclass MyStack(TerraformStack):\rdef __init__(self, scope: Construct, ns: str):\rsuper().__init__(scope, ns)\rAwsProvider(self, \u0026quot;AWS\u0026quot;, region=\u0026quot;us-east-1\u0026quot;, profile=\u0026quot;CDKTF\u0026quot;)\r#S3 Remote Backend\rS3Backend(self,\rbucket=\u0026quot;cdktf-remote-backend-2\u0026quot;,\rkey=\u0026quot;first_project/terraform.tfstate\u0026quot;,\rencrypt=True,\rregion=\u0026quot;us-east-1\u0026quot;,\rdynamodb_table=\u0026quot;cdktf-remote-backend-lock-2\u0026quot;,\rprofile=\u0026quot;CDKTF\u0026quot;,\r)\r# Resources\rs3_backend_bucket = s3.S3Bucket(self, \u0026quot;s3_backend_bucket\u0026quot;,\rbucket=\u0026quot;cdktf-remote-backend-2\u0026quot;,\r)\rdynamodb_lock_table = dynamodb.DynamodbTable(self, \u0026quot;dynamodb_lock_table\u0026quot;,\rname=\u0026quot;cdktf-remote-backend-lock-2\u0026quot;,\rbilling_mode=\u0026quot;PAY_PER_REQUEST\u0026quot;,\rattribute=[\r{\r\u0026quot;name\u0026quot;: \u0026quot;LockID\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;S\u0026quot;\r}\r],\rhash_key=\u0026quot;LockID\u0026quot;,\r)\r# Asset for Lambda Function\rasset = TerraformAsset(self, \u0026quot;lambda_file\u0026quot;,\rpath = Path.join(os.getcwd(), \u0026quot;lambda\u0026quot;),\rtype = AssetType.ARCHIVE,\r)\r# IAM Role for Lambda Function\rlambda_role = iam.IamRole(self, \u0026quot;lambda_role\u0026quot;,\rname=\u0026quot;my-lambda-url-role\u0026quot;,\rmanaged_policy_arns=[\r\u0026quot;arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\u0026quot;\r],\rassume_role_policy=\u0026quot;\u0026quot;\u0026quot;{\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;,\r\u0026quot;Principal\u0026quot;: {\r\u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot;\r},\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Sid\u0026quot;: \u0026quot;\u0026quot;\r}\r]\r}\u0026quot;\u0026quot;\u0026quot;,\r)\r# Lambda Function\rmy_lambda = lambdafunction.LambdaFunction(self, \u0026quot;my_lambda\u0026quot;,\rfunction_name=\u0026quot;my-lambda-url\u0026quot;,\rhandler=\u0026quot;lambda_function.lambda_handler\u0026quot;,\rrole=lambda_role.arn,\rruntime=\u0026quot;python3.9\u0026quot;,\rsource_code_hash = asset.asset_hash,\rfilename=asset.path,\r)\r# Lambda Function Url\rmy_lambda_url = lambdafunction.LambdaFunctionUrl(self, \u0026quot;my_lambda_url\u0026quot;,\rfunction_name=my_lambda.function_name,\rauthorization_type=\u0026quot;NONE\u0026quot;,\r)\r# Outputs for Lambda Function Url\rTerraformOutput(self, \u0026quot;lambda_url\u0026quot;,\rvalue=my_lambda_url.function_url,\r)\rapp = App()\rMyStack(app, \u0026quot;first_project\u0026quot;)\rapp.synth()\rRun cdktf deploy to deploy the stack.  cdktf deploy\r Note: you can also run cdktf deploy --auto-approve to deploy the stack without confirmation. However, I would suggest to refrain from using this option in production unless you are absolutely sure that you want to deploy the stack without confirmation.\n Finally, grab the lambda_url output and paste it in your browser. If you see the dancing bananas, then you have successfully deployed your first lambda function using CDKTF.\nCongratulations! You have successfully deployed a lambda function with a function url using CDKTF. I\u0026rsquo;m sure you feel like you are on top of the world right now. Well done!\n To delete the stack, we need to follow the below steps:\nNote, we chose to allow CDKTF to manage the remote backend. This means that CDKTF will delete the remote backend (the S3 bucket and DynamoDB Table) when we delete the stack. There are many methods to delete the stack, but I find the below method to be the easiest. Let\u0026rsquo;s go through the steps:\nA. Add force_destroy=True to the s3_backend_bucket configurations. The S3 bucket cannot be deleted if it is not empty. This is the reason why we need to add force_destroy=True.\n s3_backend_bucket = s3.S3Bucket(self, \u0026quot;s3_backend_bucket\u0026quot;,\rbucket=\u0026quot;cdktf-remote-backend-2\u0026quot;,\rforce_destroy=True\r)\rB. Run cdktf deploy to update the S3 bucket configurations:\ncdktf deploy\rC. Run cdktf destroy to delete the entire stack:\ncdktf destroy\r Note: after running cdktf destroy, you will get an error message saying failed to retrieve lock info. This is expected due to the fact the dynamodb table is deleted. You can ignore this error message.\n  Conclusion In this tutorial, we have learned how to properly install and configure CDKTF, how to migrate from a local backend to an S3 remote backend. We have also learned how to deploy a lambda function with a function url using CDKTF. We have also briefly learned how to read and utilize the CDKTF documentations from the Construct Hub.\nThe most important thing to remember is that CDKTF is still in its early stages. There are many features that are not yet available. However, I am confident that CDKTF will be a great tool for managing Terraform stacks in the future.\nCongratulations on completing this tutorial and overcoming several challenges. You have achieved many learning milestones. I hope this tutorial added value to your learning journey. Thank you for reading!\nOmar A Omar\nSite Reliability Engineer\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/new-folder/configuration/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Global site parameters On top of Hugo global configuration, Hugo-theme-learn lets you define the following parameters in your config.toml (here, values are default).\nNote that some of these parameters are explained in details in other sections of this documentation.\n[params] # Prefix URL to edit current page. Will display an \u0026#34;Edit this page\u0026#34; button on top right hand corner of every page. # Useful to give opportunity to people to create merge request for your doc. # See the config.toml file from this documentation site to have an example. editURL = \u0026#34;\u0026#34; # Author of the site, will be used in meta information author = \u0026#34;\u0026#34; # Description of the site, will be used in meta information description = \u0026#34;\u0026#34; # Shows a checkmark for visited pages on the menu showVisitedLinks = false # Disable search function. It will hide search bar disableSearch = false # Javascript and CSS cache are automatically busted when new version of site is generated. # Set this to true to disable this behavior (some proxies don\u0026#39;t handle well this optimization) disableAssetsBusting = false # Set this to true to disable copy-to-clipboard button for inline code. disableInlineCopyToClipBoard = false # A title for shortcuts in menu is set by default. Set this to true to disable it. disableShortcutsTitle = false # If set to false, a Home button will appear below the search bar on the menu. # It is redirecting to the landing page of the current language if specified. (Default is \u0026#34;/\u0026#34;) disableLandingPageButton = true # When using mulitlingual website, disable the switch language button. disableLanguageSwitchingButton = false # Hide breadcrumbs in the header and only show the current page title disableBreadcrumb = true # If set to true, prevents Hugo from including the mermaid module if not needed (will reduce load times and traffic) disableMermaid = false # Specifies the remote location of the mermaid js customMermaidURL = \u0026#34;https://unpkg.com/mermaid@8.8.0/dist/mermaid.min.js\u0026#34; # Hide Next and Previous page buttons normally displayed full height beside content disableNextPrev = true # Order sections in menu by \u0026#34;weight\u0026#34; or \u0026#34;title\u0026#34;. Default to \u0026#34;weight\u0026#34; ordersectionsby = \u0026#34;weight\u0026#34; # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;\u0026#34; # Provide a list of custom css files to load relative from the `static/` folder in the site root. custom_css = [\u0026#34;css/foo.css\u0026#34;, \u0026#34;css/bar.css\u0026#34;] # Change the title separator. Default to \u0026#34;::\u0026#34;. titleSeparator = \u0026#34;-\u0026#34; Activate search If not already present, add the follow lines in the same config.toml file.\n[outputs] home = [ \u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] Learn theme uses the last improvement available in hugo version 20+ to generate a json index file ready to be consumed by lunr.js javascript search engine.\n Hugo generate lunrjs index.json at the root of public folder. When you build the site with hugo server, hugo generates it internally and of course it doesn’t show up in the filesystem\n Mermaid The mermaid configuration parameters can also be set on a specific page. In this case, the global parameter would be overwritten by the local one.\n Example:\nMermaid is globally disabled. By default it won\u0026rsquo;t be loaded by any page.\nOn page \u0026ldquo;Architecture\u0026rdquo; you need a class diagram. You can set the mermaid parameters locally to only load mermaid on this page (not on the others).\n You also can disable mermaid for specific pages while globally enabled.\nHome Button Configuration If the disableLandingPage option is set to false, an Home button will appear on the left menu. It is an alternative for clicking on the logo. To edit the appearance, you will have to configure two parameters for the defined languages:\n[Lanugages] [Lanugages.en] ... landingPageURL = \u0026#34;/en\u0026#34; landingPageName = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-home\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Redirect to Home\u0026#34; ... [Lanugages.fr] ... landingPageURL = \u0026#34;/fr\u0026#34; landingPageName = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-home\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Accueil\u0026#34; ... If those params are not configured for a specific language, they will get their default values:\nlandingPageURL = \u0026#34;/\u0026#34; landingPageName = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-home\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Home\u0026#34; The home button is going to looks like this:\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/new-folder/installation/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "The following steps are here to help you initialize your new website. If you don\u0026rsquo;t know Hugo at all, we strongly suggest you learn more about it by following this great documentation for beginners.\nCreate your project Hugo provides a new command to create a new website.\nhugo new site \u0026lt;new_project\u0026gt;\rInstall the theme Install the Hugo-theme-learn theme by following this documentation\nThis theme\u0026rsquo;s repository is: https://github.com/matcornic/hugo-theme-learn.git\nAlternatively, you can download the theme as .zip file and extract it in the themes directory\nBasic configuration When building the website, you can set a theme by using --theme option. However, we suggest you modify the configuration file (config.toml) and set the theme as the default. You can also add the [outputs] section to enable the search functionality.\n# Change the default theme to be use when building the site with Hugo theme = \u0026#34;hugo-theme-learn\u0026#34; # For search functionality [outputs] home = [ \u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] Create your first chapter page Chapters are pages that contain other child pages. It has a special layout style and usually just contains a chapter name, the title and a brief abstract of the section.\n### Chapter 1\r# Basics\rDiscover what this Hugo theme is all about and the core concepts behind it.\rrenders as\nHugo-theme-learn provides archetypes to create skeletons for your website. Begin by creating your first chapter page with the following command\nhugo new --kind chapter basics/_index.md\rBy opening the given file, you should see the property chapter=true on top, meaning this page is a chapter.\nBy default all chapters and pages are created as a draft. If you want to render these pages, remove the property draft: true from the metadata.\nCreate your first content pages Then, create content pages inside the previously created chapter. Here are two ways to create content in the chapter:\nhugo new basics/first-content.md\rhugo new basics/second-content/_index.md\rFeel free to edit thoses files by adding some sample content and replacing the title value in the beginning of the files.\nLaunching the website locally Launch by using the following command:\nhugo serve\rGo to http://localhost:1313\nYou should notice three things:\n You have a left-side Basics menu, containing two submenus with names equal to the title properties in the previously created files. The home page explains how to customize it by following the instructions. When you run hugo serve, when the contents of the files change, the page automatically refreshes with the changes. Neat!  Build the website When your site is ready to deploy, run the following command:\nhugo\rA public folder will be generated, containing all static content and assets for your website. It can now be deployed on any web server.\nThis website can be automatically published and hosted with Netlify (Read more about Automated HUGO deployments with Netlify). Alternatively, you can use Github pages\n\r"
},
{
	"uri": "https://omar2cloud.github.io/aws/new-folder/requirements/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Thanks to the simplicity of Hugo, this page is as empty as this theme needs requirements.\nJust download latest version of Hugo binary (\u0026gt; 0.25) for your OS (Windows, Linux, Mac) : it\u0026rsquo;s that simple.\n"
},
{
	"uri": "https://omar2cloud.github.io/aws/new-folder/style-customization/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Hugo-theme-learn has been built to be as configurable as possible by defining multiple partials\nIn themes/hugo-theme-learn/layouts/partials/, you will find all the partials defined for this theme. If you need to overwrite something, don\u0026rsquo;t change the code directly. Instead follow this page. You\u0026rsquo;d create a new partial in the layouts/partials folder of your local project. This partial will have the priority.\nThis theme defines the following partials :\n header: the header of the content page (contains the breadcrumbs). Not meant to be overwritten custom-header: custom headers in page. Meant to be overwritten when adding CSS imports. Don\u0026rsquo;t forget to include style HTML tag directive in your file footer: the footer of the content page (contains the arrows). Not meant to be overwritten custom-footer: custom footer in page. Meant to be overwritten when adding Javacript. Don\u0026rsquo;t forget to include javascript HTML tag directive in your file favicon: the favicon logo: the logo, on top left hand corner. meta: HTML meta tags, if you want to change default behavior menu: left menu. Not meant to be overwritten menu-footer: footer of the the left menu search: search box toc: table of contents  Change the logo Create a new file in layouts/partials/ named logo.html. Then write any HTML you want. You could use an img HTML tag and reference an image created under the static folder, or you could paste a SVG definition !\nThe size of the logo will adapt automatically\n\rChange the favicon If your favicon is a png, just drop off your image in your local static/images/ folder and name it favicon.png\nIf you need to change this default behavior, create a new file in layouts/partials/ named favicon.html. Then write something like this:\n\u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;/images/favicon.png\u0026#34; type=\u0026#34;image/x-icon\u0026#34; /\u0026gt; Change default colors Hugo Learn theme let you choose between 3 native color scheme variants, but feel free to add one yourself ! Default color scheme is based on Grav Learn Theme.\nRed variant [params] # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;red\u0026#34; Blue variant [params] # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;blue\u0026#34; Green variant [params] # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;green\u0026#34; \u0026lsquo;Yours‘ variant First, create a new CSS file in your local static/css folder prefixed by theme (e.g. with mine theme static/css/theme-mine.css). Copy the following content and modify colors in CSS variables.\n:root{ --MAIN-TEXT-color:#323232; /* Color of text by default */ --MAIN-TITLES-TEXT-color: #5e5e5e; /* Color of titles h2-h3-h4-h5 */ --MAIN-LINK-color:#1C90F3; /* Color of links */ --MAIN-LINK-HOVER-color:#167ad0; /* Color of hovered links */ --MAIN-ANCHOR-color: #1C90F3; /* color of anchors on titles */ --MENU-HEADER-BG-color:#1C90F3; /* Background color of menu header */ --MENU-HEADER-BORDER-color:#33a1ff; /*Color of menu header border */ --MENU-SEARCH-BG-color:#167ad0; /* Search field background color (by default borders + icons) */ --MENU-SEARCH-BOX-color: #33a1ff; /* Override search field border color */ --MENU-SEARCH-BOX-ICONS-color: #a1d2fd; /* Override search field icons color */ --MENU-SECTIONS-ACTIVE-BG-color:#20272b; /* Background color of the active section and its childs */ --MENU-SECTIONS-BG-color:#252c31; /* Background color of other sections */ --MENU-SECTIONS-LINK-color: #ccc; /* Color of links in menu */ --MENU-SECTIONS-LINK-HOVER-color: #e6e6e6; /* Color of links in menu, when hovered */ --MENU-SECTION-ACTIVE-CATEGORY-color: #777; /* Color of active category text */ --MENU-SECTION-ACTIVE-CATEGORY-BG-color: #fff; /* Color of background for the active category (only) */ --MENU-VISITED-color: #33a1ff; /* Color of \u0026#39;page visited\u0026#39; icons in menu */ --MENU-SECTION-HR-color: #20272b; /* Color of \u0026lt;hr\u0026gt; separator in menu */ } body { color: var(--MAIN-TEXT-color) !important; } textarea:focus, input[type=\u0026#34;email\u0026#34;]:focus, input[type=\u0026#34;number\u0026#34;]:focus, input[type=\u0026#34;password\u0026#34;]:focus, input[type=\u0026#34;search\u0026#34;]:focus, input[type=\u0026#34;tel\u0026#34;]:focus, input[type=\u0026#34;text\u0026#34;]:focus, input[type=\u0026#34;url\u0026#34;]:focus, input[type=\u0026#34;color\u0026#34;]:focus, input[type=\u0026#34;date\u0026#34;]:focus, input[type=\u0026#34;datetime\u0026#34;]:focus, input[type=\u0026#34;datetime-local\u0026#34;]:focus, input[type=\u0026#34;month\u0026#34;]:focus, input[type=\u0026#34;time\u0026#34;]:focus, input[type=\u0026#34;week\u0026#34;]:focus, select[multiple=multiple]:focus { border-color: none; box-shadow: none; } h2, h3, h4, h5 { color: var(--MAIN-TITLES-TEXT-color) !important; } a { color: var(--MAIN-LINK-color); } .anchor { color: var(--MAIN-ANCHOR-color); } a:hover { color: var(--MAIN-LINK-HOVER-color); } #sidebar ul li.visited \u0026gt; a .read-icon { color: var(--MENU-VISITED-color); } #body a.highlight:after { display: block; content: \u0026#34;\u0026#34;; height: 1px; width: 0%; -webkit-transition: width 0.5s ease; -moz-transition: width 0.5s ease; -ms-transition: width 0.5s ease; transition: width 0.5s ease; background-color: var(--MAIN-LINK-HOVER-color); } #sidebar { background-color: var(--MENU-SECTIONS-BG-color); } #sidebar #header-wrapper { background: var(--MENU-HEADER-BG-color); color: var(--MENU-SEARCH-BOX-color); border-color: var(--MENU-HEADER-BORDER-color); } #sidebar .searchbox { border-color: var(--MENU-SEARCH-BOX-color); background: var(--MENU-SEARCH-BG-color); } #sidebar ul.topics \u0026gt; li.parent, #sidebar ul.topics \u0026gt; li.active { background: var(--MENU-SECTIONS-ACTIVE-BG-color); } #sidebar .searchbox * { color: var(--MENU-SEARCH-BOX-ICONS-color); } #sidebar a { color: var(--MENU-SECTIONS-LINK-color); } #sidebar a:hover { color: var(--MENU-SECTIONS-LINK-HOVER-color); } #sidebar ul li.active \u0026gt; a { background: var(--MENU-SECTION-ACTIVE-CATEGORY-BG-color); color: var(--MENU-SECTION-ACTIVE-CATEGORY-color) !important; } #sidebar hr { border-color: var(--MENU-SECTION-HR-color); } Then, set the themeVariant value with the name of your custom theme file. That\u0026rsquo;s it !\n[params] # Change default color scheme with a variant one. Can be \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;. themeVariant = \u0026#34;mine\u0026#34; "
},
{
	"uri": "https://omar2cloud.github.io/shortcodes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://omar2cloud.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://omar2cloud.github.io/credits/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": "Contributors Thanks to them for making Open Source Software a better place !\nAnd a special thanks to @vjeantet for his work on docdock, a fork of hugo-theme-learn. v2.0.0 of this theme is inspired by his work.\nPackages and libraries  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support  Tooling  Netlify - Continuous deployement and hosting of this documentation Hugo  "
},
{
	"uri": "https://omar2cloud.github.io/",
	"title": "Learn with Omar",
	"tags": [],
	"description": "",
	"content": "Learn With Omar As an AWS Community Builder, I believe sharing knowledge with the community has a significant and positive impact on one\u0026rsquo;s own growth, motivation, and knowledge base. Since having a sense of purpose is what matters most to me, I want to share my knowledge and experience with the community.\nAllow me to introduce myself. I graduated from the George Washington University with a bachelor’s degree in mechanical engineering. I have earned my International MBA from Texas Tech University and Post Graduate Certificate in Cloud Computing from the University of Texas at Austin and Great Learning. I have more than ten years of experience in various engineering roles. Currently, I work as a Site Reliability Engineer with Pearson. I also mentor students in the Post Graduate Certificate in Cloud Computing program at the University of Texas at Austin and Great Learning. I\u0026rsquo;m a certified AWS Solutions Architect, Developer and SysOps Administrator associate. I place a lot of value on education and practical experience.\nMost of my tutorials are built on practical and real-world solutions I\u0026rsquo;ve developed and learned over years. Additionally, since I\u0026rsquo;m a fan of Raspberry Pis, there are lots of tutorials and projects for these adorable pretty little things ❤️ I am certain that acquiring self-learning techniques significantly aids in the development of other crucial skills like problem solving, time management, and critical thinking.\nI sincerely hope you find these tutorials to be beneficial and helpful. Enjoy.\nOmar A Omar\n"
},
{
	"uri": "https://omar2cloud.github.io/showcase/",
	"title": "Showcase",
	"tags": [],
	"description": "",
	"content": "TAT by OVH Tshark.dev by Ross Jacobs inteliver by Amir Lavasani "
},
{
	"uri": "https://omar2cloud.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]